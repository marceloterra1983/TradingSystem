#!/usr/bin/env python3
"""
Ollama Multi-Model Service
Servi√ßo para gerenciar m√∫ltiplos modelos Ollama no TradingSystem
"""

import requests
import json
from typing import List, Dict, Optional
from enum import Enum


class ModelSize(Enum):
    """Tipos de modelos por tamanho/uso"""
    MONSTER = "gpt-oss:120b"    # 65GB - M√°xima qualidade
    LARGE = "gpt-oss:20b"       # 13GB - Equilibrado
    MEDIUM = "gemma2:27b"       # 15GB - Google multil√≠ngue
    FAST = "llama3.2"           # 2GB - Respostas r√°pidas


class OllamaService:
    """Servi√ßo para interagir com Ollama usando m√∫ltiplos modelos"""
    
    def __init__(self, base_url: str = "http://localhost:11434"):
        self.base_url = base_url
        self.api_url = f"{base_url}/api"
    
    def generate(
        self,
        prompt: str,
        model: ModelSize = ModelSize.LARGE,
        stream: bool = False
    ) -> str:
        """
        Gera texto usando o modelo especificado
        
        Args:
            prompt: Texto de entrada
            model: Modelo a usar (padr√£o: gpt-oss:20b)
            stream: Se True, retorna generator para streaming
        
        Returns:
            Resposta gerada pelo modelo
        """
        response = requests.post(
            f"{self.api_url}/generate",
            json={
                "model": model.value,
                "prompt": prompt,
                "stream": stream
            },
            stream=stream
        )
        
        if stream:
            return self._handle_streaming(response)
        else:
            return response.json()["response"]
    
    def chat(
        self,
        messages: List[Dict[str, str]],
        model: ModelSize = ModelSize.LARGE,
        stream: bool = False
    ) -> str:
        """
        Chat com contexto de conversa√ß√£o
        
        Args:
            messages: Lista de mensagens [{"role": "user", "content": "..."}]
            model: Modelo a usar
            stream: Se True, retorna generator
        
        Returns:
            Resposta do chat
        """
        response = requests.post(
            f"{self.api_url}/chat",
            json={
                "model": model.value,
                "messages": messages,
                "stream": stream
            },
            stream=stream
        )
        
        if stream:
            return self._handle_streaming(response)
        else:
            return response.json()["message"]["content"]
    
    def _handle_streaming(self, response):
        """Handle streaming responses"""
        for line in response.iter_lines():
            if line:
                data = json.loads(line)
                if "response" in data:
                    yield data["response"]
                elif "message" in data:
                    yield data["message"]["content"]
    
    def list_models(self) -> List[Dict]:
        """Lista todos os modelos instalados"""
        response = requests.get(f"{self.api_url}/tags")
        return response.json().get("models", [])
    
    def analyze_market(
        self,
        market_data: str,
        complexity: str = "medium"
    ) -> str:
        """
        Analisa dados de mercado usando o modelo apropriado
        
        Args:
            market_data: Dados de mercado para an√°lise
            complexity: 'simple', 'medium', 'deep'
        
        Returns:
            An√°lise gerada
        """
        model_map = {
            "simple": ModelSize.FAST,      # R√°pido para an√°lises b√°sicas
            "medium": ModelSize.LARGE,     # Equilibrado
            "deep": ModelSize.MONSTER      # An√°lise profunda
        }
        
        model = model_map.get(complexity, ModelSize.LARGE)
        
        prompt = f"""Analise os seguintes dados de mercado e forne√ßa insights:

{market_data}

Forne√ßa:
1. Tend√™ncia identificada
2. Pontos de entrada/sa√≠da
3. N√≠vel de risco
4. Recomenda√ß√£o
"""
        
        return self.generate(prompt, model)
    
    def generate_code(
        self,
        description: str,
        language: str = "python"
    ) -> str:
        """
        Gera c√≥digo usando o modelo mais adequado
        
        Args:
            description: Descri√ß√£o do c√≥digo a gerar
            language: Linguagem de programa√ß√£o
        
        Returns:
            C√≥digo gerado
        """
        prompt = f"""Gere c√≥digo {language} para: {description}

Requisitos:
- C√≥digo limpo e bem documentado
- Seguir boas pr√°ticas
- Incluir tratamento de erros
"""
        
        # Usar modelo MONSTER para c√≥digo complexo
        return self.generate(prompt, ModelSize.MONSTER)
    
    def smart_generate(
        self,
        prompt: str,
        auto_select: bool = True
    ) -> str:
        """
        Gera texto selecionando automaticamente o melhor modelo
        
        Args:
            prompt: Texto de entrada
            auto_select: Se True, escolhe modelo baseado no prompt
        
        Returns:
            Resposta gerada
        """
        if not auto_select:
            return self.generate(prompt, ModelSize.LARGE)
        
        # Heur√≠stica simples para selecionar modelo
        prompt_lower = prompt.lower()
        
        # Usar MONSTER para:
        if any(word in prompt_lower for word in [
            "an√°lise profunda", "detalhado", "complexo",
            "c√≥digo", "algoritmo", "implementa√ß√£o"
        ]):
            print("üî• Usando MONSTER (120B) - An√°lise profunda")
            return self.generate(prompt, ModelSize.MONSTER)
        
        # Usar FAST para:
        elif any(word in prompt_lower for word in [
            "ol√°", "oi", "sim", "n√£o", "r√°pido"
        ]) or len(prompt) < 50:
            print("üèÉ Usando FAST (3.2B) - Resposta r√°pida")
            return self.generate(prompt, ModelSize.FAST)
        
        # Usar LARGE para o resto
        else:
            print("üéØ Usando LARGE (20B) - Equilibrado")
            return self.generate(prompt, ModelSize.LARGE)


# ========== EXEMPLOS DE USO ==========

if __name__ == "__main__":
    ollama = OllamaService()
    
    print("=" * 60)
    print("OLLAMA MULTI-MODEL SERVICE - DEMOS")
    print("=" * 60)
    print()
    
    # 1. Teste com diferentes modelos
    print("1Ô∏è‚É£ Testando todos os modelos com a mesma pergunta:")
    print("-" * 60)
    
    question = "O que √© trading algor√≠tmico?"
    
    for model in ModelSize:
        print(f"\nü§ñ {model.name} ({model.value}):")
        try:
            response = ollama.generate(question, model)
            print(f"   {response[:150]}...")
        except Exception as e:
            print(f"   ‚ùå Erro: {e}")
    
    print("\n" + "=" * 60)
    
    # 2. Smart generation (auto-sele√ß√£o)
    print("\n2Ô∏è‚É£ Smart Generation (auto-sele√ß√£o de modelo):")
    print("-" * 60)
    
    prompts = [
        "Ol√°!",  # Deve usar FAST
        "Explique machine learning",  # Deve usar LARGE
        "An√°lise profunda sobre estrat√©gias de trading"  # Deve usar MONSTER
    ]
    
    for prompt in prompts:
        print(f"\nPrompt: {prompt}")
        response = ollama.smart_generate(prompt)
        print(f"Resposta: {response[:100]}...")
    
    print("\n" + "=" * 60)
    
    # 3. An√°lise de mercado
    print("\n3Ô∏è‚É£ An√°lise de Mercado:")
    print("-" * 60)
    
    market_data = """
    ATIVO: WINZ25
    PRE√áO: 125.500
    VOLUME: Alto
    TEND√äNCIA: Lateraliza√ß√£o ap√≥s alta de 3%
    SUPORTE: 124.800
    RESIST√äNCIA: 126.200
    """
    
    print("\nAn√°lise R√°pida (FAST):")
    quick_analysis = ollama.analyze_market(market_data, "simple")
    print(quick_analysis[:200])
    
    print("\n" + "=" * 60)
    print("‚úÖ Todos os testes conclu√≠dos!")
    print("=" * 60)








