# ğŸ¤– Ollama Multi-Model Setup

## âœ… Status Atual

**Container Ollama rodando com GPU (RTX 5090 - 32GB VRAM)**

### ğŸ¯ Modelos Instalados (95GB total):

```
ğŸ”¥ gpt-oss:120b  (65GB) - MONSTRO! MÃ¡xima qualidade
ğŸ¯ gpt-oss:20b   (13GB) - Equilibrado, uso geral
âš¡ gemma2:27b    (15GB) - Google, multilÃ­ngue
ğŸƒ llama3.2      (2GB)  - RÃ¡pido para respostas simples
```

**Endpoint:** http://localhost:11434

---

## âš¡ InÃ­cio RÃ¡pido

### Teste RÃ¡pido
```bash
# Testar modelo 120B
curl -s http://localhost:11434/api/generate -d '{
  "model": "gpt-oss:120b",
  "prompt": "OlÃ¡! Me dÃª uma dica de trading",
  "stream": false
}' | jq -r '.response'
```

### Comparar Todos os Modelos
```bash
# Executar script de comparaÃ§Ã£o
./test-all-models.sh
```

### ServiÃ§o Python
```bash
# Usar o serviÃ§o multi-modelo
python3 ollama-multi-model-service.py
```

---

## ğŸ“‹ Comandos Essenciais

```bash
# Ver modelos instalados
docker exec ollama ollama list

# Conversar (modo interativo)
docker exec -it ollama ollama run gpt-oss:120b

# Ver uso de GPU
nvidia-smi

# Ver logs do container
docker logs -f ollama

# Reiniciar container
docker restart ollama

# Parar/Iniciar
docker stop ollama
docker start ollama
```

---

## ğŸ¯ Quando Usar Cada Modelo

| Tarefa | Modelo | Motivo |
|--------|--------|--------|
| **AnÃ¡lise profunda** | gpt-oss:120b | MÃ¡xima inteligÃªncia |
| **Uso diÃ¡rio** | gpt-oss:20b | Equilibrado |
| **TraduÃ§Ã£o** | gemma2:27b | MultilÃ­ngue |
| **Testes rÃ¡pidos** | llama3.2 | Velocidade |

---

## ğŸ“š DocumentaÃ§Ã£o

- **[USAGE-GUIDE.md](USAGE-GUIDE.md)** - Guia completo de uso
- **[ollama-multi-model-service.py](ollama-multi-model-service.py)** - ServiÃ§o Python
- **[test-all-models.sh](test-all-models.sh)** - Script de testes comparativos
- **[setup-ollama-docker-gpu.sh](setup-ollama-docker-gpu.sh)** - Script de instalaÃ§Ã£o

---

## ğŸ”¥ Exemplo PrÃ¡tico no TradingSystem

```typescript
// frontend/apps/dashboard/src/services/ollamaService.ts

export class OllamaService {
  private baseUrl = 'http://localhost:11434/api';
  
  // AnÃ¡lise profunda (usa modelo 120B)
  async analyzeMarket(data: string): Promise<string> {
    return this.generate(data, 'gpt-oss:120b');
  }
  
  // Uso geral (usa modelo 20B)
  async askQuestion(question: string): Promise<string> {
    return this.generate(question, 'gpt-oss:20b');
  }
  
  // Respostas rÃ¡pidas (usa llama3.2)
  async quickResponse(prompt: string): Promise<string> {
    return this.generate(prompt, 'llama3.2');
  }
  
  private async generate(prompt: string, model: string) {
    const response = await fetch(`${this.baseUrl}/generate`, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({ model, prompt, stream: false })
    });
    const data = await response.json();
    return data.response;
  }
}
```

---

## ğŸ†˜ Troubleshooting

### Modelo estÃ¡ lento
- Modelos grandes (120B) sÃ£o naturalmente mais lentos
- Use gpt-oss:20b ou llama3.2 para respostas mais rÃ¡pidas
- Verifique GPU: `nvidia-smi`

### Falta de memÃ³ria
- Remova modelos nÃ£o usados: `docker exec ollama ollama rm llama3.2`
- O modelo 120B usa ~30GB de VRAM

### Container nÃ£o responde
```bash
docker restart ollama
docker logs ollama
```

---

## ğŸš€ Setup Completo

O setup foi feito usando:
- âœ… Docker com suporte a GPU (`--gpus all`)
- âœ… NVIDIA Container Toolkit instalado
- âœ… Volume persistente para modelos
- âœ… 4 modelos instalados (95GB)
- âœ… RTX 5090 com 32GB VRAM

**Criado em:** 2025-10-18  
**Sistema:** WSL2 + Ubuntu + Docker + NVIDIA









