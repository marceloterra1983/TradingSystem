# üè• Health Check: LlamaIndex Query Service

**Data:** 2025-11-02  
**Servi√ßo:** LlamaIndex Query Service  
**Port:** 8202  
**Container:** `rag-llamaindex-query`

---

## üìä Status Geral

| Componente | Status | Detalhes |
|------------|--------|----------|
| **Container** | ‚úÖ UP | Running (healthy) |
| **Health Endpoint** | ‚úÖ OK | `/health` responde |
| **Qdrant Connection** | ‚úÖ OK | 51,940 vetores na cole√ß√£o `documentation__nomic` |
| **Ollama Connection** | ‚úÖ OK | Modelos dispon√≠veis |
| **GPU** | ‚úÖ OK | RTX 5090 (5GB/32GB used) |
| **Query Endpoint** | ‚ö†Ô∏è **PARCIAL** | `/query` existe mas falha em execu√ß√£o |

---

## üîç Descobertas do Health Check

### 1. **Endpoints Dispon√≠veis** (via OpenAPI /docs)

```
http://localhost:8202/
‚îú‚îÄ‚îÄ GET  /health          ‚úÖ Status: healthy
‚îú‚îÄ‚îÄ POST /query           ‚ö†Ô∏è  Requer JWT + Falha com LLM
‚îú‚îÄ‚îÄ POST /search          ‚ùå Method not allowed  
‚îî‚îÄ‚îÄ GET  /gpu/policy      ‚úÖ GPU policy config
```

**Swagger UI:** http://localhost:8202/docs ‚úÖ

---

### 2. **Health Endpoint Response**

```json
{
  "collection": "documentation__nomic",
  "configuredCollection": "documentation__nomic",
  "activeCollection": "documentation__nomic",
  "status": "healthy",
  "collectionExists": true,
  "vectors": 51940,
  "fallbackApplied": false
}
```

**An√°lise:**
- ‚úÖ Cole√ß√£o `documentation__nomic` ativa
- ‚úÖ **51,940 vetores indexados** (dados dispon√≠veis!)
- ‚úÖ Sem fallback (cole√ß√£o configurada existe)
- ‚úÖ Qdrant connection funcionando

---

### 3. **Query Endpoint Problem**

#### Request Teste
```bash
curl -X POST http://localhost:8202/query \
  -H "Authorization: Bearer <JWT>" \
  -d '{"query":"test","top_k":3}'
```

#### Response
```json
{
  "detail": "Error processing query: llama runner process has terminated: signal: killed (status code: 500)"
}
```

**Diagn√≥stico:**
- ‚ö†Ô∏è Endpoint **aceita autentica√ß√£o** (JWT validado)
- ‚ùå **Falha ao executar query** (LLM process killed)
- üîç **Causa prov√°vel:** Ollama est√° tentando carregar LLM (llama3.1:latest - 4GB) para gera√ß√£o de resposta, mas:
  - Modelo muito grande ou mem√≥ria insuficiente
  - Ou configura√ß√£o incorreta (query n√£o precisa de LLM, s√≥ embedding!)

**Para Busca Vetorial Simples:**
- ‚úÖ **S√≥ precisa:** Embedding model (nomic-embed-text) ‚Üí  Qdrant search
- ‚ùå **N√ÉO precisa:** LLM (llama3.1) para gera√ß√£o de texto

---

### 4. **Qdrant Status**

```bash
curl http://localhost:6333/collections/documentation__nomic
```

**Response:**
```json
{
  "result": {
    "points_count": 51940,
    "vectors_count": 0,
    "indexed_vectors_count": 51940,
    "status": "green"
  }
}
```

**An√°lise:**
- ‚úÖ **51,940 documentos indexados** na cole√ß√£o `documentation__nomic`
- ‚úÖ Status: green (saud√°vel)
- ‚úÖ Pronto para queries!

---

### 5. **Ollama Status**

```bash
curl http://localhost:11434/api/tags
```

**Modelos Dispon√≠veis:**
- `nomic-embed-text:latest` (embedding - 0GB RAM)
- `mxbai-embed-large:latest` (embedding - 0GB RAM)
- `llama3.1:latest` (LLM - 4GB RAM) ‚Üê Causando problema
- `embeddinggemma:latest` (embedding)

**GPU:**
- Model: NVIDIA GeForce RTX 5090
- Utilization: 0% (idle)
- Memory: 5,492 MB / 32,607 MB (17%)

---

## üêõ Problema Identificado

### **Root Cause: LLM N√£o Necess√°rio para Busca Vetorial**

O LlamaIndex Query Service est√° configurado para usar **LLM** (llama3.1) para:
- Gera√ß√£o de respostas (Q&A mode)
- Reranking de resultados
- Summarization

**Mas para busca vetorial simples, s√≥ precisamos:**
1. Embedding do query (via Ollama - nomic-embed-text)
2. Similarity search no Qdrant
3. Retornar top-k resultados

**Configura√ß√£o Atual (Problem√°tica):**
```yaml
# docker-compose.rag.yml
environment:
  - OLLAMA_MODEL=llama3.1  # ‚Üê Tentando usar LLM pesado!
```

---

## üí° Solu√ß√µes Propostas

### **Op√ß√£o A: Usar Apenas Embedding (Recomendado para MVP)** ‚≠ê

Configurar LlamaIndex para **vector search puro** (sem LLM):

```python
# tools/llamaindex/query_service/main.py (ajustar)

# N√ÉO usar LLM para query
# query_engine = index.as_query_engine(llm=ollama_llm)  ‚Üê REMOVER

# Usar apenas retriever (vector search puro)
retriever = index.as_retriever(
    similarity_top_k=top_k,
    vector_store_query_mode="default"
)

results = retriever.retrieve(query)
```

**Vantagens:**
- ‚úÖ Performance muito melhor (sem carregar LLM)
- ‚úÖ Menos mem√≥ria (s√≥ embedding - ~200MB)
- ‚úÖ Lat√™ncia < 1s
- ‚úÖ GPU foca em embeddings

**Desvantagens:**
- ‚ùå Sem gera√ß√£o de respostas (s√≥ retorna chunks)
- ‚ùå Sem reranking com LLM

---

### **Op√ß√£o B: Usar LLM Menor (llama3.2:3b)**

Trocar `llama3.1:latest` (7GB) por `llama3.2:3b` (2GB):

```yaml
environment:
  - OLLAMA_MODEL=llama3.2:3b  # Modelo menor
```

**Vantagens:**
- ‚úÖ Gera√ß√£o de respostas
- ‚úÖ Reranking com LLM
- ‚úÖ Cabe na mem√≥ria

**Desvantagens:**
- ‚ö†Ô∏è Ainda usa ~2GB RAM
- ‚ö†Ô∏è Lat√™ncia maior (~2-3s)

---

### **Op√ß√£o C: Criar Endpoint Separado (H√≠brido)**

Ter dois endpoints:

1. `POST /query/vector` ‚Üí Busca vetorial pura (r√°pida, sem LLM)
2. `POST /query/qa` ‚Üí Q&A com LLM (lenta, com gera√ß√£o)

**Frontend escolhe qual usar.**

---

## üéØ Recomenda√ß√£o

### **Para MVP: Op√ß√£o A (Vector Search Puro)** ‚≠ê

**Por qu√™:**
1. **Performance:** < 1s por query
2. **Simplicidade:** Menos configura√ß√£o
3. **Recursos:** N√£o precisa LLM pesado
4. **MVP suficiente:** Retornar chunks relevantes j√° resolve 80% dos casos

**Depois (v2):** Adicionar LLM para Q&A avan√ßado

---

## ‚úÖ Checklist de Corre√ß√£o

### Op√ß√£o A (Recomendada)
- [ ] Atualizar c√≥digo Python do LlamaIndex Query Service
- [ ] Remover uso de LLM em queries
- [ ] Usar apenas `retriever.retrieve()`
- [ ] Testar performance (< 1s)
- [ ] Rebuild container

### Op√ß√£o B (Alternativa)
- [ ] Pull modelo menor: `docker exec rag-ollama ollama pull llama3.2:3b`
- [ ] Atualizar `OLLAMA_MODEL` em docker-compose
- [ ] Restart container
- [ ] Testar mem√≥ria

---

## üìã Status Final do Health Check

### ‚úÖ Funcionando
- Container UP e healthy
- Qdrant com 51,940 vetores
- Ollama com modelos de embedding
- GPU RTX 5090 dispon√≠vel
- Swagger UI acess√≠vel

### ‚ö†Ô∏è Issues
- Query endpoint falha com LLM
- LLM process killed (OOM ou config)
- Sem endpoint de busca simples (s√≥ vector)

### üîß A√ß√£o Necess√°ria
- Configurar para vector search puro OU
- Usar LLM menor OU
- Criar endpoint sem LLM

---

**Status:** ‚úÖ Health Check Completo  
**Problema:** LLM causando falha em queries  
**Solu√ß√£o:** Vector search puro (sem LLM)  
**Pr√≥ximo:** Fase 2 - Implementa√ß√£o Backend  
**Tempo Gasto:** 10 minutos


