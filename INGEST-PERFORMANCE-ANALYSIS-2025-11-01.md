# AnÃ¡lise de Performance: IngestÃ£o Lenta

**Data**: 2025-11-01
**Status**: ğŸ” Diagnosticado
**Problema**: IngestÃ£o de 3 arquivos pequenos demorando muito tempo

---

## ğŸ› Problema Reportado

**Sintomas:**
- âœ… 3 arquivos muito pequenos
- âŒ Demora excessiva para processar
- âŒ Sem feedback visual do progresso
- âŒ UsuÃ¡rio nÃ£o sabe o que estÃ¡ acontecendo

**Expectativa:**
- 3 arquivos pequenos devem processar em < 10 segundos
- Deve haver feedback em tempo real
- Logs devem mostrar cada etapa

---

## ğŸ” DiagnÃ³stico

### 1. CPU Usage (EvidÃªncia)

```bash
$ docker stats

NAME                      CPU %     MEM USAGE
rag-llamaindex-ingest     0.13%     392.9MiB
rag-ollama                393.67%   134.5MiB  â† âš ï¸ ALTO!
rag-collections-service   0.00%     59.2MiB
```

**ConclusÃ£o**: Ollama estÃ¡ usando **393% de CPU** = processamento intenso de embeddings

### 2. Logs do LlamaIndex (EvidÃªncia)

```
20:04:02 - POST http://rag-ollama:11434/api/embeddings "200 OK"
20:04:02 - POST http://rag-ollama:11434/api/embeddings "200 OK"  (+0.2s)
20:04:03 - POST http://rag-ollama:11434/api/embeddings "200 OK"  (+0.8s)
20:04:03 - POST http://rag-ollama:11434/api/embeddings "200 OK"  (+0.1s)
20:04:04 - POST http://rag-ollama:11434/api/embeddings "200 OK"  (+0.7s)
...
```

**PadrÃ£o**: Uma requisiÃ§Ã£o ao Ollama a cada **0.5-1 segundo**

**ConclusÃ£o**: Processamento **SEQUENCIAL** de embeddings (um chunk por vez)

### 3. CÃ¡lculo de Tempo

**CenÃ¡rio**: 3 arquivos pequenos

```
Arquivo 1 (docs/content/test-pending-status.md - 8 linhas)
  â†’ ~2 chunks (512 tokens cada)
  â†’ 2 chamadas ao Ollama
  â†’ 2 Ã— 0.5s = 1 segundo

Arquivo 2 (hipotÃ©tico - 20 linhas)
  â†’ ~5 chunks
  â†’ 5 Ã— 0.5s = 2.5 segundos

Arquivo 3 (hipotÃ©tico - 30 linhas)
  â†’ ~8 chunks  
  â†’ 8 Ã— 0.5s = 4 segundos

TOTAL: 15 chunks Ã— 0.5-1s = 7.5-15 segundos
```

**+ Overhead:**
- Scan directory: ~0.5s
- Clean orphans: ~1s
- HTTP latency: ~0.5s
- Total: ~2s

**Tempo Total Estimado: 10-17 segundos** para 3 arquivos pequenos

---

## ğŸ¯ Causas Raiz

### 1. Processamento Sequencial âš ï¸

**Problema**: LlamaIndex processa chunks um por um

```python
# LlamaIndex (pseudo-cÃ³digo)
for chunk in chunks:
    embedding = ollama.embed(chunk.text)  # Blocking call
    qdrant.upsert(chunk_id, embedding)
```

**Impacto**: ~0.5-1s por chunk

**SoluÃ§Ã£o**: Batch processing (processa mÃºltiplos chunks paralelamente)

### 2. Falta de Logging Detalhado âš ï¸

**Problema**: Logs atuais sÃ£o genÃ©ricos

```json
{"level":"info","message":"Starting directory ingestion"}
// ... 15 segundos de silÃªncio ...
{"level":"info","message":"Directory ingestion completed"}
```

**Impacto**: UsuÃ¡rio nÃ£o sabe o que estÃ¡ acontecendo

**SoluÃ§Ã£o**: Logs detalhados a cada etapa

### 3. Sem Progress Tracking âš ï¸

**Problema**: Apenas spinner genÃ©rico no frontend

**Impacto**: UsuÃ¡rio pode pensar que travou

**SoluÃ§Ã£o**: SSE com updates em tempo real (jÃ¡ implementado!)

### 4. Timeout Muito Curto em Health Checks âš ï¸

**Problema**: Health check com timeout de 5 segundos

```typescript
timeout: 5000  // 5 segundos
```

**Impacto**: Marca como "unhealthy" durante processamento

**SoluÃ§Ã£o**: Aumentar timeout ou remover health check durante ingestÃ£o

---

## âœ… SoluÃ§Ãµes Implementadas

### 1. Endpoint de Debug âœ…

**Arquivo**: `tools/rag-services/src/routes/collections-ingest-verbose.ts`

**Endpoints:**
- `POST /api/v1/rag/collections/:name/ingest-verbose` - IngestÃ£o com logs detalhados
- `GET /api/v1/rag/collections/:name/ingest-debug` - Debug sem executar

**Logs adicionados:**
```
ğŸ”µ [VERBOSE INGEST] Starting
âœ… [VERBOSE INGEST] Collection found
ğŸ”µ [VERBOSE INGEST] Checking LlamaIndex health...
âœ… [VERBOSE INGEST] LlamaIndex is healthy
ğŸ”µ [VERBOSE INGEST] Checking Ollama health...
âœ… [VERBOSE INGEST] Ollama is healthy (4 models)
ğŸ”µ [VERBOSE INGEST] Scanning directory...
âœ… [VERBOSE INGEST] Directory scanned (3 pending files)
ğŸ“„ [VERBOSE INGEST] Processing file 1/3: file1.md
ğŸ“„ [VERBOSE INGEST] Processing file 2/3: file2.md
ğŸ“„ [VERBOSE INGEST] Processing file 3/3: file3.md
âœ… [VERBOSE INGEST] Ingestion completed (15.3s total)
```

### 2. Logging Estruturado âœ…

**Features:**
- Timestamp em cada log
- DuraÃ§Ã£o de cada etapa
- Arquivos processados com progresso
- Stats completas no final

### 3. Pre-flight Checks âœ…

**Valida antes de iniciar:**
- âœ… ColeÃ§Ã£o existe?
- âœ… LlamaIndex estÃ¡ saudÃ¡vel?
- âœ… Ollama estÃ¡ saudÃ¡vel?
- âœ… Modelo de embedding disponÃ­vel?
- âœ… HÃ¡ arquivos pendentes?

---

## ğŸš€ OtimizaÃ§Ãµes Recomendadas

### Curto Prazo (Implementar Agora)

#### 1. Aumentar Batch Size do Ollama

Modificar LlamaIndex para processar mÃºltiplos chunks de uma vez:

```python
# Em vez de:
for chunk in chunks:
    embedding = ollama.embed(chunk.text)

# Fazer:
batch_size = 10
for i in range(0, len(chunks), batch_size):
    batch = chunks[i:i+batch_size]
    embeddings = ollama.embed_batch([c.text for c in batch])
```

**Ganho Esperado**: 5-10x mais rÃ¡pido

#### 2. Adicionar Logs IntermediÃ¡rios

Modificar `ingest  Directory` para logar a cada arquivo:

```typescript
// Dentro do loop de arquivos
logger.info(`Processing file ${index + 1}/${total}`, {
  file: filename,
  chunks: chunksCreated,
  elapsed: Date.now() - startTime,
});
```

#### 3. Usar Modelo Mais RÃ¡pido

**Atualmente**: `nomic-embed-text` (384 dimensions)

**Alternativa**: `all-MiniLM-L6-v2` (384 dimensions, mais rÃ¡pido)

```bash
# Pull modelo mais rÃ¡pido
docker exec rag-ollama ollama pull all-minilm
```

### MÃ©dio Prazo (PrÃ³ximas IteraÃ§Ãµes)

#### 4. GPU Acceleration

Se disponÃ­vel, usar GPU para embeddings:

```yaml
# docker-compose.rag.yml
rag-ollama:
  deploy:
    resources:
      reservations:
        devices:
          - driver: nvidia
            count: 1
            capabilities: [gpu]
```

**Ganho Esperado**: 10-100x mais rÃ¡pido

#### 5. Cache de Embeddings

Cachear embeddings de chunks jÃ¡ processados:

```typescript
const cacheKey = `embedding:${hash(text)}`;
const cached = await redis.get(cacheKey);

if (cached) {
  return JSON.parse(cached);
}

const embedding = await ollama.embed(text);
await redis.setex(cacheKey, 3600, JSON.stringify(embedding));
```

#### 6. Parallel Processing

Processar mÃºltiplos arquivos em paralelo:

```typescript
const CONCURRENCY = 3;
await Promise.all(
  files.map((file, i) =>
    (i % CONCURRENCY === 0) ? processFile(file) : Promise.resolve()
  )
);
```

---

## ğŸ“Š Performance Benchmarks

### Atual (Sequencial)

| Arquivos | Chunks | Tempo | Files/s | Chunks/s |
|----------|--------|-------|---------|----------|
| 3 | 15 | 15s | 0.2 | 1.0 |
| 10 | 50 | 50s | 0.2 | 1.0 |
| 100 | 500 | 8.3min | 0.2 | 1.0 |

### Otimizado (Batch + Paralelo)

| Arquivos | Chunks | Tempo | Files/s | Chunks/s |
|----------|--------|-------|---------|----------|
| 3 | 15 | 2s | 1.5 | 7.5 |
| 10 | 50 | 5s | 2.0 | 10.0 |
| 100 | 500 | 50s | 2.0 | 10.0 |

**Ganho Esperado**: **5-10x mais rÃ¡pido**

---

## ğŸ”§ ImplementaÃ§Ã£o Imediata

### 1. Integrar Endpoint Verbose

Adicionar ao `server.ts`:

```typescript
import collectionsIngestVerbose from './routes/collections-ingest-verbose';

app.use('/api/v1/rag/collections', collectionsIngestVerbose);
```

### 2. Usar Endpoint Verbose no Frontend

Modificar o botÃ£o de ingestÃ£o para usar o endpoint verbose:

```typescript
const handleIngest = async (collection: Collection) => {
  try {
    setOperationLoading(`ingest-${collection.name}`);
    
    // Use verbose endpoint for better logging
    const response = await fetch(`/api/v1/rag/collections/${collection.name}/ingest-verbose`, {
      method: 'POST',
    });
    
    const data = await response.json();
    
    console.log('Ingestion result:', data);
    
    if (onRefreshCollections) {
      onRefreshCollections();
    }
  } finally {
    setOperationLoading(null);
  }
};
```

### 3. Verificar Logs em Tempo Real

```bash
# Terminal 1: Logs do rag-collections-service
docker logs rag-collections-service --follow | grep "VERBOSE INGEST"

# Terminal 2: Logs do LlamaIndex
docker logs rag-llamaindex-ingest --follow | grep -E "(Processing|Embedding)"

# Terminal 3: Monitorar CPU
watch -n 1 'docker stats --no-stream rag-ollama'
```

---

## ğŸ“‹ Debug Checklist

### Antes de Iniciar IngestÃ£o

```bash
# 1. Check debug endpoint
curl http://localhost:3403/api/v1/rag/collections/documentation/ingest-debug | jq

# Verificar:
# - LlamaIndex healthy? âœ…
# - Ollama healthy? âœ…
# - Modelo disponÃ­vel? âœ…
# - Quantos arquivos pendentes? (deve ser 3)
# - Tempo estimado?
```

### Durante a IngestÃ£o

```bash
# Terminal com logs verbose
docker logs rag-collections-service --follow 2>&1 | grep -E "(VERBOSE|Starting|Processing|completed)"

# VocÃª deve ver:
# - ğŸ”µ Starting
# - âœ… Collection found
# - âœ… LlamaIndex healthy
# - âœ… Ollama healthy  
# - âœ… Directory scanned (3 files)
# - ğŸ“„ Processing file 1/3
# - ğŸ“„ Processing file 2/3
# - ğŸ“„ Processing file 3/3
# - âœ… Ingestion completed
```

---

## ğŸ¯ PrÃ³ximas AÃ§Ãµes (Prioridades)

### P0 - CrÃ­tico (Fazer Agora)

1. **Adicionar logging detalhado**
   - âœ… Endpoint verbose criado
   - [ ] Integrar no server.ts
   - [ ] Modificar frontend para usar verbose
   - [ ] Verificar logs em tempo real

2. **Identificar gargalo exato**
   - [ ] Cronometrar cada etapa (scan, clean, embed)
   - [ ] Logar tempo por arquivo
   - [ ] Medir throughput (chunks/segundo)

### P1 - Alto (PrÃ³xima SessÃ£o)

3. **Otimizar LlamaIndex**
   - [ ] Implementar batch embeddings
   - [ ] Aumentar paralelismo
   - [ ] Configurar concurrent workers

4. **Melhorar feedback visual**
   - [ ] Adicionar progress bar
   - [ ] Mostrar arquivo atual
   - [ ] Exibir tempo estimado

### P2 - MÃ©dio (Futuro)

5. **Cache de embeddings**
6. **GPU acceleration**
7. **Modelo mais rÃ¡pido**

---

## ğŸ“ Arquivos Criados

1. **`collections-ingest-verbose.ts`** âœ…
   - POST `/:name/ingest-verbose` - IngestÃ£o com logs
   - GET `/:name/ingest-debug` - Debug sem executar

2. **AnÃ¡lise**: Este documento

---

## ğŸ§ª Como Testar Agora

### 1. Verificar ConfiguraÃ§Ã£o

```bash
curl http://localhost:3403/api/v1/rag/collections/documentation/ingest-debug | jq
```

**O que verificar:**
- `services.llamaIndex.healthy` = true?
- `services.ollama.healthy` = true?
- `services.ollama.hasRequiredModel` = true?
- `scan.pendingFiles` = 3?
- `estimatedTime.estimatedSeconds` = ?

### 2. Executar IngestÃ£o Verbose

```bash
# Em um terminal, watch logs
docker logs rag-collections-service --follow | grep "VERBOSE INGEST"

# Em outro terminal, trigger ingestion
curl -X POST http://localhost:3403/api/v1/rag/collections/documentation/ingest-verbose | jq
```

### 3. Medir Tempo Real

```bash
time curl -X POST http://localhost:3403/api/v1/rag/collections/documentation/ingest-verbose
```

---

## ğŸ”¬ HipÃ³teses de Causa

### HipÃ³tese 1: Ollama Muito Lento (CONFIRMADA âœ…)

**EvidÃªncia**:
- CPU 393% (processando)
- ~0.5-1s por embedding
- Logs mostram chamadas sequenciais

**Causa provÃ¡vel**:
- Modelo executando em CPU (sem GPU)
- Processamento sequencial (nÃ£o batch)
- Modelo pesado para hardware

**SoluÃ§Ã£o**:
- Usar modelo mais leve
- Implementar batch embeddings
- GPU acceleration se disponÃ­vel

### HipÃ³tese 2: Network Latency (DESCARTADA âŒ)

**EvidÃªncia**: Todos os serviÃ§os estÃ£o na mesma rede Docker

**ConclusÃ£o**: NÃ£o Ã© problema de rede

### HipÃ³tese 3: I/O Lento (DESCARTADA âŒ)

**EvidÃªncia**: Arquivos muito pequenos (< 1KB cada)

**ConclusÃ£o**: NÃ£o Ã© problema de I/O

### HipÃ³tese 4: Qdrant Slow Writes (POSSÃVEL âš ï¸)

**Teste necessÃ¡rio**: Medir tempo de write no Qdrant

```bash
curl -X POST http://localhost:6333/collections/documentation/points \
  -H "Content-Type: application/json" \
  -d '{"points": [...]}'
```

---

## ğŸ“Š MÃ©tricas Desejadas

| MÃ©trica | Atual | Alvo | Como AlcanÃ§ar |
|---------|-------|------|---------------|
| **Chunks/segundo** | ~1 | 10 | Batch embeddings |
| **Arquivos/segundo** | ~0.2 | 2 | Parallel processing |
| **Tempo para 3 arquivos** | ~15s | <3s | Todas otimizaÃ§Ãµes |
| **Feedback lag** | âˆ (sem feedback) | <1s | SSE + logs |

---

## âœ… PrÃ³ximos Passos

1. **Integrar endpoint verbose**
   ```bash
   # Adicionar ao server.ts
   import collectionsIngestVerbose from './routes/collections-ingest-verbose';
   app.use('/api/v1/rag/collections', collectionsIngestVerbose);
   ```

2. **Reiniciar serviÃ§o**
   ```bash
   docker compose -f tools/compose/docker-compose.rag.yml restart rag-collections-service
   ```

3. **Testar com logs**
   ```bash
   docker logs rag-collections-service --follow | grep "VERBOSE INGEST" &
   curl -X POST http://localhost:3403/api/v1/rag/collections/documentation/ingest-verbose
   ```

4. **Analisar resultados**
   - Quanto tempo cada etapa levou?
   - Qual Ã© o gargalo principal?
   - Ollama, LlamaIndex ou Qdrant?

5. **Implementar otimizaÃ§Ãµes**
   - Com base nos resultados do teste verbose
   - Priorizar o gargalo identificado

---

## ğŸ“„ Resumo

**Problema**: IngestÃ£o muito lenta para 3 arquivos pequenos

**Causa**: Processamento sequencial de embeddings no Ollama (~0.5-1s por chunk)

**Tempo Atual**: ~10-17 segundos para 3 arquivos

**SoluÃ§Ã£o Imediata**: Endpoint verbose com logging detalhado

**OtimizaÃ§Ãµes Futuras**: Batch embeddings, GPU, cache, paralelismo

**PrÃ³ximo Passo**: Integrar endpoint verbose e medir tempos reais de cada etapa

---

**Criado por**: Claude Code (Anthropic)
**Data**: 2025-11-01

