@startuml
!theme plain
skinparam sequenceMessageAlign center
skinparam ParticipantPadding 20
skinparam BoxPadding 10
skinparam defaultFontName Courier
skinparam defaultFontSize 12

title Firecrawl Proxy - Scrape Request Flow

actor User
participant "Dashboard\n(React)" as Dashboard
participant "firecrawlService" as Service
participant "Firecrawl Proxy\n(Express)" as Proxy
participant "Validation" as Validation
participant "Rate Limiter" as RateLimit
participant "FirecrawlService" as ProxyService
participant "Firecrawl Core\n(Port 3002)" as Firecrawl
participant "Prometheus" as Metrics
participant "Logger" as Logger

== Successful Scrape ==
User -> Dashboard : Enter URL & formats
Dashboard -> Service : useScrape.mutate(options)
Service -> Proxy : POST /api/v1/scrape
Proxy -> Validation : validate request
Validation -> Validation : check URL, formats, limits
alt Validation passes
  Validation -> RateLimit : verify 100 req/min
  RateLimit -> RateLimit : track IP window
  alt Within limit
    RateLimit -> ProxyService : forward request
    ProxyService -> Firecrawl : POST /v1/scrape
    activate Firecrawl
    Firecrawl -> Firecrawl : render via Playwright
    Firecrawl --> ProxyService : scraped data
    deactivate Firecrawl
    ProxyService -> Metrics : observe scrape success
    ProxyService -> Logger : log completion
    ProxyService --> Proxy : JSON payload
    Proxy --> Service : 200 OK (markdown, metadata)
    Service --> Dashboard : resolve mutation
    Dashboard --> User : display markdown preview
  else Rate limit exceeded
    RateLimit --> Proxy : 429 Too Many Requests
    Proxy --> Service : error response + Retry-After
    Service --> Dashboard : show rate limit toast
  end
else Validation fails
  Validation --> Proxy : 400 Bad Request + details
  Proxy --> Service : validation errors
  Service --> Dashboard : surface form errors
end

== Firecrawl Error ==
ProxyService -> Firecrawl : POST /v1/scrape
Firecrawl --> ProxyService : ECONNREFUSED
ProxyService -> Metrics : record scrape failure
ProxyService -> Logger : log error context
ProxyService --> Proxy : propagate 503 error
Proxy --> Service : 503 Service Unavailable
Service --> Dashboard : show error toast

== Crawl Job with Polling ==
User -> Dashboard : Start crawl job
Dashboard -> Service : useCrawl.mutate(options)
Service -> Proxy : POST /api/v1/crawl
Proxy -> ProxyService : forward crawl request
ProxyService -> Firecrawl : POST /v1/crawl
Firecrawl --> ProxyService : job id
ProxyService --> Proxy : 200 OK { id, url }
Proxy --> Service : mutation success
Service --> Dashboard : persist job (localStorage)
Dashboard --> User : toast "Crawl started"

loop Poll every 5 seconds while status == "scraping"
  Dashboard -> Service : useCrawlStatus(jobId)
  Service -> Proxy : GET /api/v1/crawl/:id
  Proxy -> ProxyService : fetch status
  ProxyService -> Firecrawl : GET /v1/crawl/:id
  Firecrawl --> ProxyService : { status, completed, total }
  ProxyService -> Metrics : record status check
  ProxyService --> Proxy : status payload
  Proxy --> Service : current progress
  Service --> Dashboard : update UI (completed/total)
end

Firecrawl --> ProxyService : status "completed" + results
ProxyService --> Proxy : final payload with documents
Proxy --> Service : success response
Service --> Dashboard : stop polling, render pages
Dashboard --> User : show crawl results

note over Validation,RateLimit
  Validates http/https URL (<= 2048 chars),
  formats array, numeric limits, timeouts,
  then enforces 100 requests/minute per IP.
end note

note over ProxyService,Metrichs
  Metrics captured:
    - scrape_total & duration
    - crawl_jobs_total
    - crawl_status_checks_total
  Logger stores structured JSON events.
end note

@enduml
