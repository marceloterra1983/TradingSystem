---
title: PRD Prometheus Monitoring
sidebar_position: 60
tags:
    - prd
    - monitoring
    - prometheus
domain: shared
type: prd
summary: Requirements to implement the monitoring platform with Prometheus and integrations into TradingSystem
status: active
last_review: 2025-10-17
language: en
translated_from: ../pt/monitoramento-prometheus-prd.md
translated_at: "2025-10-10T03:28:51.550251+00:00"
translation_source_hash: be2e915bdeab56b63af281564213da844cfad801
translation_source_language: pt
---

# PRD: Monitoring with Prometheus - TradingSystem

**Status:** Draft  
**Version:** 0.1.0  
**Last Update:** 2025-10-10  
**Author:** Marcelo Terra  
**Stakeholders:** Backend Engineering, Frontend Engineering, Operations, SRE

---

## Overview

TradingSystem needs continuous visibility into the health of its services to support availability targets and detect regressions before they affect trading operations. Prometheus will be adopted as the standard solution for collecting, storing and querying metrics, complemented by dashboards and alerts in Grafana. This PRD outlines requirements to implement Prometheus as an internally managed platform, automate instrumentation of core services (Idea Bank, Documentation API, Dashboard), and standardize operational alerts.

## Problem and Opportunity

**Current issues**

-   Manual monitoring with ad hoc scripts and little standardization of metrics.
-   No centralized history of latency, throughput and errors for Node/React services.
-   Low visibility of infrastructure resources (CPU, memory, disk space).
-   Alerts depend on human checks; Incidents are detected late.

**Opportunities**

-   Standardize telemetry and business metrics to accelerate diagnosis.
-   Reduce MTTR with automated alerts and shareable dashboards.
-   Establish basis for SLO/SLA targets and regulatory audits.
-   Reuse exporters and dashboards for new microservices.

## Objectives and KPIs

| Objective                                | Metric/KPI                                           | Goal            |
| ---------------------------------------- | ---------------------------------------------------- | --------------- |
| Reduce average incident detection time   | Critical Incident MTTD                               | < 5 minutes     |
| Increase visibility of the Idea Bank API | Coverage of instrumented endpoints                   | 95%             |
| Instrument critical infrastructure       | Percentage of hosts with node_exporter               | 100%            |
| Automate alerts                          | Percentage of alerts generated by Prometheus/Grafana | 90%             |
| Adherence to collection standards        | Services that expose `/metrics` with documentation   | 3 main services |

## Scope and Out of Scope

**Scope**

-   Prometheus server provisioning (Windows Service or dedicated Linux container).
-   Configuration of local storage with minimum retention of 30 days.
-   Deploy `node_exporter` and `windows_exporter` according to the environment.
-   Native instrumentation of Node.js services (Idea Bank, Documentation API) via `prom-client`.
-   Creation of Grafana-based dashboards connected to Prometheus.
-   Definition and initial implementation of alerts (latency, 5xx errors, unavailability of exporters).
-   Complete installation, operation and troubleshooting documentation.

**Out of Scope**

-   Migration to external observability stack (Datadog, New Relic).
-   Collection of distributed logs or traces (will be treated in a separate project).
-   Configuration of mobile alerts or integrations with specific non-standard tools.

## Functional Requirements

1. **Metrics Collection**

    - RF1.1: Prometheus must collect metrics every 15 seconds from instrumented services.
    - RF1.2: Each Node service exposes `/metrics` with standard HTTP metrics (latency, count by status).
    - RF1.3: Operating system exporters published on configurable port, protected by ACL.

2. **Dashboards and visualization**

    - RF2.1: Grafana provides an operational dashboard with an overview (uptime, latency, errors).
    - RF2.2: Service-specific dashboard (Idea Bank, Documentation API) with business metrics.
    - RF2.3: Infrastructure dashboard with CPU, memory, disk, network.

3. **Alerts and notifications**

    - RF3.1: Alertmanager configures alert rules for scraping unavailability (> 2 intervals).
    - RF3.2: Critical alerts send notification to the `#ops-alerts` channel on Slack.
    - RF3.3: Warning alerts register automatic tickets in the internal system (integration with GitHub Issues).

4. **Operation and automation**
    - RF4.1: PowerShell/Make scripts to install and update Prometheus/Exporters.
    - RF4.2: CI pipelines check the presence of Prometheus instrumentation in new Node services.
    - RF4.3: Documentation in `docs/ops/monitoring/prometheus-setup.md` reflects the current state of the system.

## Non-Functional Requirements

-   **Availability:** Prometheus should automatically restart and maintain state after reboot.
-   **Performance:** Dashboard query must respond < 5s for windows of up to 24 hours.
-   **Retention:** Save metrics for 30 days with standard compression without exceeding 10 GB.
-   **Security:** Access to the Grafana dashboard and metrics requires authentication (GitHub SSO) and HTTPS.
-   **Observability of Prometheus itself:** Exporters must monitor scraping, disk usage and alerts.
-   **Translation reliability:** Document metric names and labels to avoid ambiguities between teams.

## Architecture and Integrations

```mermaid
flowchart LR
    subgraph Services
        IBAPI[Idea Bank API<br/>Node + Express]
        DOCAPI[Documentation API<br/>Node + Express]
        DASH[Dashboard React<br/>Custom metrics via backend]
    end

    subgraph Exporters
        NODE[node_exporter<br/>(Linux hosts)]
        WIN[windows_exporter<br/>(Windows hosts)]
    end

    Prometheus[(Prometheus Server)]
    Alertmanager[(Alertmanager)]
    Grafana[(Grafana Dashboards)]
    Slack[(Slack #ops-alerts)]
    Issues[(GitHub Issues)]

    IBAPI -->|/metrics| Prometheus
    DOCAPI -->|/metrics| Prometheus
    DASH -->|/metrics backend| Prometheus
    NODE --> Prometheus
    WIN --> Prometheus

    Prometheus --> Alertmanager --> Slack
    Alertmanager --> Issues
    Prometheus --> Grafana
```

**Key Integrations**

-   Node services depend on the `prom-client` library and middleware for HTTP metrics.
-   Future .NET backends should use `prometheus-net`.
-   Grafana uses code provisioning (`.json`) versioned in the repository.
-   Alertmanager integrates with Slack via webhook and with GitHub via custom Action.

## Operational Flow

1. **Provisioning:** Script installs Prometheus, Alertmanager and exporters, loading `prometheus.yml`.
2. **Instrumentation:** Backend team adds metrics middleware and automated testing.
3. **Validation:** QA runs a checklist that includes checking series in Prometheus and dashboards.
4. **Continuous observation:** Ops monitors dashboards; Critical alerts are handled via runbooks.
5. **Periodic review:** Each sprint, review relevant metrics and adjust alert limits.

## Schedule and Milestones

| Milestone                     | Description                                      | Responsible    | Target Date |
| ----------------------------- | ------------------------------------------------ | -------------- | ----------- |
| Kickoff                       | Approve final architecture and installation plan | Ops & Backend  | 2025-10-15  |
| Idea Bank Instrumentation     | `/metrics` with HTTP and domain metrics          | Backend        | 2025-10-22  |
| Deploy Prometheus + Exporters | Monitored production environment                 | Oops           | 2025-10-29  |
| Grafana starter dashboards    | Latency, errors, infra                           | Ops & Frontend | 2025-11-01  |
| Active critical alerts        | Slack + GitHub working                           | Oops           | 2025-11-05  |
| Post-deployment review        | Success metrics and improvements                 | Eng. + Ops     | 2025-11-12  |

## Risks and Mitigations

| Risk                                           | Impact                 | Mitigation                                                 |
| ---------------------------------------------- | ---------------------- | ---------------------------------------------------------- |
| Lack of storage capacity                       | Loss of history        | Monitor usage and configure disk compression/expansion.    |
| Noisy alerts                                   | Operational fatigue    | Adjust thresholds and use SLO/SLA-based alerts.            |
| Outdated exporters                             | Collection failures    | Automate updates via scripts and version checks.           |
| Inconsistent metrics across services           | Difficulty of analysis | Set naming convention and automatic lint.                  |
| Dependency on external services (Slack/GitHub) | Undelivered alerts     | Configure fallback via email and record Alertmanager logs. |

## Open Questions

1. What will be the definitive infrastructure model? (Windows Service vs Docker on Linux host)
2. Is there a retention requirement longer than 30 days for audits?
3. Need for multi-environment support (dev/staging/prod) in the first release?
4. Integration with internal ticketing system other than GitHub?
5. Does the Dashboard React dashboard need to expose client-side metrics (web vitals)?
