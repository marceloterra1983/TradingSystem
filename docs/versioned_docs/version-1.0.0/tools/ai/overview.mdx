---
title: AI & LLM Tools
description: AI agents, LangGraph workflows, LlamaIndex RAG, and local LLM runtime (Ollama)
tags:
  - ai
  - llm
  - agents
  - langgraph
  - llamaindex
  - ollama
  - rag
owner: ArchitectureGuild
lastReviewed: '2025-10-27'
---

# AI & LLM Tools

AI/ML infrastructure for building intelligent agents, multi-step workflows, RAG systems, and local LLM inference.

## ðŸ¤– Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Agno Agents    â”‚     â”‚   LangGraph     â”‚     â”‚  LlamaIndex     â”‚
â”‚  (Clean Arch)   â”‚     â”‚  (Workflows)    â”‚     â”‚     (RAG)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â–¼
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚     Ollama      â”‚
                         â”‚  (Local LLMs)   â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸ› ï¸ Components

### 1. Agno Agents (`tools/agno-agents/`)

**AI agents built with Clean Architecture and Domain-Driven Design principles.**

**Architecture:**

```
agno-agents/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ application/       # Use cases and orchestration
â”‚   â”œâ”€â”€ domain/            # Business logic and entities
â”‚   â”œâ”€â”€ infrastructure/    # External integrations (LLMs, APIs)
â”‚   â””â”€â”€ interfaces/        # API endpoints and CLI
â”œâ”€â”€ tests/                 # Unit and integration tests
â””â”€â”€ Dockerfile             # Containerized deployment
```

**Purpose:**
- Automated workflows and task automation
- Intelligent decision-making agents
- Multi-agent collaboration
- Clean separation of concerns (business logic isolated from infrastructure)

**Key Features:**
- Domain-Driven Design (Entities, Value Objects, Aggregates)
- Clean Architecture (dependencies point inward)
- Testable business logic (domain layer has no external dependencies)
- Pluggable infrastructure (swap LLM providers without changing domain code)

**Example Use Cases:**
- Data ingestion automation
- Alert triage and routing
- Report generation
- Trading signal validation

---

### 2. LangGraph (`tools/langgraph/`)

**Stateful, multi-agent AI applications with graph-based workflows.**

**Architecture:**

```
langgraph/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ application/       # LangGraph workflow definitions
â”‚   â”œâ”€â”€ domain/            # Business entities and rules
â”‚   â”œâ”€â”€ infrastructure/    # LLM providers, vector stores
â”‚   â”œâ”€â”€ interfaces/        # API and CLI
â”‚   â””â”€â”€ monitoring/        # Observability for LangGraph
â”œâ”€â”€ langgraph.json         # LangGraph Studio config
â””â”€â”€ Dockerfile             # Containerized deployment
```

**Purpose:**
- Complex multi-step AI workflows
- Agent orchestration with state management
- Conditional workflow branching
- Human-in-the-loop workflows

**Key Features:**
- Graph-based workflow definition (nodes = agents, edges = transitions)
- State persistence (checkpointing)
- Cycle detection and loop prevention
- Parallel agent execution
- LangGraph Studio integration (visual workflow debugging)

**Example Use Cases:**
- Market analysis pipeline (data â†’ analysis â†’ validation â†’ decision)
- Customer support automation (intake â†’ triage â†’ resolution â†’ follow-up)
- Document processing workflows (extract â†’ validate â†’ enrich â†’ store)

**Documentation:**
- `QUICK_START.md` - Getting started guide
- `DEVELOPMENT.md` - Development workflow
- `ENV_VARS.md` - Environment variables

---

### 3. LlamaIndex (`tools/llamaindex/`)

**RAG (Retrieval-Augmented Generation) implementation for document Q&A and semantic search.**

**Architecture:**

```
llamaindex/
â”œâ”€â”€ ingestion_service/     # Document ingestion and indexing
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ ingestion/    # Document loaders and transformers
â”‚   â”‚   â”œâ”€â”€ indexing/     # Vector index builders
â”‚   â”‚   â””â”€â”€ storage/      # Persistence layer
â”‚   â””â”€â”€ Dockerfile.ingestion
â”œâ”€â”€ query_service/         # Query processing and retrieval
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ retrieval/    # Semantic search and ranking
â”‚   â”‚   â”œâ”€â”€ generation/   # LLM-based response generation
â”‚   â”‚   â””â”€â”€ api/          # REST API endpoints
â”‚   â””â”€â”€ Dockerfile.query
â”œâ”€â”€ k8s/                   # Kubernetes deployments
â”‚   â”œâ”€â”€ production/
â”‚   â””â”€â”€ staging/
â””â”€â”€ tests/                 # Service tests
```

**Purpose:**
- Document Q&A (ask questions about uploaded documents)
- Semantic search (find relevant documents by meaning, not keywords)
- Knowledge retrieval (extract information from large document corpora)

**Key Features:**
- Multi-format support (PDF, DOCX, Markdown, HTML, etc.)
- Vector embeddings for semantic similarity
- Hybrid search (semantic + keyword)
- Query rewriting and expansion
- Source citation (responses include document references)

**Example Use Cases:**
- Trading documentation Q&A ("How do I configure ProfitDLL?")
- Regulatory compliance search ("Find all documents mentioning risk limits")
- Market research assistant ("Summarize recent reports on energy sector")

**Deployment:**
- **Ingestion Service** - Background job for document processing
- **Query Service** - REST API for search and Q&A

**Documentation:**
- `DEPLOYMENT.md` - Deployment guide (Docker, Kubernetes)

---

### 4. Ollama (`tools/ollama/`)

**Local LLM runtime for running open-source models without cloud dependencies.**

**Supported Models:**
- **Llama 2** (7B, 13B, 70B) - General-purpose chat
- **Mistral** (7B) - Fast, high-quality responses
- **CodeLlama** (7B, 13B, 34B) - Code generation and completion
- **Gemma** (2B, 7B) - Google's lightweight models
- **Phi-2** (2.7B) - Microsoft's small model

**Purpose:**
- Local model inference (no API costs, data stays local)
- Development without cloud dependencies
- Data privacy and compliance
- Experimentation with open-source models

**Key Features:**
- GPU acceleration (CUDA, ROCm)
- Model quantization (smaller models, faster inference)
- Multi-model support (run multiple models concurrently)
- OpenAI-compatible API (drop-in replacement for OpenAI SDK)

**Setup:**

```bash
# Install Ollama (Docker + GPU)
bash tools/ollama/setup-ollama-docker-gpu.sh

# Pull models
docker exec ollama ollama pull llama2
docker exec ollama ollama pull mistral
docker exec ollama ollama pull codellama

# Test all models
bash tools/ollama/test-all-models.sh
```

**Usage (OpenAI-compatible API):**

```python
from openai import OpenAI

client = OpenAI(
    base_url="http://localhost:11434/v1",
    api_key="ollama"  # not used but required
)

response = client.chat.completions.create(
    model="llama2",
    messages=[{"role": "user", "content": "Explain Clean Architecture"}]
)
print(response.choices[0].message.content)
```

**Documentation:**
- `USAGE-GUIDE.md` - Ollama usage guide
- `setup-ollama-docker-gpu.sh` - GPU setup script
- `test-all-models.sh` - Model testing script

---

## ðŸš€ Quick Start

### Start AI Services

```bash
# Start Ollama (local LLM runtime)
docker compose -f tools/ollama/docker-compose.yml up -d

# Start LangGraph (workflows)
docker compose -f tools/langgraph/docker-compose.yml up -d

# Start LlamaIndex (RAG)
docker compose -f tools/llamaindex/docker-compose.yml up -d

# Start Agno Agents
docker compose -f tools/agno-agents/docker-compose.yml up -d
```

### Test Ollama

```bash
# Pull and test Llama 2
docker exec ollama ollama pull llama2
docker exec ollama ollama run llama2 "Hello, what can you do?"

# Test all models
bash tools/ollama/test-all-models.sh
```

### Access Services

| Service | URL | Purpose |
|---------|-----|---------|
| Ollama API | http://localhost:11434 | Local LLM inference |
| LangGraph API | http://localhost:8000 | Workflow execution |
| LlamaIndex Query | http://localhost:8001 | Document Q&A |
| LlamaIndex Ingestion | http://localhost:8002 | Document upload |

### GPU Coordination Defaults

LlamaIndex services serialize GPU workloads via a shared semaphore and a filesystem lock (`/tmp/llamaindex-gpu.lock`). Tweak behaviour with the following environment variables:

- `LLAMAINDEX_FORCE_GPU` / `LLAMAINDEX_GPU_NUM` â€“ force GPU execution and set GPU count per request.
- `LLAMAINDEX_GPU_MAX_CONCURRENCY` â€“ cap concurrent GPU jobs (defaults to `1`).
- `LLAMAINDEX_GPU_COOLDOWN_SECONDS` â€“ optional pause between jobs.
- `LLAMAINDEX_GPU_USE_FILE_LOCK` / `LLAMAINDEX_GPU_LOCK_PATH` â€“ cross-process lock consumed by Python services and Node scripts.

---

## ðŸ“– Integration Examples

### Use Ollama with Agno Agents

```python
# agno-agents/src/infrastructure/llm/ollama_provider.py
from openai import OpenAI

class OllamaProvider:
    def __init__(self):
        self.client = OpenAI(
            base_url="http://localhost:11434/v1",
            api_key="ollama"
        )

    def generate(self, prompt: str, model: str = "llama2") -> str:
        response = self.client.chat.completions.create(
            model=model,
            messages=[{"role": "user", "content": prompt}]
        )
        return response.choices[0].message.content
```

### Use LlamaIndex for RAG

```python
# query_service/src/retrieval/search.py
from llama_index import VectorStoreIndex, SimpleDirectoryReader

# Load documents
documents = SimpleDirectoryReader("docs/").load_data()

# Create index
index = VectorStoreIndex.from_documents(documents)

# Query
query_engine = index.as_query_engine()
response = query_engine.query("How do I configure ProfitDLL?")
print(response.response)
print(response.source_nodes)  # Source citations
```

### Ingest the Full TradingSystem Repository

Trigger a repository-wide ingestion when you need a collection that covers every project doc, spec, and code file:

```bash
# 1. Ensure the RAG stack (Ollama + LlamaIndex) is running
docker compose -f tools/compose/docker-compose.rag.yml up -d

# 2. Build or refresh the collection with the helper script
bash scripts/rag/ingest-tradingsystem.sh \
  --collection tradingsystem \
  --allowed '*' \
  --exclude '.git,.github,.idea,.vscode,node_modules,dist,build,.cache,.next' \
  --max-mb 16

# Optional: override the directory exposed inside the ingestion container
# bash scripts/rag/ingest-tradingsystem.sh --dir /data/custom-path
```

The script targets the ingestion service (`http://localhost:8201`) and consumes the `/data/tradingsystem` volume provided by `docker-compose.rag.yml`. Narrow the scope with `--allowed md,mdx,ts,tsx,json` or disable file-size checks via `--max-mb 0`. Any invocation can choose a new `--collection` or supply `collection_name` directly in the JSON payload.

> **Switching query targets** â€“ Restart `rag-llamaindex-query` with `QDRANT_COLLECTION=tradingsystem` (or another name) so `/query` responses come from the repository-wide index.

### Build LangGraph Workflow

```python
# langgraph/src/application/workflows/analysis_pipeline.py
from langgraph.graph import StateGraph

# Define workflow
workflow = StateGraph()
workflow.add_node("extract", extract_data)
workflow.add_node("analyze", analyze_data)
workflow.add_node("validate", validate_results)
workflow.add_edge("extract", "analyze")
workflow.add_edge("analyze", "validate")

# Run workflow
result = workflow.invoke({"input": "market_data.csv"})
```

---

## ðŸ“– Related Documentation

- **[Agents Overview](/agents/overview)** - Agent architecture and patterns
- **[MCP Platform](/mcp)** - Model Context Protocol integration
- **[Prompts Library](/prompts/overview)** - Reusable prompt templates
- **[Python Runtime](/tools/runtime/python/overview)** - Python setup for AI tools
