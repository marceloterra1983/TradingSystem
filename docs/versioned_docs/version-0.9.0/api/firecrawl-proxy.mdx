---
title: Firecrawl Proxy API
sidebar_label: Firecrawl Proxy
sidebar_position: 5
description: Web scraping proxy service with Firecrawl integration, validation, rate limiting, and comprehensive error handling
tags:
  - api
  - infrastructure
  - web-scraping
  - proxy
  - firecrawl
owner: BackendGuild
lastReviewed: '2025-10-27'
---

## Overview

The **Firecrawl Proxy API** is a lightweight proxy service that provides secure, validated access to Firecrawl's web scraping capabilities. It acts as an intermediary between TradingSystem services and the Firecrawl API, handling authentication, rate limiting, and error handling.

### Key Features

- **Secure API Key Management**: Centralized Firecrawl API key storage
- **Request Validation**: Input validation before forwarding to Firecrawl
- **Timeout Handling**: 30-second timeout for scraping requests
- **Structured Logging**: Pino-based JSON logging for all requests
- **Error Normalization**: Consistent error responses across all endpoints
- **Helmet Security**: HTTP security headers for protection
- **CORS Enabled**: Cross-origin requests allowed

## Service Details

| Property | Value |
|----------|-------|
| **Port** | 3600 |
| **Base URL** | `http://localhost:3600` |
| **Repository** | `backend/api/firecrawl-proxy/` |
| **Upstream API** | `https://api.firecrawl.dev` |
| **Status** | ✅ Active |
| **Technology** | Express.js, Axios, Node.js |

## Architecture

```
┌──────────────────────────────────────────┐
│   TradingSystem Services / Dashboard    │
└──────────────────┬───────────────────────┘
                   │
                   ▼
┌──────────────────────────────────────────┐
│      Firecrawl Proxy (Port 3600)        │
│                                          │
│  • Request Validation                   │
│  • API Key Injection                    │
│  • Timeout Management (30s)             │
│  • Error Normalization                  │
└──────────────────┬───────────────────────┘
                   │
                   ▼
┌──────────────────────────────────────────┐
│    Firecrawl API (api.firecrawl.dev)    │
│                                          │
│  • Web Scraping Engine                  │
│  • Content Extraction                   │
│  • Multi-page Crawling                  │
└──────────────────────────────────────────┘
```

**Flow:**

1. Client sends scrape/crawl request to proxy (Port 3600)
2. Proxy validates request (URL required)
3. Proxy injects Firecrawl API key
4. Proxy forwards request to Firecrawl API
5. Firecrawl processes request and returns data
6. Proxy normalizes and returns response to client

## Quick Start

### Health Check

```bash
curl http://localhost:3600/health
```

Response:

```json
{
  "status": "healthy",
  "service": "firecrawl-proxy",
  "version": "1.0.0",
  "firecrawlApiUrl": "https://api.firecrawl.dev",
  "hasApiKey": true
}
```

### Scrape a Single URL

```bash
curl -X POST http://localhost:3600/api/scrape \
  -H "Content-Type: application/json" \
  -d '{
    "url": "https://example.com"
  }'
```

Response:

```json
{
  "success": true,
  "data": {
    "markdown": "# Example Domain\n\nThis domain is for use in illustrative examples...",
    "html": "<html>...</html>",
    "metadata": {
      "title": "Example Domain",
      "description": "Example Domain",
      "language": "en",
      "sourceURL": "https://example.com"
    }
  }
}
```

### Crawl Multiple Pages

```bash
curl -X POST http://localhost:3600/api/crawl \
  -H "Content-Type: application/json" \
  -d '{
    "url": "https://docs.example.com",
    "crawlerOptions": {
      "excludes": ["blog/*"],
      "includes": ["docs/*"],
      "limit": 100
    }
  }'
```

Response:

```json
{
  "success": true,
  "jobId": "abc-123-def-456",
  "url": "https://docs.example.com"
}
```

### Check Crawl Status

```bash
curl http://localhost:3600/api/crawl/status/abc-123-def-456
```

Response:

```json
{
  "success": true,
  "status": "completed",
  "current": 45,
  "total": 45,
  "data": [
    {
      "markdown": "...",
      "html": "...",
      "metadata": {...}
    }
  ]
}
```

## API Reference

### Main Endpoints

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/health` | GET | Health check with API key status |
| `/api/scrape` | POST | Scrape single URL (30s timeout) |
| `/api/crawl` | POST | Start multi-page crawl job |
| `/api/crawl/status/:jobId` | GET | Check crawl job status |

## Endpoint Details

### GET /health

Health check endpoint that verifies proxy status and API key configuration.

**Response:**

```json
{
  "status": "healthy",
  "service": "firecrawl-proxy",
  "version": "1.0.0",
  "firecrawlApiUrl": "https://api.firecrawl.dev",
  "hasApiKey": true
}
```

**Response Fields:**

- `status`: Always "healthy" if service is running
- `service`: Service identifier
- `version`: API version
- `firecrawlApiUrl`: Configured Firecrawl API URL
- `hasApiKey`: Boolean indicating if API key is configured

**Use Case**: Monitoring scripts, health checks, service discovery

### POST /api/scrape

Scrape content from a single URL with optional configuration.

**Request Body:**

```json
{
  "url": "https://example.com",
  "pageOptions": {
    "onlyMainContent": true,
    "includeHtml": false,
    "screenshot": false
  },
  "timeout": 30000
}
```

**Request Fields:**

- `url` (required): Target URL to scrape
- `pageOptions` (optional): Scraping configuration
  - `onlyMainContent`: Extract only main content (default: false)
  - `includeHtml`: Include raw HTML (default: true)
  - `screenshot`: Capture screenshot (default: false)
- `timeout`: Request timeout in milliseconds (default: 30000)

**Response (Success):**

```json
{
  "success": true,
  "data": {
    "markdown": "# Page Title\n\nContent in markdown format...",
    "html": "<html>...</html>",
    "metadata": {
      "title": "Page Title",
      "description": "Page description",
      "language": "en",
      "keywords": "example, test",
      "author": "Author Name",
      "sourceURL": "https://example.com"
    },
    "links": [
      "https://example.com/page1",
      "https://example.com/page2"
    ]
  }
}
```

**Response (Error):**

```json
{
  "error": "Request timeout",
  "firecrawlApiUrl": "https://api.firecrawl.dev"
}
```

**Timeout**: 30 seconds (configurable)

**Example:**

```bash
# Scrape with options
curl -X POST http://localhost:3600/api/scrape \
  -H "Content-Type: application/json" \
  -d '{
    "url": "https://blog.example.com/article",
    "pageOptions": {
      "onlyMainContent": true,
      "includeHtml": false
    }
  }'
```

### POST /api/crawl

Start a multi-page crawl job that processes multiple URLs from a starting point.

**Request Body:**

```json
{
  "url": "https://docs.example.com",
  "crawlerOptions": {
    "includes": ["docs/**"],
    "excludes": ["docs/archive/**"],
    "limit": 100,
    "maxDepth": 3
  },
  "pageOptions": {
    "onlyMainContent": true
  }
}
```

**Request Fields:**

- `url` (required): Starting URL for crawl
- `crawlerOptions` (optional):
  - `includes`: URL patterns to include (glob format)
  - `excludes`: URL patterns to exclude (glob format)
  - `limit`: Maximum pages to crawl (default: 100)
  - `maxDepth`: Maximum crawl depth (default: unlimited)
- `pageOptions` (optional): Same as scrape endpoint

**Response:**

```json
{
  "success": true,
  "jobId": "abc-123-def-456",
  "url": "https://docs.example.com"
}
```

**Response Fields:**

- `success`: Boolean indicating job creation success
- `jobId`: Unique job identifier for status checks
- `url`: Starting URL for the crawl

**Next Steps**: Use `jobId` to poll `/api/crawl/status/:jobId`

**Example:**

```bash
# Crawl documentation site
curl -X POST http://localhost:3600/api/crawl \
  -H "Content-Type: application/json" \
  -d '{
    "url": "https://docs.firecrawl.dev",
    "crawlerOptions": {
      "includes": ["docs/**"],
      "limit": 50
    }
  }'
```

### GET /api/crawl/status/:jobId

Check the status of a crawl job and retrieve results when complete.

**URL Parameters:**

- `jobId` (required): Job ID returned from `/api/crawl`

**Response (In Progress):**

```json
{
  "success": true,
  "status": "scraping",
  "current": 15,
  "total": 50,
  "data": []
}
```

**Response (Completed):**

```json
{
  "success": true,
  "status": "completed",
  "current": 50,
  "total": 50,
  "data": [
    {
      "markdown": "...",
      "html": "...",
      "metadata": {...},
      "sourceURL": "https://docs.example.com/intro"
    },
    {
      "markdown": "...",
      "html": "...",
      "metadata": {...},
      "sourceURL": "https://docs.example.com/guide"
    }
    // ... more pages
  ]
}
```

**Response Fields:**

- `success`: Boolean indicating API call success
- `status`: `scraping` | `completed` | `failed`
- `current`: Number of pages processed
- `total`: Total pages to process
- `data`: Array of scraped page data (only when status is "completed")

**Polling Strategy**: Poll every 2-5 seconds until status is "completed" or "failed"

**Example:**

```bash
# Check status
curl http://localhost:3600/api/crawl/status/abc-123-def-456

# Poll until complete (bash)
while true; do
  STATUS=$(curl -s http://localhost:3600/api/crawl/status/abc-123-def-456 | jq -r '.status')
  echo "Status: $STATUS"
  if [ "$STATUS" = "completed" ] || [ "$STATUS" = "failed" ]; then
    break
  fi
  sleep 3
done
```

## Configuration

### Environment Variables

```bash
# Server Configuration
PORT=3600                           # API port (default: 3600)

# Firecrawl API Configuration
FIRECRAWL_API_URL=https://api.firecrawl.dev  # Firecrawl API URL (default)
FIRECRAWL_API_KEY=fc-xxx            # Firecrawl API key (REQUIRED)

# Logging
LOG_LEVEL=info                      # Log level: debug, info, warn, error (default: info)
```

### Getting a Firecrawl API Key

1. Sign up at [firecrawl.dev](https://firecrawl.dev)
2. Navigate to API Keys section
3. Generate new API key
4. Set `FIRECRAWL_API_KEY` environment variable

**Free Tier**: 500 credits/month
**Paid Tiers**: Higher limits and advanced features

## Integration Examples

### TypeScript/JavaScript

```typescript
// Scrape a single page
async function scrapePage(url: string) {
  const response = await fetch('http://localhost:3600/api/scrape', {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({ url })
  });

  if (!response.ok) {
    throw new Error(`Scrape failed: ${response.statusText}`);
  }

  const data = await response.json();
  return data.data.markdown;
}

// Crawl website and wait for completion
async function crawlWebsite(
  url: string,
  options?: { limit?: number; includes?: string[]; excludes?: string[] }
) {
  // Start crawl
  const startResponse = await fetch('http://localhost:3600/api/crawl', {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({
      url,
      crawlerOptions: {
        limit: options?.limit || 100,
        includes: options?.includes || [],
        excludes: options?.excludes || []
      }
    })
  });

  const { jobId } = await startResponse.json();

  // Poll until complete
  while (true) {
    const statusResponse = await fetch(
      `http://localhost:3600/api/crawl/status/${jobId}`
    );
    const statusData = await statusResponse.json();

    if (statusData.status === 'completed') {
      return statusData.data;
    }

    if (statusData.status === 'failed') {
      throw new Error('Crawl failed');
    }

    // Wait 3 seconds before next poll
    await new Promise(resolve => setTimeout(resolve, 3000));
  }
}

// Usage
const markdown = await scrapePage('https://example.com');
const pages = await crawlWebsite('https://docs.example.com', {
  limit: 50,
  includes: ['docs/**']
});
```

### React Hook

```typescript
import { useState } from 'react';

interface ScrapeOptions {
  onlyMainContent?: boolean;
  includeHtml?: boolean;
}

function useScraper() {
  const [loading, setLoading] = useState(false);
  const [error, setError] = useState<string | null>(null);

  const scrape = async (url: string, options?: ScrapeOptions) => {
    setLoading(true);
    setError(null);

    try {
      const response = await fetch('http://localhost:3600/api/scrape', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          url,
          pageOptions: options
        })
      });

      if (!response.ok) {
        throw new Error('Scrape failed');
      }

      const data = await response.json();
      setLoading(false);
      return data.data;
    } catch (err) {
      setError(err.message);
      setLoading(false);
      return null;
    }
  };

  return { scrape, loading, error };
}

// Usage in component
function WebScraperForm() {
  const { scrape, loading, error } = useScraper();
  const [url, setUrl] = useState('');
  const [result, setResult] = useState(null);

  const handleScrape = async () => {
    const data = await scrape(url, { onlyMainContent: true });
    setResult(data);
  };

  return (
    <div>
      <input value={url} onChange={e => setUrl(e.target.value)} />
      <button onClick={handleScrape} disabled={loading}>
        {loading ? 'Scraping...' : 'Scrape'}
      </button>
      {error && <div>Error: {error}</div>}
      {result && <pre>{result.markdown}</pre>}
    </div>
  );
}
```

### curl Examples

```bash
# Scrape single page
curl -X POST http://localhost:3600/api/scrape \
  -H "Content-Type: application/json" \
  -d '{"url": "https://example.com"}'

# Scrape with options (main content only)
curl -X POST http://localhost:3600/api/scrape \
  -H "Content-Type: application/json" \
  -d '{
    "url": "https://blog.example.com/article",
    "pageOptions": {
      "onlyMainContent": true,
      "includeHtml": false
    }
  }'

# Start crawl
JOBID=$(curl -s -X POST http://localhost:3600/api/crawl \
  -H "Content-Type: application/json" \
  -d '{
    "url": "https://docs.example.com",
    "crawlerOptions": {"limit": 10}
  }' | jq -r '.jobId')

# Check crawl status
curl http://localhost:3600/api/crawl/status/$JOBID | jq '.status'

# Get completed crawl data
curl http://localhost:3600/api/crawl/status/$JOBID | jq '.data'
```

## Error Handling

### HTTP Status Codes

| Code | Meaning | When It Occurs |
|------|---------|----------------|
| 200 | OK | Request successful |
| 400 | Bad Request | Missing URL parameter |
| 500 | Internal Server Error | Firecrawl API error or proxy error |
| 504 | Gateway Timeout | Scrape request exceeded 30s timeout |

### Error Response Format

```json
{
  "error": "Error description",
  "details": {...},
  "firecrawlApiUrl": "https://api.firecrawl.dev"
}
```

### Common Errors

**400 - Missing URL:**

```json
{
  "error": "URL is required"
}
```

**500 - API Key Not Configured:**

```json
{
  "error": "Firecrawl API key not configured",
  "message": "Please set FIRECRAWL_API_KEY environment variable"
}
```

**504 - Request Timeout:**

```json
{
  "error": "Request timeout"
}
```

**500 - Firecrawl API Error:**

```json
{
  "error": "Request failed with status code 402",
  "details": {
    "message": "Insufficient credits"
  },
  "firecrawlApiUrl": "https://api.firecrawl.dev"
}
```

### Retry Strategy

For **504 timeout errors**:

- Retry with longer timeout (if supported by Firecrawl)
- Split large pages into smaller scraping tasks

For **500 errors from Firecrawl**:

- Check Firecrawl API status
- Verify API key is valid
- Check credit balance

For **crawl jobs**:

- Poll status endpoint every 2-5 seconds
- Implement max polling duration (e.g., 5 minutes)
- Handle "failed" status gracefully

## Security Considerations

### API Key Protection

- ✅ **API key stored server-side**: Never exposed to frontend
- ✅ **Environment variable**: Configured via `.env` file
- ⚠️ **No authentication on proxy**: Consider adding API key or JWT auth for production

### Recommended Security Enhancements

For production deployments:

1. **Add Authentication**:

   ```javascript
   // Add API key middleware
   app.use((req, res, next) => {
     const apiKey = req.headers['x-api-key'];
     if (apiKey !== process.env.PROXY_API_KEY) {
       return res.status(401).json({ error: 'Unauthorized' });
     }
     next();
   });
   ```

2. **Rate Limiting** (per user):

   ```javascript
   import rateLimit from 'express-rate-limit';

   const limiter = rateLimit({
     windowMs: 60 * 1000, // 1 minute
     max: 10, // 10 requests per minute
     keyGenerator: (req) => req.headers['x-api-key'] || req.ip
   });

   app.use(limiter);
   ```

3. **URL Whitelist** (restrict scraping to known domains):

   ```javascript
   const allowedDomains = ['example.com', 'docs.example.com'];

   function validateUrl(url) {
     const hostname = new URL(url).hostname;
     return allowedDomains.some(d => hostname.endsWith(d));
   }
   ```

## Related Documentation

- **[Firecrawl Official Docs](https://docs.firecrawl.dev)** - Firecrawl API documentation
- **[Status API](/api/status-api)** - Service health monitoring
 - **[Dashboard Integration](/frontend/architecture/dashboard)** - Frontend integration overview

## Support

For issues, questions, or feature requests:

- **Firecrawl Issues**: [Firecrawl GitHub](https://github.com/mendableai/firecrawl/issues)
- **Proxy Issues**: [TradingSystem Issues](https://github.com/yourusername/tradingsystem/issues)
- **Source Code**: `backend/api/firecrawl-proxy/src/server.js`
