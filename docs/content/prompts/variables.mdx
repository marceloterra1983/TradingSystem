---
title: Prompt Variables
description: Variable catalog and context construction rules for prompts.
tags:
  - prompts
  - variables
owner: PromptOps
lastReviewed: '2025-10-26'
---
## Overview

This page catalogs all environment variables and configuration flags used by LLM-powered features in TradingSystem. Variables control LLM providers, models, behavior flags, and integration settings.

**Configuration File**: `.env` at project root (see [Environment Variables](../../tools/security-config/env))

## Variable Catalog

### LLM Provider Configuration

| Variable | Description | Default | Required | Example |
|----------|-------------|---------|----------|----------|
| `OPENAI_API_KEY` | OpenAI API key for LlamaIndex and LangGraph | - | Yes | `sk-proj-...` |
| `OPENAI_BASE_URL` | OpenAI API base URL (for proxies) | `https://api.openai.com/v1` | No | Custom endpoint |
| `OPENAI_MODEL` | OpenAI model identifier | `gpt-4o-mini` | No | `gpt-4o`, `gpt-4-turbo` |
| `GEMINI_API_KEY` | Google Gemini API key | - | No | `AIza...` |
| `GEMINI_MODEL` | Gemini model identifier | `gemini-2.0-flash-exp` | No | `gemini-pro` |

**Security**:
- Never log API keys
- Rotate every 180 days
- Store in `.env` (gitignored)
- Use least-privilege scopes

---

### Agno Agents Configuration

| Variable | Description | Default | Impact |
|----------|-------------|---------|--------|
| `AGNO_ENABLE_LLM` | Enable LLM-powered analysis | `false` | When false, uses deterministic fallback |
| `AGNO_MODEL_PROVIDER` | LLM provider | `openai` | Values: `openai`, `anthropic`, `gemini` |
| `AGNO_MODEL_NAME` | Model identifier | `gpt-4o` | Model used for all agents |
| `AGNO_LOG_LEVEL` | Logging verbosity | `INFO` | DEBUG shows prompt details |
| `AGNO_API_URL` | Agno API endpoint | `http://localhost:8200` | Agent service URL |
| `AGNO_API_TIMEOUT` | API request timeout (seconds) | `30` | Affects data gathering latency |

**Behavior**:
- `AGNO_ENABLE_LLM=false`: Agents use heuristic fallback (faster, deterministic)
- `AGNO_ENABLE_LLM=true`: Agents use LLM analysis (slower, more intelligent)

---

### LangGraph Configuration

**Service Settings**:
| Variable | Description | Default |
|----------|-------------|---------|
| `LANGGRAPH_PORT` | LangGraph service port | `8111` |
| `LANGGRAPH_ENV` | Environment mode | `production` |
| `LANGGRAPH_LOG_LEVEL` | Logging verbosity | `INFO` |

**LLM Configuration**:
| Variable | Description | Default |
|----------|-------------|---------|
| `LLM_ENRICHMENT_ENABLED` | Enable LLM enrichment | `false` |
| `OPENAI_API_KEY` | OpenAI API key | - |
| `OPENAI_MODEL` | OpenAI model | `gpt-4o-mini` |
| `GEMINI_API_KEY` | Gemini API key | - |
| `GEMINI_MODEL` | Gemini model | `gemini-2.0-flash-exp` |

**Feature Flags**:
| Variable | Description | Default |
|----------|-------------|---------|
| `ENABLE_TRADING_WORKFLOWS` | Enable trading workflows | `true` |
| `ENABLE_DOCS_WORKFLOWS` | Enable documentation workflows | `true` |
| `ENABLE_WEBHOOKS` | Enable webhook triggers | `true` |
| `ENABLE_METRICS` | Enable Prometheus metrics | `true` |
| `ENABLE_TRACING` | Enable distributed tracing | `false` |

**Performance & Resilience**:
| Variable | Description | Default |
|----------|-------------|---------|
| `MAX_CONCURRENT_WORKFLOWS` | Max concurrent workflows | `10` |
| `WORKFLOW_TIMEOUT_SECONDS` | Workflow timeout | `300` |
| `RETRY_MAX_ATTEMPTS` | Max retry attempts | `3` |
| `RETRY_DELAYS` | Retry delays (seconds) | `[1,2,4]` |

---

### LlamaIndex Configuration

| Variable | Description | Default | Required |
|----------|-------------|---------|----------|
| `OPENAI_API_KEY` | OpenAI API key for LlamaIndex | - | Yes |
| `QDRANT_API_KEY` | Qdrant vector DB API key | - | No |
| `RATE_LIMIT_REQUESTS` | Rate limit requests | `100` | No |
| `RATE_LIMIT_PERIOD` | Rate limit period (seconds) | `60` | No |

## Context Assembly

### Context Sources

**MarketAnalysisAgent**:
- **Market Data**: Last 24 hours of B3 data (price, volume, indicators)
- **TP Capital Signals**: Last 100 signals (configurable via `limit` parameter)
- **Workspace Ideas**: All active ideas (filtered by symbol if applicable)
- **Context Size**: ~2000 tokens

**RiskManagementAgent**:
- **Signal Context**: Single MarketSignal entity
- **Risk Limits**: Current daily loss, position sizes, trading hours
- **Historical Context**: Not used (stateless validation)
- **Context Size**: ~500 tokens

**SignalOrchestratorAgent**:
- **Workflow Context**: Orchestration request + agent responses
- **Agent Metadata**: Agents involved, processing times
- **Context Size**: ~1000 tokens

### Context Construction Rules

**Data Prioritization**:
1. Most recent data first (last 24 hours)
2. Requested symbols only (filter irrelevant data)
3. Limit array sizes (max 100 signals, 50 ideas)
4. Truncate large text fields (max 500 characters)

**Token Management**:
- Monitor context size (log warnings at 80% of limit)
- Implement sliding window for historical data
- Compress redundant information
- Use summarization for large datasets

**Template Assembly**:
```
System: {system_instructions}

User: {task_json}

Task: {task_name}
Payload:
- Symbols: {symbols}
- Context: {context_data}

{specific_instructions}
```

**Variables Combine**:
- `{system_instructions}`: Agent role definition
- `{task_json}`: JSON-encoded task and payload
- `{task_name}`: Task identifier (analyze_market, validate_signal)
- `{symbols}`: List of symbols to process
- `{context_data}`: Assembled context from multiple sources
- `{specific_instructions}`: Task-specific guidance

## Guardrails

### Input Validation

**Before LLM Call**:
- Validate JSON structure (required fields present)
- Check data types (strings, numbers, booleans)
- Verify enum values (BUY/SELL, signal types)
- Sanitize inputs (remove special characters if needed)
- Enforce size limits (max array length, string length)

### Output Validation

**After LLM Response**:
- Parse JSON response (catch parse errors)
- Validate schema (required fields present)
- Check value ranges (confidence 0.0-1.0)
- Verify enum values (signal_type in [BUY, SELL, HOLD])
- Sanitize outputs (remove sensitive data)

### Rate Limiting

**LlamaIndex**:
- `RATE_LIMIT_REQUESTS`: 100 requests per period
- `RATE_LIMIT_PERIOD`: 60 seconds

**OpenAI**:
- Respect API rate limits (tier-based)
- Implement exponential backoff on 429 errors
- Monitor usage via Prometheus metrics

### Error Handling

**LLM Failures**:
- Catch exceptions gracefully
- Log errors with context (symbol, action, error type)
- Track metrics (`agent_errors_total`)
- Return safe defaults (HOLD signal, reject risk assessment)
- Retry with exponential backoff (max 3 attempts)

**Timeout Handling**:
- Set reasonable timeouts (10-30s for LLM calls)
- Cancel long-running requests
- Log timeout events
- Use cached results if available

## Variable Validation

**Validation Script**: `bash scripts/env/validate-env.sh`

**Required Variables**:
- `OPENAI_API_KEY` (for LlamaIndex and LangGraph)
- `JWT_SECRET_KEY` (non-LLM; see [auth & security variables](../../tools/security-config/env) for full coverage)

**Optional Variables**:
- All Agno, LangGraph, and Gemini variables

**Validation Checks**:
- All required variables present
- No placeholder values (e.g., `CHANGE_ME`)
- File permissions correct (600)

## Related Documentation

- [Prompt Patterns](./patterns) - How variables are used in templates
- [Prompt Style Guide](./style-guide) - Formatting and guidelines
- [Environment Variables](../../tools/security-config/env) - Complete env var reference
- LangGraph environment variables (migration pending)
- [Security Configuration](../../tools/security-config/overview)
