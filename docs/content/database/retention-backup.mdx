---
title: Retention & Backup
description: Data retention policies and backup/restore procedures.
tags:
  - database
  - operations
owner: DataOps
lastReviewed: '2025-10-26'
---
## Retention Policy

Retention rules balance storage cost, operational needs, and future compliance requirements. All
automation should follow these windows and archival steps.

### LowDB JSON Files
- **Retention**: 14 rolling daily snapshots.  
- **Archival**: Monthly exports to cold storage (external drive or cloud).  
- **Files**: `db/ideas.json`, `db/db.json`.  
- **Backup Schedule**: Daily at 2 AM via scheduled PowerShell script.  
- **Storage Location**: `backups/lowdb/YYYY-MM-DD/`.  
- **Cleanup**: Delete snapshots older than 14 days (automated).  
- **Cold Storage**: Monthly full backup stored indefinitely.

### QuestDB Tables

**tp_capital_signals**  
- 30-day online retention.  
- Monthly CSV export before deletion.  
- Export location: `backups/questdb/tp_capital_signals/YYYY-MM/`.  
- Cleanup command:
  ```sql
  ALTER TABLE tp_capital_signals DROP PARTITION WHERE ts < dateadd('d', -30, now());
  ```

**tp_capital_signals_deleted**  
- 90-day retention to support audit and recovery.  
- Manual review before purging.

**telegram_bots / telegram_channels**  
- Retain full history.  
- Prune records older than 365 days once PostgreSQL configuration DB launches.

### Parquet Files (Market Data)
- **Retention**: Minimum two years of intraday history.  
- **Storage**: `data/parquet/YYYY/MM/DD/`.  
- **Capacity Planning**: Monitor disk usage monthly.  
- **Archival**: Compress and move to cold storage once older than two years.  
- **Backup**: Mirror to secondary storage using `robocopy /MIR` or `rsync`.

### Uploaded Documentation Files
- **Location**: `uploads/`.  
- **Retention**: Indefinite until compliance policy defined.  
- **Review**: Quarterly audit to remove obsolete content.

### Logs
- **Structured Logs (JSONL)**: 30-day retention, daily rotation, location `logs/YYYY-MM-DD.jsonl`, automated cleanup.  
- **Application Logs**: 7-day retention for stdout/stderr, archive critical errors for 90 days, central aggregation planned.

### Compliance Considerations
- Current scope is internal tooling with no formal obligations.  
- Future plans include GDPR compliance (30 days after account deletion), financial retention (5–7 years), and audit log policies (1–3 years) as requirements emerge.

## Backup Procedures

### LowDB Backup

- **Method**: Scheduled PowerShell script copying JSON files.  
- **Schedule**: Daily at 2 AM.  
- **Script**: Planned automation (`scripts/backup/backup-lowdb.ps1`, pending addition); follow manual steps until committed.

```powershell
$timestamp = Get-Date -Format "yyyyMMdd-HHmmss"
Copy-Item db/ideas.json backups/lowdb/$timestamp/ideas.json
Copy-Item db/db.json backups/lowdb/$timestamp/db.json
```

**Verification**:

```powershell
$original = Get-FileHash db/ideas.json
$backup = Get-FileHash backups/lowdb/$timestamp/ideas.json
if ($original.Hash -eq $backup.Hash) {
    Write-Host "Backup verified successfully"
}
```

### QuestDB Backup

- **Method**: Stop service and archive QuestDB data directory.  
- **Schedule**: Weekly (Sunday, 3 AM).  
- **Script**: Planned automation (`scripts/backup/questdb-export.ps1`, pending addition); use procedure below until the script lands.

Steps:
1. Stop TP Capital service:
   ```bash
   npm --prefix apps/tp-capital run stop
   ```
2. Zip QuestDB data directory:
   ```powershell
   $timestamp = Get-Date -Format "yyyyMMdd"
   Compress-Archive -Path $env:QUESTDB_ROOT/db/ -DestinationPath backups/questdb/questdb-$timestamp.zip
   ```
3. Verify archive:
   ```powershell
   Test-Path backups/questdb/questdb-$timestamp.zip
   ```
4. Restart service:
   ```bash
   npm --prefix apps/tp-capital run start
   ```

**Alternative CSV Export**:

```bash
curl -G http://localhost:9002/exp \
  --data-urlencode "query=SELECT * FROM tp_capital_signals WHERE ts > dateadd('d', -30, now())" \
  -o backups/questdb/tp_capital_signals_$(date +%Y%m%d).csv
```

### Parquet Backup

- **Method**: Mirror directory to secondary storage.  
- **Schedule**: Weekly (Saturday, 4 AM).

```powershell
robocopy data/parquet/ E:/backups/parquet/ /MIR /LOG:logs/parquet-backup.log
```

```bash
rsync -av --delete data/parquet/ /mnt/backup/parquet/
```

**Verification**:
```bash
find data/parquet/ -type f | wc -l
find /mnt/backup/parquet/ -type f | wc -l
```

### Configuration Backup

- **Scope**: `.env`, Docker compose files, `config/`, `scripts/`.  
- **Method**: Git + encrypted archive.  
- **Schedule**: After configuration changes.

```bash
tar -czf config-backup-$(date +%Y%m%d).tar.gz .env config/ scripts/
openssl enc -aes-256-cbc -salt \
  -in config-backup-$(date +%Y%m%d).tar.gz \
  -out config-backup-$(date +%Y%m%d).tar.gz.enc
```

- **Storage**: Secure credential store or encrypted drive.  
- **Security**: `.env` must remain ignorable by Git and always encrypted in backups.

## Restore Procedures

### LowDB Restore

1. Stop dependent services:
   ```bash
   npm --prefix backend/api/workspace run stop
   npm --prefix backend/api/documentation-api run stop
   ```
2. Restore backups:
   ```bash
   cp backups/lowdb/20251024-020000/ideas.json db/ideas.json
   cp backups/lowdb/20251024-020000/db.json db/db.json
   ```
3. Verify JSON:
   ```bash
   node -e "JSON.parse(require('fs').readFileSync('db/ideas.json'))"
   node -e "JSON.parse(require('fs').readFileSync('db/db.json'))"
   ```
4. Restart services and validate endpoints.

### QuestDB Restore

1. Stop TP Capital and QuestDB containers:
   ```bash
   npm --prefix apps/tp-capital run stop
   docker compose -f tools/compose/docker-compose.questdb.yml down
   ```
2. Replace corrupted data with backup:
   ```powershell
   Remove-Item -Recurse -Force $env:QUESTDB_ROOT/db/
   Expand-Archive -Path backups/questdb/questdb-20251024.zip -DestinationPath $env:QUESTDB_ROOT/
   ```
3. Restart QuestDB and TP Capital service, then validate:
   ```bash
   docker compose -f tools/compose/docker-compose.questdb.yml up -d
   npm --prefix apps/tp-capital run start
   curl http://localhost:4005/health
   curl http://localhost:4005/signals?limit=5
   curl http://localhost:4005/telegram/bots
   curl http://localhost:4005/telegram/channels
   ```

### Parquet Restore

1. Identify missing files:
   ```bash
   diff -r data/parquet/ /mnt/backup/parquet/
   ```
2. Restore from backup:
   ```bash
   rsync -av /mnt/backup/parquet/ data/parquet/
   ```
3. Validate sample read:
   ```python
   import polars as pl
   df = pl.read_parquet('data/parquet/2025/10/24/market-data.parquet')
   print(df.shape)
   ```

### Configuration Restore

1. Decrypt archive:
   ```bash
   openssl enc -aes-256-cbc -d -in config-backup-20251024.tar.gz.enc -out config-backup-20251024.tar.gz
   ```
2. Extract and restore:
   ```bash
   tar -xzf config-backup-20251024.tar.gz
   cp .env.backup .env
   cp -r config.backup/* config/
   cp -r scripts.backup/* scripts/
   ```
3. Validate environment:
   ```bash
   bash scripts/env/validate-env.sh
   bash scripts/startup/start-all.sh
   ```

## Monitoring and Alerts

- Track backup job success via Prometheus metrics and alert after two consecutive failures.  
- Monitor backup storage capacity, alert at 80% utilization.  
- Run weekly checksum validation and monthly restore drills to staging.  
- Audit retention compliance monthly and alert if data exceeds window by seven days.  
- Conduct quarterly disaster recovery exercises.

## Related Documentation

- [Database Overview](./overview) – Architecture and store inventory.  
- [Schema Reference](./schema) – Table definitions and constraints.  
- [Migrations](./migrations) – Cutover strategy and checklists.  
- Data quality runbook and automation scripts migrate separately.
