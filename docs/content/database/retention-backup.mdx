---
title: Retention & Backup
slug: /database/retention-backup
description: Data retention policies and backup/restore procedures.
tags:
  - database
  - operations
owner: DataOps
lastReviewed: '2025-10-27'
---
## Retention Policy

Retention rules balance storage cost, operational needs, and compliance requirements. All databases follow automated retention windows with proper archival procedures.

### TimescaleDB Databases

**APPS-TPCAPITAL** (`tp-capital` schema):

- **tp_capital_signals**: 90-day online retention with hypertable compression after 7 days
- **telegram_bots**: Full history retention (configuration data)
- **telegram_channels**: Full history retention (configuration data)
- **Backup Schedule**: Daily automated backups via `scripts/database/backup-timescaledb.sh`
- **Storage Location**: `backups/timescaledb/APPS-TPCAPITAL/YYYY-MM-DD/`
- **Cleanup**: Automated compression policies, manual partition drops after 90 days

**APPS-WORKSPACE** (`workspace` schema):

- **workspace_items**: 2-year online retention with monthly hypertable partitions
- **workspace_audit_log**: 1-year retention with compression after 30 days
- **schema_version**: Permanent retention
- **Backup Schedule**: Daily automated backups via `scripts/database/backup-timescaledb.sh`
- **Storage Location**: `backups/timescaledb/APPS-WORKSPACE/YYYY-MM-DD/`
- **Cleanup**: Automated compression, manual partition drops after retention period

### PostgreSQL + Prisma Database

**documentation-api**:

- **documentation_systems**: Full history retention
- **documentation_ideas**: Full history retention until marked deleted
- **documentation_files**: Retention tied to filesystem cleanup policies
- **documentation_audit_log**: 2-year retention
- **Backup Schedule**: Daily automated backups
- **Storage Location**: `backups/postgres/documentation-api/YYYY-MM-DD/`
- **Cleanup**: Manual pruning of old audit logs

### QuestDB Tables (Legacy)

**tp_capital_signals**  

- Legacy time-series data retention
- 30-day online retention for legacy queries
- Monthly CSV export before deletion
- Export location: `backups/questdb/tp_capital_signals/YYYY-MM/`
- Cleanup command:

  ```sql
  ALTER TABLE tp_capital_signals DROP PARTITION WHERE ts < dateadd('d', -30, now());
  ```

**tp_capital_signals_deleted**  

- 90-day retention for audit recovery
- Manual review before purging

**telegram_bots / telegram_channels** (Legacy)  

- Historical data only (migrated to TimescaleDB)
- Can be archived after migration validation

### LowDB JSON Files (Legacy)

- **Retention**: 14 rolling daily snapshots
- **Files**: `db/ideas.json` (legacy Idea Bank)
- **Backup Schedule**: Daily at 2 AM
- **Storage Location**: `backups/lowdb/YYYY-MM-DD/`
- **Cleanup**: Delete snapshots older than 14 days
- **Status**: Active for legacy Idea Bank, migration pending Q1 2026

### Parquet Files (Market Data)

- **Retention**: Minimum two years of intraday history.  
- **Storage**: `data/parquet/YYYY/MM/DD/`.  
- **Capacity Planning**: Monitor disk usage monthly.  
- **Archival**: Compress and move to cold storage once older than two years.  
- **Backup**: Mirror to secondary storage using `robocopy /MIR` or `rsync`.

### Uploaded Documentation Files

- **Location**: `uploads/`.  
- **Retention**: Indefinite until compliance policy defined.  
- **Review**: Quarterly audit to remove obsolete content.

### Logs

- **Structured Logs (JSONL)**: 30-day retention, daily rotation, location `logs/YYYY-MM-DD.jsonl`, automated cleanup.  
- **Application Logs**: 7-day retention for stdout/stderr, archive critical errors for 90 days, central aggregation planned.

### Compliance Considerations

- Current scope is internal tooling with no formal obligations.  
- Future plans include GDPR compliance (30 days after account deletion), financial retention (5–7 years), and audit log policies (1–3 years) as requirements emerge.

## Backup Procedures

### TimescaleDB Backup

**Automated Daily Backups**:

- **Script**: `scripts/database/backup-timescaledb.sh`
- **Schedule**: Daily at 2 AM via cron or Task Scheduler
- **Databases**: `APPS-TPCAPITAL`, `APPS-WORKSPACE`
- **Format**: PostgreSQL dump format (compressed)

```bash
# Manual backup execution
cd /home/marce/Projetos/TradingSystem
./scripts/database/backup-timescaledb.sh

# Creates backups:
# backups/timescaledb/APPS-TPCAPITAL-YYYYMMDD.dump
# backups/timescaledb/APPS-WORKSPACE-YYYYMMDD.dump
```

**Individual Database Backup**:

```bash
# Backup APPS-TPCAPITAL
docker exec data-timescaledb pg_dump -U timescale -d "APPS-TPCAPITAL" -F c -f /tmp/tpcapital.dump
docker cp data-timescaledb:/tmp/tpcapital.dump backups/timescaledb/APPS-TPCAPITAL-$(date +%Y%m%d).dump

# Backup APPS-WORKSPACE
docker exec data-timescaledb pg_dump -U timescale -d "APPS-WORKSPACE" -F c -f /tmp/workspace.dump
docker cp data-timescaledb:/tmp/workspace.dump backups/timescaledb/APPS-WORKSPACE-$(date +%Y%m%d).dump
```

**Verification**:

```bash
# List backup files
ls -lh backups/timescaledb/

# Verify dump integrity
docker exec data-timescaledb pg_restore --list /tmp/tpcapital.dump | head -20
```

---

### PostgreSQL + Prisma Backup

**Automated Backups**:

```bash
# Backup documentation-api database
docker exec data-documentation-postgres pg_dump -U postgres -d documentation-api -F c -f /tmp/documentation-api.dump
docker cp data-documentation-postgres:/tmp/documentation-api.dump backups/postgres/documentation-api-$(date +%Y%m%d).dump
```

**Prisma Schema Backup**:

```bash
# Backup Prisma schema and migrations
tar -czf backups/prisma/prisma-schema-$(date +%Y%m%d).tar.gz \
  backend/api/documentation-api/prisma/
```

---

### LowDB Backup (Legacy)

- **Method**: Copy JSON files with timestamp
- **Schedule**: Daily at 2 AM
- **Script**: Manual or automated via cron

```bash
# Backup ideas.json
timestamp=$(date +%Y%m%d-%H%M%S)
mkdir -p backups/lowdb/$timestamp
cp db/ideas.json backups/lowdb/$timestamp/ideas.json

# Verify backup
diff db/ideas.json backups/lowdb/$timestamp/ideas.json
```

---

### QuestDB Backup (Legacy)

- **Method**: Archive QuestDB data directory (legacy data only)
- **Schedule**: Monthly or as needed
- **Script**: `scripts/database/restore-questdb.sh`

```bash
# Stop QuestDB container
docker-compose -f tools/compose/docker-compose.questdb.yml down

# Backup data directory
tar -czf backups/questdb/questdb-$(date +%Y%m%d).tar.gz \
  $QUESTDB_ROOT/db/

# Restart QuestDB
docker-compose -f tools/compose/docker-compose.questdb.yml up -d
```

### Parquet Backup

- **Method**: Mirror directory to secondary storage.  
- **Schedule**: Weekly (Saturday, 4 AM).

```powershell
robocopy data/parquet/ E:/backups/parquet/ /MIR /LOG:logs/parquet-backup.log
```

```bash
rsync -av --delete data/parquet/ /mnt/backup/parquet/
```

**Verification**:

```bash
find data/parquet/ -type f | wc -l
find /mnt/backup/parquet/ -type f | wc -l
```

### Configuration Backup

- **Scope**: `.env`, Docker compose files, `config/`, `scripts/`.  
- **Method**: Git + encrypted archive.  
- **Schedule**: After configuration changes.

```bash
tar -czf config-backup-$(date +%Y%m%d).tar.gz .env config/ scripts/
openssl enc -aes-256-cbc -salt \
  -in config-backup-$(date +%Y%m%d).tar.gz \
  -out config-backup-$(date +%Y%m%d).tar.gz.enc
```

- **Storage**: Secure credential store or encrypted drive.  
- **Security**: `.env` must remain ignorable by Git and always encrypted in backups.

## Restore Procedures

### TimescaleDB Restore

**Full Database Restore**:

1. **Stop dependent services**:

   ```bash
   # Stop TP Capital API
   docker-compose -f apps/tp-capital/api/docker-compose.yml down
   
   # Stop Workspace API
   docker-compose -f backend/api/workspace/docker-compose.yml down
   ```

2. **Drop existing database** (if needed):

   ```bash
   docker exec data-timescaledb psql -U timescale -d postgres -c 'DROP DATABASE IF EXISTS "APPS-TPCAPITAL";'
   docker exec data-timescaledb psql -U timescale -d postgres -c 'CREATE DATABASE "APPS-TPCAPITAL";'
   ```

3. **Restore from backup**:

   ```bash
   # Copy backup to container
   docker cp backups/timescaledb/APPS-TPCAPITAL-20251024.dump data-timescaledb:/tmp/restore.dump
   
   # Restore database
   docker exec data-timescaledb pg_restore -U timescale -d "APPS-TPCAPITAL" -c /tmp/restore.dump
   
   # Clean up
   docker exec data-timescaledb rm /tmp/restore.dump
   ```

4. **Verify restore**:

   ```bash
   # Check record counts
   docker exec data-timescaledb psql -U timescale -d "APPS-TPCAPITAL" -c \
     'SELECT COUNT(*) FROM "tp-capital".tp_capital_signals;'
   
   # Verify hypertables
   docker exec data-timescaledb psql -U timescale -d "APPS-TPCAPITAL" -c \
     "SELECT * FROM timescaledb_information.hypertables;"
   ```

5. **Restart services**:

   ```bash
   docker-compose -f apps/tp-capital/api/docker-compose.yml up -d
   docker-compose -f backend/api/workspace/docker-compose.yml up -d
   ```

**Same process applies to APPS-WORKSPACE**

---

### PostgreSQL + Prisma Restore

1. **Stop Documentation API**:

   ```bash
   docker-compose -f backend/api/documentation-api/docker-compose.yml down
   ```

2. **Restore database**:

   ```bash
   # Copy backup
   docker cp backups/postgres/documentation-api-20251024.dump data-documentation-postgres:/tmp/restore.dump
   
   # Drop and recreate
   docker exec data-documentation-postgres psql -U postgres -c 'DROP DATABASE IF EXISTS "documentation-api";'
   docker exec data-documentation-postgres psql -U postgres -c 'CREATE DATABASE "documentation-api";'
   
   # Restore
   docker exec data-documentation-postgres pg_restore -U postgres -d documentation-api -c /tmp/restore.dump
   ```

3. **Run Prisma migrations**:

   ```bash
   cd backend/api/documentation-api
   npx prisma migrate deploy
   ```

4. **Restart service**:

   ```bash
   docker-compose -f backend/api/documentation-api/docker-compose.yml up -d
   ```

---

### LowDB Restore (Legacy)

1. **Stop services using LowDB**:

   ```bash
   # Stop any service using ideas.json
   ```

2. **Restore backup**:

   ```bash
   cp backups/lowdb/20251024-020000/ideas.json db/ideas.json
   ```

3. **Verify JSON integrity**:

   ```bash
   node -e "JSON.parse(require('fs').readFileSync('db/ideas.json'))"
   ```

4. **Restart services**

---

### QuestDB Restore (Legacy)

1. **Stop QuestDB**:

   ```bash
   docker-compose -f tools/compose/docker-compose.questdb.yml down
   ```

2. **Restore data directory**:

   ```bash
   # Backup current data (if any)
   mv $QUESTDB_ROOT/db $QUESTDB_ROOT/db.old
   
   # Extract backup
   tar -xzf backups/questdb/questdb-20251024.tar.gz -C $QUESTDB_ROOT/
   ```

3. **Restart QuestDB**:

   ```bash
   docker-compose -f tools/compose/docker-compose.questdb.yml up -d
   ```

4. **Verify data**:

   ```bash
   curl "http://localhost:9002/exp?query=SELECT+COUNT(*)+FROM+tp_capital_signals"
   ```

### Parquet Restore

1. Identify missing files:

   ```bash
   diff -r data/parquet/ /mnt/backup/parquet/
   ```

2. Restore from backup:

   ```bash
   rsync -av /mnt/backup/parquet/ data/parquet/
   ```

3. Validate sample read:

   ```python
   import polars as pl
   df = pl.read_parquet('data/parquet/2025/10/24/market-data.parquet')
   print(df.shape)
   ```

### Configuration Restore

1. Decrypt archive:

   ```bash
   openssl enc -aes-256-cbc -d -in config-backup-20251024.tar.gz.enc -out config-backup-20251024.tar.gz
   ```

2. Extract and restore:

   ```bash
   tar -xzf config-backup-20251024.tar.gz
   cp .env.backup .env
   cp -r config.backup/* config/
   cp -r scripts.backup/* scripts/
   ```

3. Validate environment:

   ```bash
   bash scripts/env/validate-env.sh
   bash scripts/startup/start-all.sh
   ```

## Monitoring and Alerts

- Track backup job success via Prometheus metrics and alert after two consecutive failures.  
- Monitor backup storage capacity, alert at 80% utilization.  
- Run weekly checksum validation and monthly restore drills to staging.  
- Audit retention compliance monthly and alert if data exceeds window by seven days.  
- Conduct quarterly disaster recovery exercises.

## Related Documentation

- [Database Overview](overview.mdx) – Architecture and store inventory.  
- [Schema Reference](schema.mdx) – Table definitions and constraints.  
- [Migrations](migrations.mdx) – Cutover strategy and checklists.  
- Data quality runbook and automation scripts migrate separately.
