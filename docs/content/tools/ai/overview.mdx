---
title: AI & LLM Tools
description: RAG pipelines with LlamaIndex plus local inference using Ollama.
tags:
  - ai
  - llm
  - llamaindex
  - ollama
  - rag
owner: ArchitectureGuild
lastReviewed: '2025-11-07'
---

# AI & LLM Tools

RAG (Retrieval-Augmented Generation) services for ingestion, querying, and local inference. The stack focuses on two building blocks:

1. **LlamaIndex** â€“ Handles document ingestion, chunking, metadata enrichment, and query serving.
2. **Ollama** â€“ Provides local LLM inference for both ingestion and query flows.

## ğŸ§± Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   LlamaIndex Ingest    â”‚      â”‚   LlamaIndex Query     â”‚
â”‚ (FastAPI + Workers)    â”‚      â”‚  (FastAPI + Adapters)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚                               â”‚
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚     Ollama     â”‚
                    â”‚  Local LLMs    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

- **Storage:** Qdrant collections + local file store for embeddings.
- **Transport:** FastAPI services exposed via `docker-compose.4-4-rag-stack.yml`.
- **Observability:** Prometheus exporters included in the same stack.

---

## ğŸ§© Components

### 1. LlamaIndex (`tools/llamaindex/`)

```
tools/llamaindex/
â”œâ”€â”€ Dockerfile.ingestion     # Ingestion service image (FastAPI worker)
â”œâ”€â”€ Dockerfile.query         # Query service image (FastAPI endpoint)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ingestion/           # Watchers, chunkers, metadata enrichers
â”‚   â”œâ”€â”€ query/               # REST APIs, aggregations, similarity search
â”‚   â”œâ”€â”€ pipelines/           # Shared orchestration utilities
â”‚   â””â”€â”€ settings.py          # Centralized configuration
â””â”€â”€ scripts/
    â”œâ”€â”€ bootstrap.sh         # Creates collections, seeds metadata
    â””â”€â”€ sync.sh              # Sync helper for local datasets
```

**Capabilities**
- Chunking + embeddings via Ollama (`nomic-embed-text`).
- Metadata templates per collection (Docs, Runbooks, Reports).
- Hybrid search (dense + keyword) with Qdrant filters.
- Background ingestion workers with retry + backoff.

### 2. Ollama (`tools/ollama/`)

Local LLM runtime used by both ingestion and query flows.

- Default models: `llama3.1:8b` and `nomic-embed-text`.
- Configured through `tools/compose/docker-compose.4-4-rag-stack.yml` (ports 11434/11435).
- GPU optional; CPU fallback supported for development laptops.

---

## âš™ï¸ Running the Stack

```bash
# Build custom images
npm run docker:build rag

# Start RAG services + Ollama
bash scripts/docker/start-stacks.sh rag

# Or use compose directly
docker compose -f tools/compose/docker-compose.4-4-rag-stack.yml up -d
```

### Health Checks

| Service | Command |
|---------|---------|
| LlamaIndex Ingestion | `curl -s http://localhost:8201/health` |
| LlamaIndex Query     | `curl -s http://localhost:8202/health` |
| Ollama               | `curl -s http://localhost:11434/api/tags` |

### Useful Scripts

- `scripts/rag/sync-collections.sh` â€“ Syncs docs from `docs/content/` to the ingestion folder.
- `scripts/rag/test-query.sh` â€“ Smoke test against the query API.
- `scripts/rag/reset.sh` â€“ Drops Qdrant collections and recreates them from scratch.

---

## ğŸ“¦ Environment Variables

| Variable | Description | Default |
|----------|-------------|---------|
| `LLAMAINDEX_QUERY_URL` | Query API base URL | `http://localhost:8202` |
| `LLAMAINDEX_INGESTION_URL` | Ingestion API base URL | `http://localhost:8201` |
| `LLAMAINDEX_JWT` | Optional JWT for direct access | `dev-secret` |
| `QDRANT_URL` | Vector store endpoint | `http://localhost:6333` |
| `OLLAMA_BASE_URL` | LLM runtime | `http://localhost:11434` |

(All variables live in `config/.env.defaults` and inherit from the central `.env`).

---

## ğŸ§ª Testing & Validation

```bash
# Run ingestion unit tests
cd tools/llamaindex && pytest src/ingestion

# Run query unit tests
cd tools/llamaindex && pytest src/query

# Lint Python code
cd tools/llamaindex && ruff check src
```

### Manual Validation

1. Drop a Markdown file into `tools/llamaindex/data/docs`.
2. Tail ingestion logs: `docker logs -f rag-llamaindex-ingest`.
3. Query the collection:
   ```bash
   curl -s -X POST http://localhost:8202/query \
     -H 'Content-Type: application/json' \
     -d '{"collection":"docs","question":"Como reiniciar o Kestra?"}' | jq
   ```
4. Response should include highlights + answer from the ingested content.

This page intentionally focuses only on the maintained stack (LlamaIndex + Ollama); legacy multi-agent tooling has been fully deprecated and removed from the repository.
