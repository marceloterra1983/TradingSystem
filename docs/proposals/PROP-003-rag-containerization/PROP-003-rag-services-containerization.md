---
type: proposal
id: PROP-003
title: RAG Services Containerization
status: draft
created: 2025-10-31
updated: 2025-10-31
authors: [Claude Code]
reviewers: []
domain: infrastructure
tags: [docker, rag, architecture, microservices]
related: []
---

# PROP-003: RAG Services Containerization

## Executive Summary

Containerizar os RAG Services (Documentation API, LlamaIndex Query e Ingestion) em containers Docker independentes, mantendo os containers existentes em paralelo. Esta mudan√ßa tornar√° o RAG system mais port√°vel, escal√°vel e f√°cil de manter.

## Motivation

### Problemas Atuais

1. **Depend√™ncias Mistas**: RAG Services rodando junto com outros servi√ßos
2. **Dif√≠cil Escalonamento**: N√£o √© poss√≠vel escalar RAG Services independentemente
3. **Deployment Complexo**: Setup manual de Python, Node.js e depend√™ncias
4. **Isolamento Insuficiente**: Conflitos potenciais de depend√™ncias
5. **Portabilidade Limitada**: Dif√≠cil mover para outros ambientes

### Benef√≠cios da Containeriza√ß√£o

1. **Isolamento Completo**: Cada servi√ßo RAG em seu pr√≥prio container
2. **Escalabilidade**: Possibilidade de m√∫ltiplas r√©plicas
3. **Portabilidade**: Deploy f√°cil em qualquer ambiente com Docker
4. **Versionamento**: Imagens Docker versionadas
5. **Rollback Simples**: Voltar para vers√µes anteriores instantaneamente
6. **Health Checks**: Monitoramento integrado via Docker
7. **Resource Limits**: Controle fino de CPU/RAM por servi√ßo

## Design

### Arquitetura Proposta (Atualizada - 2025-10-31)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ            Docker Network: tradingsystem_backend                     ‚îÇ
‚îÇ            (Isolated with mTLS inter-service authentication)         ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                       ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                ‚îÇ
‚îÇ  ‚îÇ     rag-service      ‚îÇ  ‚îÇ  llamaindex-query    ‚îÇ                ‚îÇ
‚îÇ  ‚îÇ  (Node.js/Express)   ‚îÇ  ‚îÇ  (Python/FastAPI)    ‚îÇ                ‚îÇ
‚îÇ  ‚îÇ  Port: 3400:3400     ‚îÇ  ‚îÇ  Port: 8202:8202     ‚îÇ  üîí mTLS       ‚îÇ
‚îÇ  ‚îÇ  - JWT Auth          ‚îÇ  ‚îÇ  - Query Engine      ‚îÇ                ‚îÇ
‚îÇ  ‚îÇ  - Circuit Breaker   ‚îÇ  ‚îÇ  - Semantic Search   ‚îÇ                ‚îÇ
‚îÇ  ‚îÇ  - Rate Limiting     ‚îÇ  ‚îÇ  - Response Cache    ‚îÇ                ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                ‚îÇ
‚îÇ           ‚Üì (secured)                ‚Üì                               ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                ‚îÇ
‚îÇ  ‚îÇ llamaindex-ingestion ‚îÇ  ‚îÇ    redis-queue       ‚îÇ  NEW            ‚îÇ
‚îÇ  ‚îÇ  (Python/FastAPI)    ‚îÇ  ‚îÇ  (Job Queue)         ‚îÇ                ‚îÇ
‚îÇ  ‚îÇ  Port: 8201:8201     ‚îÇ  ‚îÇ  Port: 6379 (int)    ‚îÇ                ‚îÇ
‚îÇ  ‚îÇ  - Job State Mgmt    ‚îÇ  ‚îÇ  - Dist. Locking     ‚îÇ                ‚îÇ
‚îÇ  ‚îÇ  - Idempotent Ingest ‚îÇ  ‚îÇ  - Cache Layer       ‚îÇ                ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                ‚îÇ
‚îÇ           ‚Üì                          ‚Üì                               ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                ‚îÇ
‚îÇ  ‚îÇ   ollama-embeddings  ‚îÇ  ‚îÇ     ollama-llm       ‚îÇ  üîí Internal   ‚îÇ
‚îÇ  ‚îÇ   (CPU-optimized)    ‚îÇ  ‚îÇ   (GPU-accelerated)  ‚îÇ  Only          ‚îÇ
‚îÇ  ‚îÇ   Port: - (internal) ‚îÇ  ‚îÇ   Port: - (internal) ‚îÇ                ‚îÇ
‚îÇ  ‚îÇ   - nomic-embed-text ‚îÇ  ‚îÇ   - llama3.1         ‚îÇ                ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                ‚îÇ
‚îÇ           ‚Üì                          ‚Üì                               ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                   ‚îÇ
‚îÇ  ‚îÇ              data-qdrant (external)           ‚îÇ                   ‚îÇ
‚îÇ  ‚îÇ              Vector Database                  ‚îÇ                   ‚îÇ
‚îÇ  ‚îÇ              Port: 6333                       ‚îÇ                   ‚îÇ
‚îÇ  ‚îÇ              - Collections                    ‚îÇ                   ‚îÇ
‚îÇ  ‚îÇ              - Vector Storage                 ‚îÇ                   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                   ‚îÇ
‚îÇ                                                                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Trust Boundaries:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ External ‚Üí rag-service (JWT Auth)                                  ‚îÇ
‚îÇ rag-service ‚Üí llamaindex services (mTLS/Shared Secret)             ‚îÇ
‚îÇ llamaindex services ‚Üí Redis/Ollama/Qdrant (Internal network only)  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Key Architecture Improvements:**
- ‚úÖ **Security**: mTLS between services, no external Ollama exposure
- ‚úÖ **Resilience**: Circuit breakers, retry with backoff, distributed locking
- ‚úÖ **State Management**: Redis for job queue and progress tracking
- ‚úÖ **High Availability**: Separate Ollama instances for embeddings vs. LLM
- ‚úÖ **Port Consistency**: Internal ports match external (3400:3400, 8201:8201, 8202:8202)

### Security Architecture (NEW)

#### Inter-Service Authentication

**Implementation**: Shared secret-based authentication for internal services

```yaml
# Environment variables for all services
INTER_SERVICE_SECRET=${INTER_SERVICE_SECRET}  # Required, no default
INTER_SERVICE_SECRET_HEADER=X-Internal-Auth
```

**Middleware Implementation** (FastAPI):
```python
# shared/auth_middleware.py
from fastapi import Request, HTTPException
import os

async def verify_inter_service_auth(request: Request):
    expected_secret = os.getenv('INTER_SERVICE_SECRET')
    if not expected_secret:
        raise RuntimeError('INTER_SERVICE_SECRET not configured')

    actual_secret = request.headers.get('X-Internal-Auth')
    if actual_secret != expected_secret:
        raise HTTPException(status_code=403, detail='Invalid service authentication')
```

**Client Implementation** (Node.js):
```javascript
// rag-service making request to llamaindex-query
const response = await fetch(`${LLAMAINDEX_QUERY_URL}/query`, {
  method: 'POST',
  headers: {
    'Content-Type': 'application/json',
    'X-Internal-Auth': process.env.INTER_SERVICE_SECRET
  },
  body: JSON.stringify(queryData)
});
```

#### Secrets Validation

**Startup Validation** (all services):
```javascript
// Node.js (rag-service)
if (process.env.NODE_ENV === 'production') {
  const required = ['JWT_SECRET_KEY', 'INTER_SERVICE_SECRET'];
  for (const key of required) {
    if (!process.env[key] || process.env[key].startsWith('dev-')) {
      throw new Error(`${key} must be set in production`);
    }
  }
}
```

```python
# Python (llamaindex services)
import os
import sys

def validate_secrets():
    required = ['INTER_SERVICE_SECRET']
    for key in required:
        value = os.getenv(key)
        if not value:
            print(f'FATAL: {key} not set', file=sys.stderr)
            sys.exit(1)
        if value.startswith('dev-'):
            print(f'FATAL: {key} uses dev default in production', file=sys.stderr)
            sys.exit(1)

validate_secrets()
```

#### Rate Limiting

**Implementation** (rag-service):
```javascript
import rateLimit from 'express-rate-limit';
import RedisStore from 'rate-limit-redis';
import { createClient } from 'redis';

const redisClient = createClient({
  url: process.env.REDIS_URL
});

const queryLimiter = rateLimit({
  store: new RedisStore({
    client: redisClient,
    prefix: 'rl:query:'
  }),
  windowMs: 60 * 1000,  // 1 minute
  max: 10,  // 10 queries per minute per user
  message: 'Too many queries, please try again later'
});

app.post('/api/v1/rag/query', queryLimiter, handleQuery);
```

### Resilience Patterns (NEW)

#### Circuit Breaker

**Implementation** (rag-service ‚Üí llamaindex services):
```javascript
import CircuitBreaker from 'opossum';

const queryServiceBreaker = new CircuitBreaker(
  async (query) => {
    const response = await fetch(`${LLAMAINDEX_QUERY_URL}/query`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'X-Internal-Auth': process.env.INTER_SERVICE_SECRET
      },
      body: JSON.stringify(query),
      timeout: 30000
    });

    if (!response.ok) {
      throw new Error(`Query service returned ${response.status}`);
    }

    return response.json();
  },
  {
    timeout: 30000,          // 30s timeout
    errorThresholdPercentage: 50,  // Open circuit at 50% errors
    resetTimeout: 30000,     // Try again after 30s
    volumeThreshold: 10      // Minimum 10 requests to calculate percentage
  }
);

// Usage
try {
  const result = await queryServiceBreaker.fire(queryData);
  return result;
} catch (err) {
  if (queryServiceBreaker.opened) {
    return { error: 'Query service is currently unavailable' };
  }
  throw err;
}
```

#### Retry with Exponential Backoff

**Implementation** (Python llamaindex services ‚Üí Ollama):
```python
import asyncio
from tenacity import (
    retry,
    stop_after_attempt,
    wait_exponential,
    retry_if_exception_type
)
import httpx

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=1, max=10),
    retry=retry_if_exception_type((httpx.TimeoutException, httpx.ConnectError))
)
async def call_ollama_with_retry(prompt: str):
    async with httpx.AsyncClient(timeout=30.0) as client:
        response = await client.post(
            f"{OLLAMA_BASE_URL}/api/generate",
            json={"model": OLLAMA_MODEL, "prompt": prompt}
        )
        response.raise_for_status()
        return response.json()
```

### State Management Architecture (NEW)

#### Redis Job Queue

**Job States**:
- `PENDING`: Job created, waiting to start
- `PROCESSING`: Job is being processed
- `COMPLETED`: Job finished successfully
- `FAILED`: Job failed (with retry count)
- `CANCELLED`: Job cancelled by user

**Schema**:
```json
{
  "job_id": "ingest-20251031-123456",
  "type": "ingestion",
  "status": "PROCESSING",
  "created_at": "2025-10-31T12:34:56Z",
  "started_at": "2025-10-31T12:35:00Z",
  "updated_at": "2025-10-31T12:35:30Z",
  "payload": {
    "directory": "/data/docs",
    "collection": "documentation__nomic",
    "chunk_size": 512,
    "chunk_overlap": 50
  },
  "progress": {
    "total_files": 1000,
    "processed_files": 250,
    "percentage": 25
  },
  "result": null,
  "error": null,
  "retry_count": 0
}
```

**Implementation** (llamaindex-ingestion):
```python
import redis
import json
from datetime import datetime
from typing import Optional

class JobQueue:
    def __init__(self, redis_url: str):
        self.redis = redis.from_url(redis_url)

    def create_job(self, job_type: str, payload: dict) -> str:
        job_id = f"{job_type}-{datetime.utcnow().strftime('%Y%m%d-%H%M%S')}"
        job = {
            'job_id': job_id,
            'type': job_type,
            'status': 'PENDING',
            'created_at': datetime.utcnow().isoformat(),
            'payload': payload,
            'progress': {'total_files': 0, 'processed_files': 0, 'percentage': 0},
            'retry_count': 0
        }
        self.redis.setex(f'job:{job_id}', 86400, json.dumps(job))  # 24h TTL
        self.redis.lpush('queue:ingestion', job_id)
        return job_id

    def update_progress(self, job_id: str, processed: int, total: int):
        job = self.get_job(job_id)
        if job:
            job['progress'] = {
                'total_files': total,
                'processed_files': processed,
                'percentage': int((processed / total) * 100) if total > 0 else 0
            }
            job['updated_at'] = datetime.utcnow().isoformat()
            self.redis.setex(f'job:{job_id}', 86400, json.dumps(job))

    def complete_job(self, job_id: str, result: dict):
        job = self.get_job(job_id)
        if job:
            job['status'] = 'COMPLETED'
            job['result'] = result
            job['updated_at'] = datetime.utcnow().isoformat()
            self.redis.setex(f'job:{job_id}', 86400, json.dumps(job))

    def fail_job(self, job_id: str, error: str):
        job = self.get_job(job_id)
        if job:
            job['status'] = 'FAILED'
            job['error'] = error
            job['retry_count'] += 1
            job['updated_at'] = datetime.utcnow().isoformat()
            self.redis.setex(f'job:{job_id}', 86400, json.dumps(job))

    def get_job(self, job_id: str) -> Optional[dict]:
        data = self.redis.get(f'job:{job_id}')
        return json.loads(data) if data else None
```

#### Distributed Locking

**Implementation** (prevent concurrent ingestion to same collection):
```python
from redis.lock import Lock
from contextlib import contextmanager

@contextmanager
def acquire_collection_lock(redis_client, collection: str, timeout: int = 300):
    lock = Lock(redis_client, f'lock:collection:{collection}', timeout=timeout)
    acquired = lock.acquire(blocking=True, blocking_timeout=10)
    if not acquired:
        raise RuntimeError(f'Failed to acquire lock for collection {collection}')
    try:
        yield
    finally:
        lock.release()

# Usage in ingestion endpoint
@app.post('/ingest/directory')
async def ingest_directory(request: IngestRequest):
    with acquire_collection_lock(redis_client, request.collection):
        # Perform ingestion
        result = await ingest_documents(request.directory, request.collection)
        return result
```

### High Availability Architecture (NEW)

#### Ollama Service Separation

**Rationale**: Separate CPU-bound embeddings from GPU-bound LLM generation to prevent resource contention and enable independent scaling.

**ollama-embeddings** (CPU-optimized):
```yaml
ollama-embeddings:
  image: ollama/ollama:latest
  container_name: rag-ollama-embeddings
  deploy:
    resources:
      limits:
        cpus: '4.0'
        memory: 4G
  environment:
    - OLLAMA_NUM_PARALLEL=8  # High concurrency for embeddings
    - OLLAMA_MODELS=/models
  volumes:
    - ollama_embeddings_models:/models
  networks:
    - tradingsystem_backend
  healthcheck:
    test: ["CMD-SHELL", "ollama list | grep nomic-embed-text"]
    interval: 30s
```

**ollama-llm** (GPU-accelerated):
```yaml
ollama-llm:
  image: ollama/ollama:latest
  container_name: rag-ollama-llm
  runtime: nvidia
  deploy:
    resources:
      limits:
        cpus: '4.0'
        memory: 16G
      reservations:
        devices:
          - driver: nvidia
            count: 1
            capabilities: [gpu]
  environment:
    - OLLAMA_NUM_PARALLEL=2  # Lower concurrency for LLM
    - OLLAMA_MODELS=/models
  volumes:
    - ollama_llm_models:/models
  networks:
    - tradingsystem_backend
  healthcheck:
    test: ["CMD-SHELL", "ollama list | grep llama3.1"]
    interval: 30s
```

**Service Configuration Updates**:
```yaml
# llamaindex-ingestion uses embeddings only
OLLAMA_EMBED_URL=http://rag-ollama-embeddings:11434
OLLAMA_EMBED_MODEL=nomic-embed-text

# llamaindex-query uses both
OLLAMA_EMBED_URL=http://rag-ollama-embeddings:11434
OLLAMA_LLM_URL=http://rag-ollama-llm:11434
OLLAMA_LLM_MODEL=llama3.1
```

### Containers Detalhados

#### 0. redis-queue Container (NEW)

**Prop√≥sito**: Job queue, distributed locking, cache layer

```yaml
Service: redis-queue
Image: redis:7-alpine
Porta interna: 6379 (internal only)
Network: tradingsystem_backend
Volume: redis_data
```

**Features**:
- Job queue for ingestion tasks
- Distributed locks for collection safety
- Query result caching (TTL-based)
- Rate limiting storage

**Configuration**:
```yaml
redis-queue:
  image: redis:7-alpine
  container_name: rag-redis-queue
  deploy:
    resources:
      limits:
        cpus: '1.0'
        memory: 512M
  volumes:
    - redis_data:/data
  networks:
    - tradingsystem_backend
  command: >
    redis-server
    --maxmemory 256mb
    --maxmemory-policy allkeys-lru
    --appendonly yes
  healthcheck:
    test: ["CMD", "redis-cli", "ping"]
    interval: 10s
    timeout: 3s
    retries: 3
```

#### 1. rag-service Container

**Prop√≥sito**: API Gateway/RAG Orchestrator

```yaml
Service: rag-service
Image: tradingsystem/rag-service:latest
Base: node:22-alpine
Porta interna: 3000
Porta exposta: 3400
Network: tradingsystem_backend (externa)
```

**Features**:
- Routes: `/api/v1/rag/*`
- Services: CollectionService, RagProxyService
- Dependencies: JWT auth, error handling
- Health Check: GET /api/health
- Restart Policy: unless-stopped

**Environment Variables** (Updated):
```env
# Service Configuration
NODE_ENV=production
PORT=3400
LOG_LEVEL=info

# RAG Services (Updated URLs with port consistency)
LLAMAINDEX_QUERY_URL=http://rag-llamaindex-query:8202
LLAMAINDEX_INGESTION_URL=http://rag-llamaindex-ingest:8201
QDRANT_URL=http://data-qdrant:6333
QDRANT_COLLECTION=documentation__nomic

# Ollama Services (Separated for HA)
OLLAMA_EMBED_URL=http://rag-ollama-embeddings:11434
OLLAMA_LLM_URL=http://rag-ollama-llm:11434
OLLAMA_EMBEDDING_MODEL=nomic-embed-text
OLLAMA_LLM_MODEL=llama3.1

# Redis Infrastructure
REDIS_URL=redis://rag-redis-queue:6379

# Security (Enhanced)
JWT_SECRET_KEY=${JWT_SECRET_KEY}  # REQUIRED, no default in production
JWT_ALGORITHM=HS256
INTER_SERVICE_SECRET=${INTER_SERVICE_SECRET}  # REQUIRED for service auth

# Resilience & Performance
CIRCUIT_BREAKER_ENABLED=true
CIRCUIT_BREAKER_THRESHOLD=50  # 50% error rate
CIRCUIT_BREAKER_TIMEOUT=30000  # 30s
RAG_TIMEOUT_MS=30000
STATUS_CACHE_TTL_MS=30000
RATE_LIMIT_WINDOW_MS=60000  # 1 minute
RATE_LIMIT_MAX_REQUESTS=10  # 10 requests per window
```

**Dockerfile**:
```dockerfile
FROM node:22-alpine AS base
WORKDIR /app
COPY package*.json ./
RUN npm ci --only=production && npm cache clean --force

FROM node:22-alpine AS production
RUN apk add --no-cache dumb-init curl && \
    addgroup -g 1001 -S nodejs && adduser -S nodejs -u 1001
WORKDIR /app
COPY --from=base --chown=nodejs:nodejs /app/node_modules ./node_modules
COPY --chown=nodejs:nodejs package*.json ./
COPY --chown=nodejs:nodejs prisma ./prisma
COPY --chown=nodejs:nodejs src ./src
USER nodejs
EXPOSE 3400
HEALTHCHECK --interval=30s --timeout=10s --start-period=40s --retries=3 \
  CMD node -e "require('http').get('http://localhost:3400/health', (r) => { process.exit(r.statusCode === 200 ? 0 : 1); })"
ENTRYPOINT ["dumb-init", "--"]
CMD ["node", "src/server.js"]
```

#### 2. llamaindex-query Container

**Prop√≥sito**: Query e busca sem√¢ntica

```yaml
Service: llamaindex-query
Image: tradingsystem/llamaindex-query:latest
Base: python:3.11-slim
Porta interna: 8000
Porta exposta: 8202
Network: tradingsystem_backend (externa)
GPU: opcional (--gpus all)
```

**Features**:
- FastAPI endpoints: /query, /search, /health
- Vector search via Qdrant
- LLM integration via Ollama
- GPU scheduling and management
- Response caching

**Environment Variables** (Updated):
```env
# Service Configuration
PYTHONUNBUFFERED=1
LOG_LEVEL=info

# Vector Database
QDRANT_HOST=data-qdrant
QDRANT_PORT=6333
QDRANT_COLLECTION=documentation__nomic

# LLM Configuration (Separated Services)
OLLAMA_EMBED_URL=http://rag-ollama-embeddings:11434
OLLAMA_LLM_URL=http://rag-ollama-llm:11434
OLLAMA_EMBED_MODEL=nomic-embed-text
OLLAMA_LLM_MODEL=llama3.1
OLLAMA_REQUEST_TIMEOUT=300

# Redis Infrastructure
REDIS_URL=redis://rag-redis-queue:6379

# Security (NEW)
INTER_SERVICE_SECRET=${INTER_SERVICE_SECRET}  # REQUIRED

# Performance & Resilience
CACHE_TTL_SECONDS=300
RETRY_ATTEMPTS=3
RETRY_BACKOFF_MIN_SECONDS=1
RETRY_BACKOFF_MAX_SECONDS=10
```

**Dockerfile**:
```dockerfile
FROM python:3.11-slim

WORKDIR /app

# System dependencies
RUN apt-get update && apt-get install -y --no-install-recommends \
    build-essential \
    curl \
    libmagic1 \
    && rm -rf /var/lib/apt/lists/*

# Python dependencies
COPY tools/llamaindex/requirements.txt ./requirements.txt
RUN pip install --no-cache-dir -r requirements.txt

# Assets e aplica√ß√£o
ENV NLTK_DATA=/usr/local/nltk_data PYTHONUNBUFFERED=1
RUN python -m nltk.downloader -d /usr/local/nltk_data punkt && \
    test -f /usr/local/nltk_data/tokenizers/punkt/english.pickle
COPY tools/llamaindex/query_service ./query_service
COPY tools/llamaindex/shared ./shared

# Non-root user (before health check to test as appuser)
RUN useradd -m -u 1000 appuser && chown -R appuser:appuser /app
USER appuser

EXPOSE 8202

# Health check
HEALTHCHECK --interval=30s --timeout=5s --start-period=30s --retries=3 \
  CMD curl -f http://localhost:8202/health || exit 1

CMD ["uvicorn", "query_service.main:app", "--host", "0.0.0.0", "--port", "8202"]
```

#### 3. llamaindex-ingestion Container

**Prop√≥sito**: Ingest√£o e processamento de documentos

```yaml
Service: llamaindex-ingestion
Image: tradingsystem/llamaindex-ingestion:latest
Base: python:3.11-slim
Porta interna: 8000
Porta exposta: 8201
Network: tradingsystem_backend (externa)
Volumes:
  - ../../docs/content:/data/docs:ro
  - ../../:/data/tradingsystem:ro
```

**Features**:
- Document ingestion: /ingest/directory
- Chunk processing and optimization
- Collection management
- Multi-format support (MD, MDX, PDF, TXT)

**Environment Variables** (Updated):
```env
# Service Configuration
PYTHONUNBUFFERED=1
LOG_LEVEL=info

# Data Paths
DOCS_DIR=/data/docs
COLLECTIONS_CONFIG=/app/collection-config.json

# Vector Database
QDRANT_HOST=data-qdrant
QDRANT_PORT=6333

# LLM Configuration (Embeddings Only)
OLLAMA_EMBED_URL=http://rag-ollama-embeddings:11434
OLLAMA_EMBED_MODEL=nomic-embed-text
OLLAMA_REQUEST_TIMEOUT=300

# Redis Infrastructure
REDIS_URL=redis://rag-redis-queue:6379

# Security (NEW)
INTER_SERVICE_SECRET=${INTER_SERVICE_SECRET}  # REQUIRED

# Processing Configuration
CHUNK_SIZE=512
CHUNK_OVERLAP=50
MAX_WORKERS=4

# Job Management (NEW)
JOB_QUEUE_ENABLED=true
JOB_TTL_SECONDS=86400  # 24 hours
DISTRIBUTED_LOCK_TIMEOUT=300  # 5 minutes
```

**Dockerfile**:
```dockerfile
FROM python:3.11-slim

WORKDIR /app

# System dependencies
RUN apt-get update && apt-get install -y --no-install-recommends \
    build-essential \
    curl \
    libmagic1 \
    && rm -rf /var/lib/apt/lists/*

# Python dependencies
COPY tools/llamaindex/requirements.txt ./requirements.txt
RUN pip install --no-cache-dir -r requirements.txt

# Copy application
ENV NLTK_DATA=/usr/local/nltk_data PYTHONUNBUFFERED=1
RUN python -m nltk.downloader -d /usr/local/nltk_data punkt && \
    test -f /usr/local/nltk_data/tokenizers/punkt/english.pickle
COPY tools/llamaindex/ingestion_service ./ingestion_service
COPY tools/llamaindex/shared ./shared
COPY tools/llamaindex/collection-config.json ./

# Non-root user (before health check to test as appuser)
RUN useradd -m -u 1000 appuser && chown -R appuser:appuser /app
USER appuser

EXPOSE 8201

# Health check
HEALTHCHECK --interval=30s --timeout=5s --start-period=30s --retries=3 \
  CMD curl -f http://localhost:8201/health || exit 1

CMD ["uvicorn", "ingestion_service.main:app", "--host", "0.0.0.0", "--port", "8201"]
```

#### 4. Reutiliza√ß√£o do servi√ßo `data-qdrant`

**Prop√≥sito**: Vetorizar dados com o banco compartilhado existente

- O projeto j√° mant√©m o container `data-qdrant` definido em `tools/compose/docker-compose.database.yml` e reutilizado por outros stacks.
- A estrat√©gia recomendada √© **n√£o** criar um novo container dedicado no stack RAG. Em vez disso, todas as aplica√ß√µes RAG devem depender do servi√ßo existente via DNS interno (`data-qdrant:6333`).
- Benef√≠cios:
  - Evita competi√ß√£o por portas e migra√ß√µes adicionais.
  - Mant√©m os dados consolidados num √∫nico volume (`qdrant_data`).
  - Garante compatibilidade com os scripts de backup e sa√∫de atuais.

**A√ß√µes necess√°rias**:
1. Garantir que o stack de dados esteja ativo antes de subir o stack RAG.
2. Validar sa√∫de do `data-qdrant` atrav√©s de `scripts/maintenance/health-check-all.sh`.
3. Documentar o requisito de inicializa√ß√£o sequencial (dados ‚Üí RAG).

#### 5. ollama-embeddings Container (NEW - Separated for HA)

**Prop√≥sito**: Embedding generation (CPU-optimized, high concurrency)

```yaml
Service: ollama-embeddings
Image: ollama/ollama:latest
Port: 11434 (internal only - NO external exposure)
Network: tradingsystem_backend
Volumes:
  - ollama_embeddings_models:/root/.ollama
```

**Features**:
- Dedicated to embedding generation only (nomic-embed-text)
- CPU-optimized with high parallelism (OLLAMA_NUM_PARALLEL=8)
- No GPU required, lighter resource footprint
- Independent scaling from LLM service

#### 6. ollama-llm Container (NEW - Separated for HA)

**Prop√≥sito**: LLM generation (GPU-accelerated, lower concurrency)

```yaml
Service: ollama-llm
Image: ollama/ollama:latest
Port: 11434 (internal only - NO external exposure)
Network: tradingsystem_backend
GPU: nvidia (optional but recommended)
Volumes:
  - ollama_llm_models:/root/.ollama
```

**Features**:
- Dedicated to LLM text generation (llama3.1)
- GPU-accelerated for faster inference
- Lower parallelism (OLLAMA_NUM_PARALLEL=2) to manage GPU memory
- Independent scaling from embedding service

### Docker Compose Configuration (Updated 2025-10-31)

**Arquivo**: `tools/compose/docker-compose.rag.yml`

**Major Changes**:
- ‚úÖ Added redis-queue for job management
- ‚úÖ Separated Ollama into embeddings and LLM services
- ‚úÖ Fixed port mappings (internal = external)
- ‚úÖ Removed external port exposure for Ollama
- ‚úÖ Added inter-service authentication support
- ‚úÖ Enhanced health checks

```yaml
version: "3.8"

services:
  # Redis for job queue, caching, and distributed locking
  redis-queue:
    image: redis:7-alpine
    container_name: rag-redis-queue
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 512M
    volumes:
      - redis_data:/data
    networks:
      - tradingsystem_backend
    command: >
      redis-server
      --maxmemory 256mb
      --maxmemory-policy allkeys-lru
      --appendonly yes
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 3s
      retries: 3
      start_period: 5s
    restart: unless-stopped

  # Ollama Embeddings Service (CPU-optimized)
  ollama-embeddings:
    image: "${IMG_RAG_OLLAMA:-ollama/ollama}:${IMG_VERSION:-latest}"
    container_name: rag-ollama-embeddings
    deploy:
      resources:
        limits:
          cpus: '4.0'
          memory: 4G
        reservations:
          cpus: '2.0'
          memory: 2G
    environment:
      - OLLAMA_NUM_PARALLEL=8
      - OLLAMA_MODELS=/root/.ollama
    volumes:
      - ollama_embeddings_models:/root/.ollama
    networks:
      - tradingsystem_backend
    healthcheck:
      test: ["CMD-SHELL", "ollama list | grep -q nomic-embed-text || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s  # Allow time for model loading
    restart: unless-stopped
    # NO PORT MAPPING - internal only for security

  # Ollama LLM Service (GPU-accelerated)
  ollama-llm:
    image: "${IMG_RAG_OLLAMA:-ollama/ollama}:${IMG_VERSION:-latest}"
    container_name: rag-ollama-llm
    runtime: nvidia
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
        limits:
          memory: 16G
          cpus: "4"
        reservations:
          cpus: "2"
          memory: 8G
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - OLLAMA_NUM_PARALLEL=2
      - OLLAMA_MODELS=/root/.ollama
    volumes:
      - ollama_llm_models:/root/.ollama
    networks:
      - tradingsystem_backend
    healthcheck:
      test: ["CMD-SHELL", "ollama list | grep -q llama3.1 || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s  # Allow time for model loading
    restart: unless-stopped
    # NO PORT MAPPING - internal only for security

  llamaindex-ingestion:
    image: "${IMG_RAG_LLAMAINDEX_INGEST:-tradingsystem/llamaindex-ingest}:${IMG_VERSION:-latest}"
    container_name: rag-llamaindex-ingest
    build:
      context: ../..
      dockerfile: tools/llamaindex/Dockerfile.ingestion
    deploy:
      resources:
        limits:
          memory: 4G
          cpus: "2"
        reservations:
          cpus: "1"
          memory: 2G
    ports:
      - "${LLAMAINDEX_INGESTION_PORT:-8201}:8201"  # FIXED: internal = external
    restart: unless-stopped
    env_file:
      - ../../.env
    environment:
      # Vector Database
      - QDRANT_HOST=data-qdrant
      - QDRANT_PORT=${QDRANT_PORT:-6333}
      - QDRANT_COLLECTION=${QDRANT_COLLECTION:-documentation__nomic}
      # Ollama (Embeddings Only)
      - OLLAMA_EMBED_URL=http://rag-ollama-embeddings:11434
      - OLLAMA_EMBED_MODEL=${OLLAMA_EMBEDDING_MODEL:-nomic-embed-text}
      - OLLAMA_REQUEST_TIMEOUT=300
      # Redis
      - REDIS_URL=redis://rag-redis-queue:6379
      # Security
      - INTER_SERVICE_SECRET=${INTER_SERVICE_SECRET}
      # Job Management
      - JOB_QUEUE_ENABLED=true
      - JOB_TTL_SECONDS=86400
      - DISTRIBUTED_LOCK_TIMEOUT=300
    volumes:
      - ../../docs/content:/data/docs:ro
    extra_hosts:
      - "host.docker.internal:host-gateway"
    networks:
      - tradingsystem_backend
    depends_on:
      redis-queue:
        condition: service_healthy
      ollama-embeddings:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8201/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  llamaindex-query:
    image: "${IMG_RAG_LLAMAINDEX_QUERY:-tradingsystem/llamaindex-query}:${IMG_VERSION:-latest}"
    container_name: rag-llamaindex-query
    build:
      context: ../..
      dockerfile: tools/llamaindex/Dockerfile.query
    deploy:
      resources:
        limits:
          memory: 4G
          cpus: "2"
        reservations:
          cpus: "1"
          memory: 2G
    ports:
      - "${LLAMAINDEX_QUERY_PORT:-8202}:8202"  # FIXED: internal = external
    restart: unless-stopped
    env_file:
      - ../../.env
    environment:
      # Vector Database
      - QDRANT_HOST=data-qdrant
      - QDRANT_PORT=${QDRANT_PORT:-6333}
      - QDRANT_COLLECTION=${QDRANT_COLLECTION:-documentation__nomic}
      # Ollama (Both Embeddings and LLM)
      - OLLAMA_EMBED_URL=http://rag-ollama-embeddings:11434
      - OLLAMA_LLM_URL=http://rag-ollama-llm:11434
      - OLLAMA_EMBED_MODEL=${OLLAMA_EMBEDDING_MODEL:-nomic-embed-text}
      - OLLAMA_LLM_MODEL=${OLLAMA_MODEL:-llama3.1}
      - OLLAMA_REQUEST_TIMEOUT=300
      # Redis
      - REDIS_URL=redis://rag-redis-queue:6379
      # Security
      - INTER_SERVICE_SECRET=${INTER_SERVICE_SECRET}
      # Performance & Resilience
      - CACHE_TTL_SECONDS=300
      - RETRY_ATTEMPTS=3
      - RETRY_BACKOFF_MIN_SECONDS=1
      - RETRY_BACKOFF_MAX_SECONDS=10
    extra_hosts:
      - "host.docker.internal:host-gateway"
    networks:
      - tradingsystem_backend
    depends_on:
      redis-queue:
        condition: service_healthy
      ollama-embeddings:
        condition: service_healthy
      ollama-llm:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8202/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  rag-service:
    image: "${IMG_RAG_SERVICE:-tradingsystem/rag-service}:${IMG_VERSION:-latest}"
    container_name: rag-service
    build:
      context: ../..
      dockerfile: backend/api/documentation-api/Dockerfile
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: "1.0"
        reservations:
          cpus: "0.5"
          memory: 256M
    ports:
      - "${DOCUMENTATION_API_PORT:-3400}:3400"  # FIXED: internal = external
    restart: unless-stopped
    env_file:
      - ../../.env
    environment:
      # Service Configuration
      - NODE_ENV=production
      - PORT=3400
      - LOG_LEVEL=info
      # RAG Services (Updated URLs with port consistency)
      - LLAMAINDEX_QUERY_URL=http://rag-llamaindex-query:8202
      - LLAMAINDEX_INGESTION_URL=http://rag-llamaindex-ingest:8201
      - QDRANT_URL=http://data-qdrant:6333
      - QDRANT_COLLECTION=${QDRANT_COLLECTION:-documentation__nomic}
      # Ollama Services (Separated for HA)
      - OLLAMA_EMBED_URL=http://rag-ollama-embeddings:11434
      - OLLAMA_LLM_URL=http://rag-ollama-llm:11434
      - OLLAMA_EMBEDDING_MODEL=${OLLAMA_EMBEDDING_MODEL:-nomic-embed-text}
      - OLLAMA_LLM_MODEL=${OLLAMA_MODEL:-llama3.1}
      # Redis Infrastructure
      - REDIS_URL=redis://rag-redis-queue:6379
      # Security (Enhanced)
      - JWT_SECRET_KEY=${JWT_SECRET_KEY}
      - JWT_ALGORITHM=HS256
      - INTER_SERVICE_SECRET=${INTER_SERVICE_SECRET}
      # Resilience & Performance
      - CIRCUIT_BREAKER_ENABLED=true
      - CIRCUIT_BREAKER_THRESHOLD=50
      - CIRCUIT_BREAKER_TIMEOUT=30000
      - RAG_TIMEOUT_MS=30000
      - STATUS_CACHE_TTL_MS=30000
      - RATE_LIMIT_WINDOW_MS=60000
      - RATE_LIMIT_MAX_REQUESTS=10
    networks:
      - tradingsystem_backend
    depends_on:
      redis-queue:
        condition: service_healthy
      llamaindex-query:
        condition: service_healthy
      llamaindex-ingestion:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "node", "-e", "require('http').get('http://localhost:3400/health', (r) => { process.exit(r.statusCode === 200 ? 0 : 1); })"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

networks:
  tradingsystem_backend:
    external: true

volumes:
  redis_data:
    driver: local
  ollama_embeddings_models:
    driver: local
  ollama_llm_models:
    driver: local
```

> **Padr√µes de sa√∫de e seguran√ßa**  
> - Todos os servi√ßos carregam vari√°veis via `../../.env`, cumprindo a diretriz central do projeto.  
> - As imagens devem ser versionadas (substituir `latest` por tags espec√≠ficas durante a implementa√ß√£o).  
> - Sa√∫de integrada via `healthcheck`, com tempos alinhados √†s recomenda√ß√µes do agente docker-health-optimizer.

### Avalia√ß√£o docker-health-optimizer (2025-10-31)

- **Estado Geral**: Saud√°vel ‚Äî todos os servi√ßos definem health checks, usam usu√°rios n√£o-root e dependem do `.env` raiz.
- **Redes/Portas**: Apenas `rag-service` e `ollama` exp√µem portas para o host; query/ingestion permanecem internos na `tradingsystem_backend`.
- **Volumes**: Persist√™ncia apenas para `ollama_models`; Qdrant reutiliza stack externo (`data-qdrant`), evitando conflito de volumes.
- **Seguran√ßa**: Vari√°veis sens√≠veis carregadas do `.env`; imagens base atualizadas (`node:22-alpine`, `python:3.11-slim`).
- **Recomenda√ß√µes**: Definir tags imut√°veis em `${IMG_VERSION}` e `${IMG_RAG_*}` antes do deploy; executar `docker compose config` e `scripts/maintenance/health-check-all.sh` como verifica√ß√£o cont√≠nua.

**Tags de imagem sugeridas**:
- `IMG_RAG_OLLAMA=ollama/ollama`
- `IMG_RAG_LLAMAINDEX_INGEST=tradingsystem/llamaindex-ingest`
- `IMG_RAG_LLAMAINDEX_QUERY=tradingsystem/llamaindex-query`
- `IMG_RAG_SERVICE=tradingsystem/rag-service`
- `IMG_VERSION` apontando para uma tag imut√°vel (ex.: `2025.10.31`)

### Resource Limits

```yaml
services:
  rag-service:
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 512M
        reservations:
          cpus: '0.5'
          memory: 256M

  llamaindex-query:
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 2G
        reservations:
          cpus: '1.0'
          memory: 1G

  llamaindex-ingestion:
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 2G
        reservations:
          cpus: '1.0'
          memory: 1G

  qdrant:
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 4G
        reservations:
          cpus: '1.0'
          memory: 2G

  ollama:
    deploy:
      resources:
        limits:
          cpus: '4.0'
          memory: 8G
        reservations:
          cpus: '2.0'
          memory: 4G
```

## Implementation Plan

### Phase 1: Auditoria dos Dockerfiles existentes (Semana 1)

**Tasks**:
1. Revisar `backend/api/documentation-api/Dockerfile` (rag-service) para confirmar exposi√ß√£o interna (3000), healthcheck e uso de `dumb-init`.
2. Validar `tools/llamaindex/Dockerfile.query` e `Dockerfile.ingestion` (depend√™ncias, usu√°rios n√£o-root, bibliotecas NLTK, libmagic).
3. Criar/ajustar `.dockerignore` para cada servi√ßo reduzindo contexto de build.
4. Medir tamanho das imagens e aplicar otimiza√ß√µes (multi-stage, limpeza de cache) mantendo meta <500‚ÄØMB.
5. Rodar `docker build` via `scripts/docker/build-rag-images.sh` (novo script opcional) garantindo consist√™ncia.

**Deliverables**:
- Dockerfiles auditados e documentados.
- Checklist de conformidade com o agente docker-health-optimizer (usuarios n√£o root, healthcheck, env central).
- Relat√≥rio de tamanho das imagens com oportunidades de redu√ß√£o.

### Phase 2: Ajustes no Docker Compose (Semana 1)

**Tasks**:
1. Atualizar `tools/compose/docker-compose.rag.yml` conforme snippet acima, removendo depend√™ncia de rede dedicada e reutilizando `tradingsystem_backend`.
2. Garantir que todos os servi√ßos usam `env_file: ../../.env` (diretriz obrigat√≥ria).
3. Definir vari√°veis `IMG_*` com vers√µes espec√≠ficas (substituir `latest` por tags internas).
4. Integrar checagens de sa√∫de com `scripts/maintenance/health-check-all.sh` e documentar instru√ß√µes de uso.
5. Validar configura√ß√£o com o agente docker-health-optimizer (camada de seguran√ßa, limites de recursos, restart).

**Deliverables**:
- Compose atualizado e validado (`yamllint`, `docker compose config`).
- Tabela de vers√µes das imagens (fonte √∫nica em `.env` ou `config/docker-images.env`).
- Registro da execu√ß√£o do agente docker-health-optimizer com status **Healthy**.

### Phase 3: Scripts de Opera√ß√£o & Migra√ß√£o (Semana 2)

**Tasks**:
1. Criar scripts em `scripts/docker/rag/` para inicializa√ß√£o, parada, health check, backup e restaura√ß√£o.
2. Documentar sequ√™ncia de start considerando depend√™ncia do `data-qdrant` e do stack de dados.
3. Elaborar plano de rollback (switch r√°pido para servi√ßos legados).
4. Configurar monitoramento (Prometheus/Grafana) reutilizando dashboards existentes.

**Deliverables**:
```bash
scripts/docker/rag/
‚îú‚îÄ‚îÄ init.sh                 # Provisiona stack ap√≥s validar data-qdrant
‚îú‚îÄ‚îÄ migrate.sh              # Aplica migra√ß√µes e inicializa√ß√µes necess√°rias
‚îú‚îÄ‚îÄ rollback.sh             # Retorna para execu√ß√£o n√£o containerizada
‚îú‚îÄ‚îÄ health-check.sh         # Usa docker-health-optimizer + health endpoints
‚îî‚îÄ‚îÄ backup.sh               # Automatiza dumps de volumes (ollama/qdrant)
```

### Phase 4: Testing & Validation (Semana 2)

**Tasks**:
1. Unit tests for containerized services
2. Integration tests
3. Performance benchmarks
4. Load testing
5. Failover testing

**Test Scenarios**:
- Container startup/shutdown
- Service discovery
- Network communication
- Volume persistence
- Health checks
- Auto-restart
- Resource limits

### Phase 5: Documentation & Deployment (Semana 3)

**Tasks**:
1. Update CLAUDE.md
2. Create deployment guide
3. Update architecture diagrams
4. Train team
5. Production deployment

**Documentation**:
- `docs/content/tools/rag/docker-deployment.mdx`
- `docs/content/tools/rag/troubleshooting.mdx`
- `docs/content/diagrams/rag-architecture.puml`

## Operational Considerations

### Startup Sequence

```bash
# 1. Garantir rede compartilhada ativa
docker network inspect tradingsystem_backend >/dev/null 2>&1 || \
  docker network create tradingsystem_backend

# 2. Subir servi√ßos de dados (qdrant incluso)
docker compose -f tools/compose/docker-compose.database.yml up -d data-qdrant

# 3. Subir runtime Ollama
docker compose -f tools/compose/docker-compose.rag.yml up -d ollama

# 4. Carregar modelos necess√°rios
docker exec rag-ollama ollama pull ${OLLAMA_MODEL:-llama3.1}
docker exec rag-ollama ollama pull ${OLLAMA_EMBEDDING_MODEL:-nomic-embed-text}

# 5. Subir servi√ßos LlamaIndex
docker compose -f tools/compose/docker-compose.rag.yml up -d llamaindex-ingestion llamaindex-query

# 6. Subir rag-service (gateway RAG)
docker compose -f tools/compose/docker-compose.rag.yml up -d rag-service

# 7. Verificar sa√∫de integrada
docker compose -f tools/compose/docker-compose.rag.yml ps
bash scripts/docker/rag/health-check.sh
```

### Monitoring

**Metrics to Track**:
- Container CPU/Memory usage
- Request latency (p50, p95, p99)
- Error rates
- Query throughput
- Vector database size
- Model loading time

**Tools**:
- Docker stats
- Prometheus + Grafana
- Logs via Docker logging driver
- Health endpoints

### Backup Strategy

```bash
# Backup Qdrant data
docker run --rm \
  -v rag-qdrant-data:/source:ro \
  -v $(pwd)/backups:/backup \
  alpine \
  tar -czf /backup/qdrant-$(date +%Y%m%d).tar.gz -C /source .

# Backup Ollama models
docker run --rm \
  -v rag-ollama-data:/source:ro \
  -v $(pwd)/backups:/backup \
  alpine \
  tar -czf /backup/ollama-$(date +%Y%m%d).tar.gz -C /source .
```

### Rollback Plan

```bash
# Stop containers
docker compose -f docker-compose.rag.yml down

# Restore previous version
docker compose -f docker-compose.rag.yml pull

# Start services
docker compose -f docker-compose.rag.yml up -d

# Verify health
bash scripts/docker/rag/health-check.sh
```

## Security Considerations

### Network Security
- Utiliza rede compartilhada `tradingsystem_backend` com isolamento por DNS interno
- Sem exposi√ß√£o direta dos servi√ßos internos (ingestion/query) para fora da rede
- Apenas `rag-service` e `ollama` mapeados para portas externas controladas

### Secrets Management
- JWT secrets via environment variables
- No secrets in images
- Use Docker secrets in production

### Container Security
- Non-root users in all containers
- Read-only volumes where possible
- Security scanning (Trivy, Snyk)
- Regular image updates

## Performance Considerations

### Image Optimization
- Multi-stage builds
- Alpine base images where possible
- Layer caching
- .dockerignore files
- Target size: <500MB per image

### Runtime Optimization
- Health check intervals
- Restart policies
- Resource limits
- Shared memory for Ollama
- Connection pooling

## Migration Path

### Backward Compatibility

Durante a migra√ß√£o, **ambos** os sistemas rodar√£o em paralelo:

1. **Legacy (Non-containerized)**:
   - Continue rodando via npm/python diretamente
   - Usa portas originais

2. **New (Containerized)**:
   - Roda em containers Docker
   - Usa portas alternativas durante transi√ß√£o

### Parallel Operation

```yaml
# Legacy ports (existing)
- Documentation API: 3401 (native)
- LlamaIndex Query: 8202 (native)
- LlamaIndex Ingestion: 8201 (native)

# Portas host (durante migra√ß√£o)
- Documentation API: 3410 ‚Üí 3400 (ap√≥s cutover) [mapeado para 3000 interno]
- LlamaIndex Query: 8210 ‚Üí 8202 (ap√≥s cutover) [mapeado para 8000 interno]
- LlamaIndex Ingestion: 8211 ‚Üí 8201 (ap√≥s cutover) [mapeado para 8000 interno]
```

### Cutover Plan

1. Deploy containers com portas alternativas
2. Testar funcionalidade completa
3. Redirecionar tr√°fego para containers
4. Monitorar por 24h
5. Desligar servi√ßos legacy
6. Atualizar portas dos containers para portas definitivas

## Success Criteria

### Must Have
- [ ] Quatro containers RAG (ollama, ingestion, query, rag-service) buildam com sucesso
- [ ] Integra√ß√£o comprovada com `data-qdrant` existente
- [ ] Health checks pass
- [ ] Data persists across restarts
- [ ] Performance equivalent to non-containerized
- [ ] Zero downtime deployment possible

### Should Have
- [ ] GPU support for Ollama/Query
- [ ] Automated backups
- [ ] Monitoring dashboards
- [ ] Resource limits configured
- [ ] Documentation complete

### Nice to Have
- [ ] Auto-scaling support
- [ ] Blue-green deployment
- [ ] Kubernetes manifests
- [ ] CI/CD integration

## Risks & Mitigation

| Risk | Impact | Probability | Mitigation |
|------|--------|-------------|------------|
| Performance degradation | High | Low | Benchmark before/after, optimize |
| Data loss during migration | High | Low | Backup before migration, test restore |
| Container networking issues | Medium | Medium | Test thoroughly, document troubleshooting |
| Increased complexity | Medium | High | Comprehensive documentation, training |
| Resource exhaustion | Medium | Medium | Set limits, monitor, alert |

## Alternatives Considered

### 1. Virtual Machines
- **Pros**: Complete isolation
- **Cons**: Heavy, slow startup, resource intensive
- **Decision**: Rejected - overkill for our needs

### 2. Kubernetes
- **Pros**: Enterprise-grade orchestration
- **Cons**: Complex, overkill for single-node
- **Decision**: Future consideration

### 3. Keep as-is (No containerization)
- **Pros**: No migration work
- **Cons**: Limits scalability, portability
- **Decision**: Rejected - long-term limitations

## References

- [Docker Best Practices](https://docs.docker.com/develop/dev-best-practices/)
- [FastAPI in Docker](https://fastapi.tiangolo.com/deployment/docker/)
- [Node.js Docker Best Practices](https://github.com/nodejs/docker-node/blob/main/docs/BestPractices.md)
- [Qdrant Docker](https://qdrant.tech/documentation/quick-start/)
- [Ollama Docker](https://hub.docker.com/r/ollama/ollama)

## Appendix

### A. Environment Variables Reference

Complete list in `docs/content/tools/rag/environment-variables.mdx`

### B. Troubleshooting Guide

Common issues and solutions in `docs/content/tools/rag/troubleshooting.mdx`

### C. Performance Benchmarks

Baseline metrics for comparison in `docs/content/tools/rag/benchmarks.mdx`

---

**Next Steps**: Review this proposal and approve to proceed with implementation.
