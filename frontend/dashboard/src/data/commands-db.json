{
  "schemaVersion": "1.1.0",
  "updatedAt": "2025-11-05T12:29:19.559Z",
  "commands": [
    {
      "command": "/add-performance-monitoring",
      "label": "`/add-performance-monitoring`",
      "category": "Observabilidade e Performance",
      "exemplos": [
        "`/add-performance-monitoring --apm`",
        "`/add-performance-monitoring --rum`",
        "`/add-performance-monitoring --custom`"
      ],
      "capacidades": "Implanta APM, tracing, metricas customizadas e alertas para aplicacoes.",
      "momentoIdeal": "Quando um servico ganha SLAs definidos e precisa de visibilidade (ex.: workspace API).",
      "exemploMomento": "Habilitar tracing no proxy RAG para diagnosticar latencia em chamadas encadeadas.",
      "tipoSaida": "Plano de integracao com ferramentas (ex.: New Relic, Datadog) contendo dashboards, alertas e endpoints monitorados.",
      "fileName": "add-performance-monitoring.md",
      "filePath": ".claude/commands/add-performance-monitoring.md",
      "fileContent": "# Add Performance Monitoring\n\nSetup application performance monitoring: **$ARGUMENTS**\n\n## Instructions\n\n1. **Performance Monitoring Strategy**\n   - Define key performance indicators (KPIs) and service level objectives (SLOs)\n   - Identify critical user journeys and performance bottlenecks\n   - Plan monitoring architecture and data collection strategy\n   - Assess existing monitoring infrastructure and integration points\n   - Define alerting thresholds and escalation procedures\n\n2. **Application Performance Monitoring (APM)**\n   - Set up comprehensive APM solution (New Relic, Datadog, AppDynamics)\n   - Configure distributed tracing for request lifecycle visibility\n   - Implement custom metrics and performance tracking\n   - Set up transaction monitoring and error tracking\n   - Configure performance profiling and diagnostics\n\n3. **Real User Monitoring (RUM)**\n   - Implement client-side performance tracking and web vitals monitoring\n   - Set up user experience metrics collection (LCP, FID, CLS, TTFB)\n   - Configure custom performance metrics for user interactions\n   - Monitor page load performance and resource loading\n   - Track user journey performance across different devices\n\n4. **Server Performance Monitoring**\n   - Monitor system metrics (CPU, memory, disk, network)\n   - Set up process and application-level monitoring\n   - Configure event loop lag and garbage collection monitoring\n   - Implement custom server performance metrics\n   - Monitor resource utilization and capacity planning\n\n5. **Database Performance Monitoring**\n   - Track database query performance and slow query identification\n   - Monitor database connection pool utilization\n   - Set up database performance metrics and alerting\n   - Implement query execution plan analysis\n   - Monitor database resource usage and optimization opportunities\n\n6. **Error Tracking and Monitoring**\n   - Implement comprehensive error tracking (Sentry, Bugsnag, Rollbar)\n   - Configure error categorization and impact analysis\n   - Set up error alerting and notification systems\n   - Track error trends and resolution metrics\n   - Implement error context and debugging information\n\n7. **Custom Metrics and Dashboards**\n   - Implement business metrics tracking (Prometheus, StatsD)\n   - Create performance dashboards and visualizations\n   - Configure custom alerting rules and thresholds\n   - Set up performance trend analysis and reporting\n   - Implement performance regression detection\n\n8. **Alerting and Notification System**\n   - Configure intelligent alerting based on performance thresholds\n   - Set up multi-channel notifications (email, Slack, PagerDuty)\n   - Implement alert escalation and on-call procedures\n   - Configure alert fatigue prevention and noise reduction\n   - Set up performance incident management workflows\n\n9. **Performance Testing Integration**\n   - Integrate monitoring with load testing and performance testing\n   - Set up continuous performance testing and monitoring\n   - Configure performance baseline tracking and comparison\n   - Implement performance test result analysis and reporting\n   - Monitor performance under different load scenarios\n\n10. **Performance Optimization Recommendations**\n    - Generate actionable performance insights and recommendations\n    - Implement automated performance analysis and reporting\n    - Set up performance optimization tracking and measurement\n    - Configure performance improvement validation\n    - Create performance optimization prioritization frameworks\n\nFocus on monitoring strategies that provide actionable insights for performance optimization. Ensure monitoring overhead is minimal and doesn't impact application performance.",
      "tags": ["observability", "monitoring"]
    },
    {
      "command": "/all-tools",
      "label": "`/all-tools`",
      "category": "Referencia e Organizacao",
      "exemplos": ["`/all-tools`"],
      "capacidades": "Lista todas as ferramentas MCP habilitadas mostrando assinatura e proposito.",
      "momentoIdeal": "Antes de iniciar uma sessao de automacao para saber quais recursos (filesystem, github, docker etc.) estao acessiveis.",
      "exemploMomento": "Quando for planejar uma acao complexa (ex.: rodar scripts em serie) e quiser validar se o servidor docker MCP esta ativo.",
      "tipoSaida": "Lista textual em bullets com nome da ferramenta, assinatura TypeScript e breve descricao.",
      "fileName": "all-tools.md",
      "filePath": ".claude/commands/all-tools.md",
      "fileContent": "# Display All Available Development Tools\n\nDisplay all available development tools\n\n*Command originally created by IndyDevDan (YouTube: https://www.youtube.com/@indydevdan) / DislerH (GitHub: https://github.com/disler)*\n\n## Instructions\n\nDisplay all available tools from your system prompt in the following format:\n\n1. **List each tool** with its TypeScript function signature\n2. **Include the purpose** of each tool as a suffix\n3. **Use double line breaks** between tools for readability\n4. **Format as bullet points** for clear organization\n\nThe output should help developers understand:\n- What tools are available in the current Claude Code session\n- The exact function signatures for reference\n- The primary purpose of each tool\n\nExample format:\n```typescript\n• functionName(parameters: Type): ReturnType - Purpose of the tool\n\n• anotherFunction(params: ParamType): ResultType - What this tool does\n```\n\nThis command is useful for:\n- Quick reference of available capabilities\n- Understanding tool signatures\n- Planning which tools to use for specific tasks",
      "tags": ["overview", "cli"]
    },
    {
      "command": "/architecture-review",
      "label": "`/architecture-review`",
      "category": "Arquitetura e Estrategia",
      "exemplos": [
        "`/architecture-review backend --dependencies`",
        "`/architecture-review frontend --modules`",
        "`/architecture-review --security`"
      ],
      "capacidades": "Faz auditoria arquitetural completa (estrutura, dependencias, seguranca, testes).",
      "momentoIdeal": "No fechamento de esteiras de auditoria (ex.: tp-capital) antes de aprovar refatores estruturais.",
      "exemploMomento": "Verificar se o `docs/documentation-api` segue principios de clean architecture antes de um refactor.",
      "tipoSaida": "Relatorio tecnico em markdown destacando achados, riscos, metricas e plano de melhorias.",
      "fileName": "architecture-review.md",
      "filePath": ".claude/commands/architecture-review.md",
      "fileContent": "# Architecture Review\n\nPerform comprehensive system architecture analysis and improvement planning: **$ARGUMENTS**\n\n## Current Architecture Context\n\n- Project structure: !`find . -name \"*.js\" -o -name \"*.ts\" -o -name \"*.py\" -o -name \"*.go\" | head -5 && echo \"...\"`\n- Package dependencies: !`[ -f package.json ] && echo \"Node.js project\" || [ -f requirements.txt ] && echo \"Python project\" || [ -f go.mod ] && echo \"Go project\" || echo \"Multiple languages\"`\n- Testing framework: !`find . -name \"*.test.*\" -o -name \"*spec.*\" | head -3 && echo \"...\" || echo \"No test files found\"`\n- Documentation: !`find . -name \"README*\" -o -name \"*.md\" | wc -l` documentation files\n\n## Task\n\nExecute comprehensive architectural analysis with actionable improvement recommendations:\n\n**Review Scope**: Use $ARGUMENTS to focus on specific modules, design patterns, dependency analysis, or security architecture\n\n**Architecture Analysis Framework**:\n1. **System Structure Assessment** - Map component hierarchy, identify architectural patterns, analyze module boundaries, assess layered design\n2. **Design Pattern Evaluation** - Identify implemented patterns, assess pattern consistency, detect anti-patterns, evaluate pattern effectiveness\n3. **Dependency Architecture** - Analyze coupling levels, detect circular dependencies, evaluate dependency injection, assess architectural boundaries\n4. **Data Flow Analysis** - Trace information flow, evaluate state management, assess data persistence strategies, validate transformation patterns\n5. **Scalability & Performance** - Analyze scaling capabilities, evaluate caching strategies, assess bottlenecks, review resource management\n6. **Security Architecture** - Review trust boundaries, assess authentication patterns, analyze authorization flows, evaluate data protection\n\n**Advanced Analysis**: Component testability, configuration management, error handling patterns, monitoring integration, extensibility assessment.\n\n**Quality Assessment**: Code organization, documentation adequacy, team communication patterns, technical debt evaluation.\n\n**Output**: Detailed architecture assessment with specific improvement recommendations, refactoring strategies, and implementation roadmap.",
      "tags": ["architecture", "audit"]
    },
    {
      "command": "/audit",
      "label": "`/audit`",
      "category": "Qualidade e Testes",
      "exemplos": [
        "`/audit`",
        "`/audit --level high`",
        "`/audit --fix`",
        "`/audit all`",
        "`/audit --json`"
      ],
      "capacidades": "Executa `npm audit` com filtros, fix e relatorios.",
      "momentoIdeal": "Periodicamente ou apos atualizar dependencias para detectar vulnerabilidades antes de ir a producao.",
      "exemploMomento": "Depois de aceitar uma PR que adiciona pacotes novos ao dashboard, verificando vulnerabilidades.",
      "tipoSaida": "Log de auditoria apontando vulnerabilidades por pacote, severidade e sugestao de correcao.",
      "fileName": "audit.md",
      "filePath": ".claude/commands/audit.md",
      "fileContent": "# Security Audit Command\n\nExecute npm audit para verificar vulnerabilidades em dependências.\n\n## Usage\n\n```bash\n/audit [target] [options]\n```\n\n## Targets\n\n- `frontend` - Audit frontend/dashboard (default)\n- `backend` - Audit all backend APIs\n- `all` - Audit frontend + backend\n\n## Options\n\n- `--fix` - Auto-fix vulnerabilities (pode quebrar dependências!)\n- `--level <severity>` - Filter by severity: low, moderate, high, critical\n- `--production` - Only production dependencies\n- `--json` - Output JSON format\n\n## Examples\n\n```bash\n# Audit frontend\n/audit\n\n# Only high/critical vulnerabilities\n/audit --level high\n\n# Auto-fix (CUIDADO!)\n/audit --fix\n\n# Audit all projects\n/audit all\n\n# JSON output\n/audit --json\n\n# Production dependencies only\n/audit --production\n```\n\n## Implementation\n\n```bash\n# Frontend\nif [[ \"{{target}}\" == \"frontend\" ]] || [[ \"{{target}}\" == \"\" ]]; then\n  cd frontend/dashboard\n\n  cmd=\"npm audit\"\n\n  if [[ \"{{args}}\" == *\"--fix\"* ]]; then\n    cmd=\"npm audit fix\"\n  elif [[ \"{{args}}\" == *\"--level\"* ]]; then\n    level=$(echo \"{{args}}\" | grep -oP '(?<=--level )\\w+')\n    cmd=\"npm audit --audit-level=$level\"\n  fi\n\n  if [[ \"{{args}}\" == *\"--production\"* ]]; then\n    cmd=\"$cmd --production\"\n  fi\n\n  if [[ \"{{args}}\" == *\"--json\"* ]]; then\n    cmd=\"$cmd --json\"\n  fi\n\n  eval \"$cmd\"\n  cd ../..\nfi\n\n# Backend\nif [[ \"{{target}}\" == \"backend\" ]] || [[ \"{{target}}\" == \"all\" ]]; then\n  for api in backend/api/*/; do\n    cd \"$api\"\n    if [[ -f \"package.json\" ]]; then\n      echo \"Auditing $api...\"\n      npm audit --audit-level=high || true\n    fi\n    cd ../../..\n  done\nfi\n```\n\n## Severity Levels\n\n| Level | Action | Timeline |\n|-------|--------|----------|\n| **Critical** | ❌ Fix immediately | < 24h |\n| **High** | ⚠️  Fix urgently | < 7 days |\n| **Moderate** | ⚠️  Review and plan | < 30 days |\n| **Low** | ℹ️  Monitor | Backlog |\n\n## Understanding Output\n\n```bash\n# Vulnerabilities found: 5 (2 low, 1 moderate, 2 high)\n```\n\n- **Severity**: critical > high > moderate > low\n- **Path**: Which dependency chain caused it\n- **Recommendation**: Action to fix\n\n## Auto-Fix Warnings\n\n⚠️ **CUIDADO com `npm audit fix`**:\n\n- Pode atualizar dependências para versões breaking\n- Pode quebrar o build\n- Sempre teste após auto-fix\n- Considere `npm audit fix --dry-run` primeiro\n\n### Safe Fix Process\n\n```bash\n# 1. Dry run (ver o que seria modificado)\nnpm audit fix --dry-run\n\n# 2. Backup package-lock.json\ncp package-lock.json package-lock.json.backup\n\n# 3. Fix\nnpm audit fix\n\n# 4. Test\nnpm test && npm run build\n\n# 5. Se quebrou, restore\n# cp package-lock.json.backup package-lock.json\n# npm install\n```\n\n## Alternative: Snyk\n\nPara análise mais robusta:\n\n```bash\n# Install\nnpm install -g snyk\n\n# Authenticate\nsnyk auth\n\n# Test\nsnyk test\n\n# Monitor (continuous)\nsnyk monitor\n\n# Fix interactively\nsnyk wizard\n```\n\n## Ignoring Vulnerabilities\n\nSe não puder corrigir imediatamente (ex: dependência do framework):\n\n```bash\n# Criar .npmrc\necho \"audit-level=high\" >> .npmrc\n\n# Ou usar --audit-level no CI\nnpm install --audit-level=high\n```\n\n## CI/CD Integration\n\n```yaml\n# .github/workflows/security.yml\n- name: Security Audit\n  run: |\n    cd frontend/dashboard\n    npm audit --audit-level=high\n    # Fail build se encontrar high/critical\n```\n\n## Related Commands\n\n- `/quality-check` - Full quality check (includes audit)\n- `/dependencies` - List and update dependencies\n- `/snyk` - Run Snyk security scan\n",
      "tags": ["security", "dependencies"]
    },
    {
      "command": "/build",
      "label": "`/build`",
      "category": "Entrega e DevOps",
      "exemplos": [
        "`/build`",
        "`/build --clean`",
        "`/build --analyze`",
        "`/build all`",
        "`/build --watch`"
      ],
      "capacidades": "Executa builds de producao para frontend e APIs, com opcoes de clean, analyze, watch.",
      "momentoIdeal": "Antes de subir imagens Docker garantindo que o bundle Vite ou build Node esta consistente.",
      "exemploMomento": "Validar build do dashboard antes de gerar artefatos para deploy nas instancias internas.",
      "tipoSaida": "Logs de build do Vite/Node e, quando solicitado, relatorio de analise de bundle.",
      "fileName": "build.md",
      "filePath": ".claude/commands/build.md",
      "fileContent": "# Build Command\n\nExecute build de produção para verificar se o código compila corretamente.\n\n## Usage\n\n```bash\n/build [target] [options]\n```\n\n## Targets\n\n- `frontend` - Build frontend/dashboard (default)\n- `backend` - Build backend services (if applicable)\n- `all` - Build all projects\n\n## Options\n\n- `--analyze` - Analyze bundle size\n- `--clean` - Clean before build\n- `--watch` - Watch mode (rebuild on changes)\n\n## Examples\n\n```bash\n# Build frontend\n/build\n\n# Clean build\n/build --clean\n\n# Build with bundle analysis\n/build --analyze\n\n# Build all\n/build all\n\n# Watch mode (dev)\n/build --watch\n```\n\n## Implementation\n\n```bash\n# Frontend\nif [[ \"{{target}}\" == \"frontend\" ]] || [[ \"{{target}}\" == \"\" ]]; then\n  cd frontend/dashboard\n\n  # Clean if requested\n  if [[ \"{{args}}\" == *\"--clean\"* ]]; then\n    echo \"Cleaning dist/...\"\n    rm -rf dist/\n  fi\n\n  # Build\n  if [[ \"{{args}}\" == *\"--watch\"* ]]; then\n    npm run dev\n  else\n    npm run build\n  fi\n\n  # Analyze bundle\n  if [[ \"{{args}}\" == *\"--analyze\"* ]]; then\n    echo \"\"\n    echo \"Bundle Analysis:\"\n    echo \"================\"\n    ls -lh dist/assets/*.js | awk '{print $9 \" - \" $5}'\n    echo \"\"\n    echo \"Total size:\"\n    du -sh dist/\n    echo \"\"\n    echo \"For interactive analysis, run:\"\n    echo \"  npx vite-bundle-visualizer\"\n  fi\n\n  cd ../..\nfi\n\n# Backend\nif [[ \"{{target}}\" == \"backend\" ]] || [[ \"{{target}}\" == \"all\" ]]; then\n  for api in backend/api/*/; do\n    cd \"$api\"\n    if [[ -f \"package.json\" ]] && grep -q '\"build\"' package.json; then\n      echo \"Building $api...\"\n      npm run build\n    fi\n    cd ../../..\n  done\nfi\n```\n\n## Build Output (Frontend)\n\n```\nvite v5.0.0 building for production...\n✓ 142 modules transformed.\ndist/index.html                   0.45 kB │ gzip:  0.30 kB\ndist/assets/index-B2jU8v9z.css   12.34 kB │ gzip:  3.21 kB\ndist/assets/index-C4sD9f8g.js   234.56 kB │ gzip: 76.54 kB\n✓ built in 3.21s\n```\n\n## Bundle Size Targets\n\n| Asset Type | Target | Warning | Critical |\n|------------|--------|---------|----------|\n| Initial JS | < 200 KB | 300 KB | 500 KB |\n| CSS | < 50 KB | 100 KB | 150 KB |\n| Total (gzipped) | < 300 KB | 500 KB | 800 KB |\n| Lazy chunks | < 100 KB | 150 KB | 200 KB |\n\n## Bundle Analysis\n\n### Interactive Visualization\n\n```bash\nnpm run build\nnpx vite-bundle-visualizer\n```\n\nOpens browser with treemap showing:\n- Which packages are largest\n- Code splitting effectiveness\n- Duplicate code across chunks\n\n### Command Line Analysis\n\n```bash\n# List chunks by size\nls -lhS dist/assets/*.js | head -10\n\n# Find large dependencies\nnpx vite-bundle-visualizer --mode json | jq '.modules | sort_by(.size) | reverse | .[0:10]'\n\n# Check gzipped sizes\ngzip -c dist/assets/index-*.js | wc -c\n```\n\n## Optimization Tips\n\n### Code Splitting\n\n```typescript\n// ❌ Bad: Import everything upfront\nimport { HeavyComponent } from './HeavyComponent';\n\n// ✅ Good: Lazy load\nconst HeavyComponent = lazy(() => import('./HeavyComponent'));\n```\n\n### Tree Shaking\n\n```typescript\n// ❌ Bad: Import entire library\nimport _ from 'lodash';\n\n// ✅ Good: Import specific functions\nimport { debounce } from 'lodash-es';\n// Or even better:\nimport debounce from 'lodash-es/debounce';\n```\n\n### Dynamic Imports\n\n```typescript\n// Load heavy library only when needed\nasync function processImage(file: File) {\n  const { default: imageCompression } = await import('browser-image-compression');\n  return imageCompression(file, { maxSizeMB: 1 });\n}\n```\n\n## Build Errors\n\n### Out of Memory\n\n```bash\n# Increase Node memory\nNODE_OPTIONS=\"--max-old-space-size=4096\" npm run build\n```\n\n### Module Not Found\n\n```bash\n# Clear cache and reinstall\nrm -rf node_modules package-lock.json\nnpm install\nnpm run build\n```\n\n### TypeScript Errors\n\n```bash\n# Type check first\nnpx tsc --noEmit\n\n# Then build\nnpm run build\n```\n\n## CI/CD Build\n\n```yaml\n# .github/workflows/build.yml\n- name: Build\n  run: |\n    cd frontend/dashboard\n    npm run build\n\n- name: Check Bundle Size\n  run: |\n    SIZE=$(du -sb frontend/dashboard/dist | cut -f1)\n    if [ $SIZE -gt 838860800 ]; then  # 800MB\n      echo \"Bundle too large: ${SIZE} bytes\"\n      exit 1\n    fi\n```\n\n## Production Build\n\nFor deployment:\n\n```bash\n# 1. Clean\n/build --clean\n\n# 2. Build with analysis\n/build --analyze\n\n# 3. Verify output\nls -la dist/\n\n# 4. Test production build locally\nnpx vite preview\n```\n\n## Related Commands\n\n- `/quality-check` - Full quality check (includes build)\n- `/type-check` - TypeScript verification before build\n- `/bundle-analyze` - Detailed bundle analysis\n",
      "tags": ["ci", "build"]
    },
    {
      "command": "/ci-pipeline",
      "label": "`/ci-pipeline`",
      "category": "Entrega e DevOps",
      "exemplos": [
        "`/ci-pipeline setup`",
        "`/ci-pipeline status`",
        "`/ci-pipeline fix`",
        "`/ci-pipeline <pipeline-name>`"
      ],
      "capacidades": "Administra pipelines CI/CD existentes, gerando workflows padronizados, checando status e propondo correcoes para ambientes multi-stage.",
      "momentoIdeal": "Quando for necessario garantir estabilidade do GitHub Actions antes de releases ou apos incidentes recorrentes no deploy continuo.",
      "exemploMomento": "Revisar a pipeline do `tp-capital` depois de expandir testes end-to-end, ajustando matrizes de Node e etapas de seguranca.",
      "tipoSaida": "Playbook com YAML sugerido, comandos de diagnostico (`gh run`, `npm run build`) e checklist de etapas do pipeline.",
      "fileName": "ci-pipeline.md",
      "filePath": ".claude/commands/ci-pipeline.md",
      "fileContent": "# CI/CD Pipeline Manager\n\nManage CI/CD pipeline automation: $ARGUMENTS\n\n## Current Pipeline State\n\n- GitHub Actions: !`find .github/workflows -name \"*.yml\" -o -name \"*.yaml\" 2>/dev/null | head -5`\n- CI configuration: @.github/workflows/ (if exists)\n- Package scripts: @package.json\n- Environment files: !`find . -name \".env*\" | head -3`\n- Recent workflow runs: !`gh run list --limit 5 2>/dev/null || echo \"GitHub CLI not available\"`\n\n## Task\n\nAutomate CI/CD pipeline management with comprehensive workflow orchestration.\n\n## Pipeline Operations\n\n### Setup New Pipeline\nCreate complete CI/CD pipeline with:\n\n```yaml\n# .github/workflows/ci.yml\nname: CI Pipeline\non:\n  push:\n    branches: [main, develop]\n  pull_request:\n    branches: [main]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    strategy:\n      matrix:\n        node-version: [18, 20]\n    steps:\n      - uses: actions/checkout@v4\n      - name: Setup Node.js\n        uses: actions/setup-node@v4\n        with:\n          node-version: ${{ matrix.node-version }}\n          cache: 'npm'\n      \n      - name: Install dependencies\n        run: npm ci\n      \n      - name: Run linter\n        run: npm run lint\n      \n      - name: Run tests\n        run: npm run test:coverage\n      \n      - name: Build application\n        run: npm run build\n      \n      - name: Upload coverage\n        uses: codecov/codecov-action@v3\n        with:\n          file: ./coverage/lcov.info\n```\n\n### Multi-Environment Deployment\n```yaml\n# .github/workflows/deploy.yml\nname: Deploy\non:\n  push:\n    branches: [main]\n  release:\n    types: [published]\n\njobs:\n  deploy-staging:\n    if: github.ref == 'refs/heads/main'\n    runs-on: ubuntu-latest\n    environment: staging\n    steps:\n      - uses: actions/checkout@v4\n      - name: Deploy to Staging\n        run: |\n          npm run build:staging\n          npm run deploy:staging\n        env:\n          STAGING_API_URL: ${{ secrets.STAGING_API_URL }}\n          STAGING_SECRET: ${{ secrets.STAGING_SECRET }}\n\n  deploy-production:\n    if: github.event_name == 'release'\n    runs-on: ubuntu-latest\n    environment: production\n    needs: [test]\n    steps:\n      - uses: actions/checkout@v4\n      - name: Deploy to Production\n        run: |\n          npm run build:production\n          npm run deploy:production\n        env:\n          PROD_API_URL: ${{ secrets.PROD_API_URL }}\n          PROD_SECRET: ${{ secrets.PROD_SECRET }}\n```\n\n### Security & Quality Gates\n```yaml\n  security-scan:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      \n      - name: Run security audit\n        run: npm audit --audit-level=moderate\n      \n      - name: Scan for secrets\n        uses: trufflesecurity/trufflehog@main\n        with:\n          path: ./\n          base: main\n          head: HEAD\n      \n      - name: SAST Scan\n        uses: github/super-linter@v4\n        env:\n          DEFAULT_BRANCH: main\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n```\n\n### Performance Testing\n```yaml\n  performance:\n    runs-on: ubuntu-latest\n    if: github.event_name == 'pull_request'\n    steps:\n      - uses: actions/checkout@v4\n      - name: Performance Test\n        run: |\n          npm run build\n          npm run start:test &\n          sleep 10\n          npx lighthouse http://localhost:3000 --output=json --output-path=./lighthouse.json\n      \n      - name: Comment PR\n        uses: actions/github-script@v6\n        with:\n          script: |\n            const fs = require('fs');\n            const lighthouse = JSON.parse(fs.readFileSync('./lighthouse.json'));\n            const score = lighthouse.lhr.categories.performance.score * 100;\n            \n            github.rest.issues.createComment({\n              issue_number: context.issue.number,\n              owner: context.repo.owner,\n              repo: context.repo.repo,\n              body: `⚡ Performance Score: ${score}/100`\n            });\n```\n\n## Advanced Features\n\n### 1. **Matrix Strategy Testing**\n```yaml\nstrategy:\n  matrix:\n    os: [ubuntu-latest, windows-latest, macos-latest]\n    node-version: [16, 18, 20]\n    include:\n      - os: ubuntu-latest\n        node-version: 20\n        coverage: true\n```\n\n### 2. **Conditional Workflows**\n```yaml\n- name: Skip CI\n  if: contains(github.event.head_commit.message, '[skip ci]')\n  run: echo \"Skipping CI as requested\"\n\n- name: Deploy only on version tags\n  if: startsWith(github.ref, 'refs/tags/v')\n  run: npm run deploy\n```\n\n### 3. **Workflow Dependencies**\n```yaml\njobs:\n  test:\n    runs-on: ubuntu-latest\n    \n  build:\n    needs: test\n    runs-on: ubuntu-latest\n    \n  deploy:\n    needs: [test, build]\n    if: github.ref == 'refs/heads/main'\n    runs-on: ubuntu-latest\n```\n\n### 4. **Cache Optimization**\n```yaml\n- name: Cache node modules\n  uses: actions/cache@v3\n  with:\n    path: ~/.npm\n    key: ${{ runner.os }}-node-${{ hashFiles('**/package-lock.json') }}\n    restore-keys: |\n      ${{ runner.os }}-node-\n\n- name: Cache build output\n  uses: actions/cache@v3\n  with:\n    path: dist\n    key: build-${{ github.sha }}\n```\n\n### 5. **Artifact Management**\n```yaml\n- name: Upload build artifacts\n  uses: actions/upload-artifact@v3\n  with:\n    name: dist-files\n    path: dist/\n    retention-days: 7\n\n- name: Download artifacts\n  uses: actions/download-artifact@v3\n  with:\n    name: dist-files\n    path: dist/\n```\n\n### 6. **Environment Management**\n```yaml\nenvironments:\n  staging:\n    url: https://staging.example.com\n    \n  production:\n    url: https://example.com\n    protection_rules:\n      - type: required_reviewers\n        required_reviewers:\n          - devops-team\n      - type: wait_timer\n        wait_timer: 5\n```\n\n## Pipeline Monitoring\n\n### Status Checks\n```bash\n# Check workflow status\ngh run list --workflow=ci.yml --limit=10\n\n# View specific run\ngh run view [run-id] --log\n\n# Monitor failure rate\ngh api repos/:owner/:repo/actions/runs \\\n  --jq '.workflow_runs[0:20] | map(select(.conclusion==\"failure\")) | length'\n```\n\n### Performance Metrics\n```bash\n# Average build time\ngh api repos/:owner/:repo/actions/runs \\\n  --jq '.workflow_runs[0:50] | map(.run_duration_ms) | add / length'\n\n# Success rate calculation\ngh api repos/:owner/:repo/actions/runs \\\n  --jq '.workflow_runs[0:100] | [group_by(.conclusion)[] | {conclusion: .[0].conclusion, count: length}]'\n```\n\n## Troubleshooting\n\n### Common Issues\n\n#### 1. **Workflow Permission Issues**\n```yaml\npermissions:\n  contents: read\n  actions: write\n  security-events: write\n  pull-requests: write\n```\n\n#### 2. **Secret Management**\n```bash\n# Add repository secret\ngh secret set STAGING_API_URL --body \"https://staging-api.example.com\"\n\n# List secrets\ngh secret list\n```\n\n#### 3. **Timeout Configuration**\n```yaml\njobs:\n  long-running-job:\n    runs-on: ubuntu-latest\n    timeout-minutes: 60\n    steps:\n      - name: Long task\n        timeout-minutes: 30\n        run: npm run long-task\n```\n\n#### 4. **Debugging Workflows**\n```yaml\n- name: Debug information\n  run: |\n    echo \"Event name: ${{ github.event_name }}\"\n    echo \"Ref: ${{ github.ref }}\"\n    echo \"SHA: ${{ github.sha }}\"\n    echo \"Actor: ${{ github.actor }}\"\n```\n\n## Best Practices\n\n### 1. **Fail Fast Strategy**\n- Run fastest jobs first\n- Use `fail-fast: true` in matrix\n- Implement early validation steps\n\n### 2. **Security First**\n- Never store secrets in code\n- Use least privilege permissions\n- Scan for vulnerabilities early\n\n### 3. **Efficiency Optimization**\n- Use appropriate cache strategies\n- Minimize workflow duration\n- Parallel job execution\n\n### 4. **Monitoring & Alerting**\n- Track build success rates\n- Monitor deployment frequency\n- Alert on critical failures\n\n## Integration Examples\n\n### Docker Integration\n```yaml\n- name: Build Docker image\n  run: |\n    docker build -t myapp:${{ github.sha }} .\n    docker tag myapp:${{ github.sha }} myapp:latest\n\n- name: Push to registry\n  run: |\n    echo ${{ secrets.DOCKER_PASSWORD }} | docker login -u ${{ secrets.DOCKER_USERNAME }} --password-stdin\n    docker push myapp:${{ github.sha }}\n    docker push myapp:latest\n```\n\n### Cloud Deployment\n```yaml\n- name: Deploy to AWS\n  uses: aws-actions/configure-aws-credentials@v2\n  with:\n    aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}\n    aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\n    aws-region: us-east-1\n\n- name: Deploy to S3\n  run: |\n    aws s3 sync dist/ s3://my-bucket --delete\n    aws cloudfront create-invalidation --distribution-id ${{ secrets.CLOUDFRONT_ID }} --paths \"/*\"\n```\n\nThis pipeline manager provides comprehensive automation for modern CI/CD workflows with security, performance, and monitoring built-in.",
      "tags": ["ci", "pipeline"]
    },
    {
      "command": "/code-review",
      "label": "`/code-review`",
      "category": "Qualidade e Testes",
      "exemplos": [
        "`/code-review frontend/dashboard --full`",
        "`/code-review backend/api/documentation-api`",
        "`/code-review --security`"
      ],
      "capacidades": "Checklist de revisao cobrindo arquitetura, seguranca, performance e cobertura.",
      "momentoIdeal": "Durante revisao de PRs criticos (ex.: atualizacao de rag proxy) para nao esquecer riscos.",
      "exemploMomento": "Revisar PR que altera `RagProxyService.js`, garantindo que fluxos de autorizacao estejam corretos.",
      "tipoSaida": "Lista de observacoes estruturadas com severidade, arquivos e linhas recomendadas para ajuste.",
      "fileName": "code-review.md",
      "filePath": ".claude/commands/code-review.md",
      "fileContent": "# Code Quality Review\n\nPerform comprehensive code quality review: $ARGUMENTS\n\n## Current State\n\n- Git status: !`git status --porcelain`\n- Recent changes: !`git diff --stat HEAD~5`\n- Repository info: !`git log --oneline -5`\n- Build status: !`npm run build --dry-run 2>/dev/null || echo \"No build script\"`\n\n## Task\n\nFollow these steps to conduct a thorough code review:\n\n1. **Repository Analysis**\n   - Examine the repository structure and identify the primary language/framework\n   - Check for configuration files (package.json, requirements.txt, Cargo.toml, etc.)\n   - Review README and documentation for context\n\n2. **Code Quality Assessment**\n   - Scan for code smells, anti-patterns, and potential bugs\n   - Check for consistent coding style and naming conventions\n   - Identify unused imports, variables, or dead code\n   - Review error handling and logging practices\n\n3. **Security Review**\n   - Look for common security vulnerabilities (SQL injection, XSS, etc.)\n   - Check for hardcoded secrets, API keys, or passwords\n   - Review authentication and authorization logic\n   - Examine input validation and sanitization\n\n4. **Performance Analysis**\n   - Identify potential performance bottlenecks\n   - Check for inefficient algorithms or database queries\n   - Review memory usage patterns and potential leaks\n   - Analyze bundle size and optimization opportunities\n\n5. **Architecture & Design**\n   - Evaluate code organization and separation of concerns\n   - Check for proper abstraction and modularity\n   - Review dependency management and coupling\n   - Assess scalability and maintainability\n\n6. **Testing Coverage**\n   - Check existing test coverage and quality\n   - Identify areas lacking proper testing\n   - Review test structure and organization\n   - Suggest additional test scenarios\n\n7. **Documentation Review**\n   - Evaluate code comments and inline documentation\n   - Check API documentation completeness\n   - Review README and setup instructions\n   - Identify areas needing better documentation\n\n8. **Recommendations**\n   - Prioritize issues by severity (critical, high, medium, low)\n   - Provide specific, actionable recommendations\n   - Suggest tools and practices for improvement\n   - Create a summary report with next steps\n\nRemember to be constructive and provide specific examples with file paths and line numbers where applicable.",
      "tags": ["quality", "review"]
    },
    {
      "command": "/commit",
      "label": "`/commit`",
      "category": "Entrega e DevOps",
      "exemplos": [
        "`/commit \"feat: adicionar healthcheck tp capital\"`",
        "`/commit \"fix: corrigir parse de sinais\" --no-verify`",
        "`/commit \"chore: atualizar deps\" --amend`"
      ],
      "capacidades": "Automatiza commits convencionais com verificacoes de lint/build/docs e mensagens com emoji.",
      "momentoIdeal": "No fluxo diario, evitando commits fora do padrao `feat/fix/chore` com validacao automatica.",
      "exemploMomento": "Finalizar ajustes em `useRagQuery.ts` e criar commit bem formatado sem esquecer comandos de validacao.",
      "tipoSaida": "Commit registrado no git com mensagem formatada e feedback textual das verificacoes executadas.",
      "fileName": "commit.md",
      "filePath": ".claude/commands/commit.md",
      "fileContent": "# Orchestration Commit Command\n\nCreate git commits aligned with task completion, maintaining clean version control synchronized with task progress.\n\n## Usage\n\n```\n/orchestration/commit [TASK-ID] [options]\n```\n\n## Description\n\nAutomatically creates well-structured commits when tasks move to QA or completion, using task metadata to generate meaningful commit messages following Conventional Commits specification.\n\n## Basic Commands\n\n### Commit Current Task\n```\n/orchestration/commit\n```\nCommits changes for the task currently in progress.\n\n### Commit Specific Task\n```\n/orchestration/commit TASK-003\n```\nCommits changes related to a specific task.\n\n### Batch Commit\n```\n/orchestration/commit --batch\n```\nGroups related completed tasks into logical commits.\n\n## Commit Message Generation\n\n### Automatic Format\nBased on task type and content:\n```\nfeat(auth): implement JWT token validation\n\n- Add token verification middleware\n- Implement refresh token logic\n- Add expiration handling\n\nTask: TASK-003\nStatus: todos -> in_progress -> qa\nTime: 4.5 hours\n```\n\n### Type Mapping\n```\nTask Type     -> Commit Type\n━━━━━━━━━━━━━━━━━━━━━━━━━━━\nfeature       -> feat:\nbugfix        -> fix:\nrefactor      -> refactor:\ntest          -> test:\ndocs          -> docs:\nperformance   -> perf:\nsecurity      -> fix:        (with security note)\n```\n\n## Workflow Integration\n\n### Auto-commit on Status Change\n```\n/orchestration/move TASK-003 qa --auto-commit\n```\nAutomatically commits when moving to QA status.\n\n### Pre-commit Validation\n```\n/orchestration/commit --validate\n```\nChecks:\n- All tests pass\n- No linting errors\n- Task requirements met\n- Files match task scope\n\n## Options\n\n### Custom Message\n```\n/orchestration/commit TASK-003 --message \"Custom commit message\"\n```\nOverride automatic message generation.\n\n### Scope Detection\n```\n/orchestration/commit --detect-scope\n```\nAutomatically detects scope from changed files:\n- `auth` for auth-related files\n- `api` for API changes\n- `ui` for frontend changes\n\n### Breaking Changes\n```\n/orchestration/commit --breaking\n```\nAdds breaking change indicator:\n```\nfeat(api)!: restructure authentication endpoints\n\nBREAKING CHANGE: Auth endpoints moved from /auth to /api/v2/auth\n```\n\n## Batch Operations\n\n### Commit by Feature\n```\n/orchestration/commit --feature authentication\n```\nGroups all completed auth tasks into one commit.\n\n### Commit by Status\n```\n/orchestration/commit --status qa\n```\nCommits all tasks currently in QA.\n\n### Smart Grouping\n```\n/orchestration/commit --smart-group\n```\nIntelligently groups related tasks:\n```\nFeature Group: Authentication (3 tasks)\n- TASK-001: Database schema\n- TASK-003: JWT implementation  \n- TASK-005: Login endpoint\n\nSuggested commit: feat(auth): implement complete authentication system\n```\n\n## Worktree Support\n\n### Worktree-Aware Commits\n```\n/orchestration/commit --worktree\n```\nDetects current worktree and commits only relevant tasks.\n\n### Cross-Worktree Status\n```\n/orchestration/commit --all-worktrees\n```\nShows commit status across all worktrees:\n```\nWorktree Status:\n- feature/auth: 2 tasks ready to commit\n- feature/payments: 1 task ready to commit\n- feature/ui: No uncommitted changes\n```\n\n## Validation Features\n\n### Pre-commit Checks\n```\n## Pre-commit Validation\n✓ All tests passing\n✓ No linting errors\n✓ Task requirements met\n✗ Uncommitted files outside task scope: src/unrelated.js\n\nProceed with commit? [y/n]\n```\n\n### Task Alignment\n```\n## Task Alignment Check\nChanged files:\n- src/auth/jwt.ts ✓ (matches TASK-003)\n- src/auth/validate.ts ✓ (matches TASK-003)\n- src/payments/stripe.ts ✗ (not in TASK-003 scope)\n\nWarning: Changes outside task scope detected\n```\n\n## Integration Features\n\n### Link to Task\n```\n/orchestration/commit --link-task\n```\nAdds task URL/reference to commit:\n```\nfeat(auth): implement JWT validation\n\nTask: TASK-003\nLink: http://orchestration/03_15_2024/auth_system/tasks/TASK-003\n```\n\n### Update Status Tracker\n```\n/orchestration/commit --update-tracker\n```\nUpdates TASK-STATUS-TRACKER.yaml with commit info:\n```yaml\ngit_tracking:\n  TASK-003:\n    commits: [\"abc123def\"]\n    commit_message: \"feat(auth): implement JWT validation\"\n    committed_at: \"2024-03-15T14:30:00Z\"\n```\n\n## Examples\n\n### Example 1: Simple Task Commit\n```\n/orchestration/commit TASK-003\n\nGenerated commit:\nfeat(auth): implement JWT token validation\n\n- Add verification middleware\n- Handle token expiration\n- Implement refresh logic\n\nTask: TASK-003 (4.5 hours)\n```\n\n### Example 2: Batch Feature Commit\n```\n/orchestration/commit --feature authentication --batch\n\nGrouping 3 related tasks:\nfeat(auth): complete authentication system implementation\n\n- Set up database schema (TASK-001)\n- Implement JWT validation (TASK-003)\n- Create login endpoints (TASK-005)\n\nTasks: TASK-001, TASK-003, TASK-005 (12 hours total)\n```\n\n### Example 3: Fix with Test\n```\n/orchestration/commit TASK-007\n\nGenerated commit:\nfix(auth): resolve token expiration race condition\n\n- Fix async validation timing issue\n- Add comprehensive test coverage\n- Prevent edge case in refresh flow\n\nFixes: #123\nTask: TASK-007 (2 hours)\n```\n\n## Commit Templates\n\n### Feature Template\n```\nfeat(<scope>): <task-title>\n\n- <implementation-detail-1>\n- <implementation-detail-2>\n- <implementation-detail-3>\n\nTask: <task-id> (<duration>)\nStatus: <status-transition>\n```\n\n### Fix Template\n```\nfix(<scope>): <issue-description>\n\n- <root-cause>\n- <solution>\n- <test-coverage>\n\nFixes: #<issue-number>\nTask: <task-id>\n```\n\n## Best Practices\n\n1. **Commit at Natural Breakpoints**: When moving tasks to QA\n2. **Keep Commits Atomic**: One logical change per commit\n3. **Use Batch Wisely**: Only group truly related tasks\n4. **Validate First**: Always run validation before committing\n5. **Update Status**: Ensure task status is current\n\n## Configuration\n\n### Auto-commit Rules\nSet in orchestration config:\n```yaml\nauto_commit:\n  on_qa: true\n  on_complete: false\n  require_tests: true\n  require_validation: true\n```\n\n## Notes\n\n- Integrates with task-commit-manager agent for complex scenarios\n- Respects .gitignore and excluded files\n- Supports conventional commits specification\n- Maintains traceable history between tasks and commits",
      "tags": ["git", "workflow"]
    },
    {
      "command": "/containerize-application",
      "label": "`/containerize-application`",
      "category": "Infraestrutura e Deploy",
      "exemplos": [
        "`/containerize-application --node`",
        "`/containerize-application --python`",
        "`/containerize-application --go`",
        "`/containerize-application --multi-stage`"
      ],
      "capacidades": "Cria Dockerfiles otimizados, multi-stage e seguros com boas praticas.",
      "momentoIdeal": "Ao preparar um novo servico (ex.: rag-services) para entrar na stack docker-compose.",
      "exemploMomento": "Containerizar script de analise para rodar junto aos demais servicos via compose.",
      "tipoSaida": "Dockerfile(s) gerados ou revisados, acompanhados de notas de configuracao e testes de build.",
      "fileName": "containerize-application.md",
      "filePath": ".claude/commands/containerize-application.md",
      "fileContent": "# Application Containerization\n\nContainerize application for deployment: $ARGUMENTS\n\n## Current Application Analysis\n\n- Application type: @package.json or @setup.py or @go.mod or @pom.xml (detect runtime)\n- Existing Docker: @Dockerfile or @docker-compose.yml (if exists)\n- Dependencies: !`find . -name \"*requirements*.txt\" -o -name \"package*.json\" -o -name \"go.mod\" | head -3`\n- Port configuration: !`grep -r \"PORT\\|listen\\|bind\" src/ 2>/dev/null | head -3 || echo \"Port detection needed\"`\n- Build tools: @Makefile or build scripts detection\n\n## Task\n\nImplement production-ready containerization strategy:\n\n1. **Application Analysis and Containerization Strategy**\n   - Analyze application architecture and runtime requirements\n   - Identify application dependencies and external services\n   - Determine optimal base image and runtime environment\n   - Plan multi-stage build strategy for optimization\n   - Assess security requirements and compliance needs\n\n2. **Dockerfile Creation and Optimization**\n   - Create comprehensive Dockerfile with multi-stage builds\n   - Select minimal base images (Alpine, distroless, or slim variants)\n   - Configure proper layer caching and build optimization\n   - Implement security best practices (non-root user, minimal attack surface)\n   - Set up proper file permissions and ownership\n\n3. **Build Process Configuration**\n   - Configure .dockerignore file to exclude unnecessary files\n   - Set up build arguments and environment variables\n   - Implement build-time dependency installation and cleanup\n   - Configure application bundling and asset optimization\n   - Set up proper build context and file structure\n\n4. **Runtime Configuration**\n   - Configure application startup and health checks\n   - Set up proper signal handling and graceful shutdown\n   - Configure logging and output redirection\n   - Set up environment-specific configuration management\n   - Configure resource limits and performance tuning\n\n5. **Security Hardening**\n   - Run application as non-root user with minimal privileges\n   - Configure security scanning and vulnerability assessment\n   - Implement secrets management and secure credential handling\n   - Set up network security and firewall rules\n   - Configure security policies and access controls\n\n6. **Docker Compose Configuration**\n   - Create docker-compose.yml for local development\n   - Configure service dependencies and networking\n   - Set up volume mounting and data persistence\n   - Configure environment variables and secrets\n   - Set up development vs production configurations\n\n7. **Container Orchestration Preparation**\n   - Prepare configurations for Kubernetes deployment\n   - Create deployment manifests and service definitions\n   - Configure ingress and load balancing\n   - Set up persistent volumes and storage classes\n   - Configure auto-scaling and resource management\n\n8. **Monitoring and Observability**\n   - Configure application metrics and health endpoints\n   - Set up logging aggregation and centralized logging\n   - Configure distributed tracing and monitoring\n   - Set up alerting and notification systems\n   - Configure performance monitoring and profiling\n\n9. **CI/CD Integration**\n   - Configure automated Docker image building\n   - Set up image scanning and security validation\n   - Configure image registry and artifact management\n   - Set up automated deployment pipelines\n   - Configure rollback and blue-green deployment strategies\n\n10. **Testing and Validation**\n    - Test container builds and functionality\n    - Validate security configurations and compliance\n    - Test deployment in different environments\n    - Validate performance and resource utilization\n    - Test backup and disaster recovery procedures\n    - Create documentation for container deployment and management",
      "tags": ["devops", "docker"]
    },
    {
      "command": "/create-architecture-documentation",
      "label": "`/create-architecture-documentation`",
      "category": "Arquitetura e Estrategia",
      "exemplos": [
        "`/create-architecture-documentation --c4-model`",
        "`/create-architecture-documentation --arc42`",
        "`/create-architecture-documentation --adr`",
        "`/create-architecture-documentation --plantuml`",
        "`/create-architecture-documentation --full-suite`"
      ],
      "capacidades": "Gera docs arquiteturais C4, ADRs e diagramas PlantUML/Structurizr.",
      "momentoIdeal": "Quando novas decisoes impactam a topologia (ex.: adicionar novo servico websocket) e precisam ser registradas.",
      "exemploMomento": "Documentar o fluxo de webhooks do TP Capital apos criar diagramas em `outputs/workflow-tp-capital`.",
      "tipoSaida": "Pacote de artefatos (diagramas, ADRs, resumos) prontos para incorporar ao repositorio de arquitetura.",
      "fileName": "create-architecture-documentation.md",
      "filePath": ".claude/commands/create-architecture-documentation.md",
      "fileContent": "# Architecture Documentation Generator\n\nGenerate comprehensive architecture documentation: $ARGUMENTS\n\n## Current Architecture Context\n\n- Project structure: !`find . -type f -name \"*.json\" -o -name \"*.yaml\" -o -name \"*.toml\" | head -5`\n- Documentation exists: @docs/ or @README.md (if exists)\n- Architecture files: !`find . -name \"*architecture*\" -o -name \"*design*\" -o -name \"*.puml\" | head -3`\n- Services/containers: @docker-compose.yml or @k8s/ (if exists)\n- API definitions: !`find . -name \"*api*\" -o -name \"*openapi*\" -o -name \"*swagger*\" | head -3`\n\n## Task\n\nGenerate comprehensive architecture documentation with modern tooling and best practices:\n\n1. **Architecture Analysis and Discovery**\n   - Analyze current system architecture and component relationships\n   - Identify key architectural patterns and design decisions\n   - Document system boundaries, interfaces, and dependencies\n   - Assess data flow and communication patterns\n   - Identify architectural debt and improvement opportunities\n\n2. **Architecture Documentation Framework**\n   - Choose appropriate documentation framework and tools:\n     - **C4 Model**: Context, Containers, Components, Code diagrams\n     - **Arc42**: Comprehensive architecture documentation template\n     - **Architecture Decision Records (ADRs)**: Decision documentation\n     - **PlantUML/Mermaid**: Diagram-as-code documentation\n     - **Structurizr**: C4 model tooling and visualization\n     - **Draw.io/Lucidchart**: Visual diagramming tools\n\n3. **System Context Documentation**\n   - Create high-level system context diagrams\n   - Document external systems and integrations\n   - Define system boundaries and responsibilities\n   - Document user personas and stakeholders\n   - Create system landscape and ecosystem overview\n\n4. **Container and Service Architecture**\n   - Document container/service architecture and deployment view\n   - Create service dependency maps and communication patterns\n   - Document deployment architecture and infrastructure\n   - Define service boundaries and API contracts\n   - Document data persistence and storage architecture\n\n5. **Component and Module Documentation**\n   - Create detailed component architecture diagrams\n   - Document internal module structure and relationships\n   - Define component responsibilities and interfaces\n   - Document design patterns and architectural styles\n   - Create code organization and package structure documentation\n\n6. **Data Architecture Documentation**\n   - Document data models and database schemas\n   - Create data flow diagrams and processing pipelines\n   - Document data storage strategies and technologies\n   - Define data governance and lifecycle management\n   - Create data integration and synchronization documentation\n\n7. **Security and Compliance Architecture**\n   - Document security architecture and threat model\n   - Create authentication and authorization flow diagrams\n   - Document compliance requirements and controls\n   - Define security boundaries and trust zones\n   - Create incident response and security monitoring documentation\n\n8. **Quality Attributes and Cross-Cutting Concerns**\n   - Document performance characteristics and scalability patterns\n   - Create reliability and availability architecture documentation\n   - Document monitoring and observability architecture\n   - Define maintainability and evolution strategies\n   - Create disaster recovery and business continuity documentation\n\n9. **Architecture Decision Records (ADRs)**\n   - Create comprehensive ADR template and process\n   - Document historical architectural decisions and rationale\n   - Create decision tracking and review process\n   - Document trade-offs and alternatives considered\n   - Set up ADR maintenance and evolution procedures\n\n10. **Documentation Automation and Maintenance**\n    - Set up automated diagram generation from code annotations\n    - Configure documentation pipeline and publishing automation\n    - Set up documentation validation and consistency checking\n    - Create documentation review and approval process\n    - Train team on architecture documentation practices and tools\n    - Set up documentation versioning and change management",
      "tags": ["architecture", "documentation"]
    },
    {
      "command": "/create-database-migrations",
      "label": "`/create-database-migrations`",
      "category": "Arquitetura e Estrategia",
      "exemplos": [
        "`/create-database-migrations add_signal_latency --add-column`",
        "`/create-database-migrations create_trades_table --create-table`",
        "`/create-database-migrations adjust_positions --alter-table`"
      ],
      "capacidades": "Gera arquivos de migracao com up/down, cobrindo alteracoes de schema e dados.",
      "momentoIdeal": "Logo apos aprovar o desenho do schema, garantindo trilha auditavel entre ambientes.",
      "exemploMomento": "Criar migracao que adiciona coluna de latencia no Timescale apos documentar o schema correspondente.",
      "tipoSaida": "Scripts de migracao (arquivos em `migrations/`) acompanhados de instrucoes de execucao e rollback.",
      "fileName": "create-database-migrations.md",
      "filePath": ".claude/commands/create-database-migrations.md",
      "fileContent": "# Create Database Migrations\n\nCreate and manage database migrations: **$ARGUMENTS**\n\n## Current Database State\n\n- ORM detection: @package.json or @requirements.txt (detect Sequelize, Prisma, Alembic, etc.)\n- Migration files: !`find . -name \"*migration*\" -type f | head -5`\n- Database config: @config/database.* or @prisma/schema.prisma\n- Current schema: !`ls migrations/ 2>/dev/null | wc -l` migrations found\n\n## Task\n\nCreate comprehensive database migrations with proper versioning and rollback capabilities:\n\n**Migration Types**: Use $ARGUMENTS to specify table creation, column addition, table alteration, or data migration\n\n**Migration Framework**:\n1. **Migration Planning** - Analyze schema changes, dependencies, and data impact\n2. **Migration Generation** - Create timestamped migration files with up/down methods\n3. **Schema Updates** - Table creation, column modifications, index management\n4. **Data Migrations** - Safe data transformations and backfills\n5. **Rollback Strategy** - Implement reliable rollback procedures for each change\n6. **Testing** - Validate migrations in development and staging environments\n\n**Best Practices**: Follow database-specific conventions, maintain referential integrity, handle large datasets efficiently, and ensure zero-downtime deployments.\n\n**Output**: Production-ready migration files with comprehensive rollback support, proper indexing, and data safety measures.",
      "tags": ["database", "migration"]
    },
    {
      "command": "/create-feature",
      "label": "`/create-feature`",
      "category": "Planejamento e Orquestracao",
      "exemplos": [
        "`/create-feature position-aggregate`",
        "`/create-feature rag-embeddings`"
      ],
      "capacidades": "Fornece roteiro de planejamento, implementacao e testes de uma nova feature ponta a ponta.",
      "momentoIdeal": "Ao iniciar desenvolvimento de um novo agregado (ex.: PositionAggregate) garantindo aderencia ao DDD.",
      "exemploMomento": "Antes de criar endpoints para ingestao de sinais customizados, definindo passos e impactos.",
      "tipoSaida": "Plano em markdown com etapas, checklist tecnico e dependencias cruzadas para a feature.",
      "fileName": "create-feature.md",
      "filePath": ".claude/commands/create-feature.md",
      "fileContent": "# Create Feature\n\nScaffold new feature: $ARGUMENTS\n\n## Current Project Context\n\n- Project structure: !`find . -maxdepth 2 -type d -name src -o -name components -o -name features | head -5`\n- Current branch: !`git branch --show-current`\n- Package info: @package.json or @Cargo.toml or @requirements.txt (if exists)\n- Architecture docs: @docs/architecture.md or @README.md (if exists)\n\n## Task\n\nFollow this systematic approach to create a new feature: $ARGUMENTS\n\n1. **Feature Planning**\n   - Define the feature requirements and acceptance criteria\n   - Break down the feature into smaller, manageable tasks\n   - Identify affected components and potential impact areas\n   - Plan the API/interface design before implementation\n\n2. **Research and Analysis**\n   - Study existing codebase patterns and conventions\n   - Identify similar features for consistency\n   - Research external dependencies or libraries needed\n   - Review any relevant documentation or specifications\n\n3. **Architecture Design**\n   - Design the feature architecture and data flow\n   - Plan database schema changes if needed\n   - Define API endpoints and contracts\n   - Consider scalability and performance implications\n\n4. **Environment Setup**\n   - Create a new feature branch: `git checkout -b feature/$ARGUMENTS`\n   - Ensure development environment is up to date\n   - Install any new dependencies required\n   - Set up feature flags if applicable\n\n5. **Implementation Strategy**\n   - Start with core functionality and build incrementally\n   - Follow the project's coding standards and patterns\n   - Implement proper error handling and validation\n   - Use dependency injection and maintain loose coupling\n\n6. **Database Changes (if applicable)**\n   - Create migration scripts for schema changes\n   - Ensure backward compatibility\n   - Plan for rollback scenarios\n   - Test migrations on sample data\n\n7. **API Development**\n   - Implement API endpoints with proper HTTP status codes\n   - Add request/response validation\n   - Implement proper authentication and authorization\n   - Document API contracts and examples\n\n8. **Frontend Implementation (if applicable)**\n   - Create reusable components following project patterns\n   - Implement responsive design and accessibility\n   - Add proper state management\n   - Handle loading and error states\n\n9. **Testing Implementation**\n   - Write unit tests for core business logic\n   - Create integration tests for API endpoints\n   - Add end-to-end tests for user workflows\n   - Test error scenarios and edge cases\n\n10. **Security Considerations**\n    - Implement proper input validation and sanitization\n    - Add authorization checks for sensitive operations\n    - Review for common security vulnerabilities\n    - Ensure data protection and privacy compliance\n\n11. **Performance Optimization**\n    - Optimize database queries and indexes\n    - Implement caching where appropriate\n    - Monitor memory usage and optimize algorithms\n    - Consider lazy loading and pagination\n\n12. **Documentation**\n    - Add inline code documentation and comments\n    - Update API documentation\n    - Create user documentation if needed\n    - Update project README if applicable\n\n13. **Code Review Preparation**\n    - Run all tests and ensure they pass\n    - Run linting and formatting tools\n    - Check for code coverage and quality metrics\n    - Perform self-review of the changes\n\n14. **Integration Testing**\n    - Test feature integration with existing functionality\n    - Verify feature flags work correctly\n    - Test deployment and rollback procedures\n    - Validate monitoring and logging\n\n15. **Commit and Push**\n    - Create atomic commits with descriptive messages\n    - Follow conventional commit format if project uses it\n    - Push feature branch: `git push origin feature/$ARGUMENTS`\n\n16. **Pull Request Creation**\n    - Create PR with comprehensive description\n    - Include screenshots or demos if applicable\n    - Add appropriate labels and reviewers\n    - Link to any related issues or specifications\n\n17. **Quality Assurance**\n    - Coordinate with QA team for testing\n    - Address any bugs or issues found\n    - Verify accessibility and usability requirements\n    - Test on different environments and browsers\n\n18. **Deployment Planning**\n    - Plan feature rollout strategy\n    - Set up monitoring and alerting\n    - Prepare rollback procedures\n    - Schedule deployment and communication\n\nRemember to maintain code quality, follow project conventions, and prioritize user experience throughout the development process.",
      "tags": ["planning", "delivery"]
    },
    {
      "command": "/create-onboarding-guide",
      "label": "`/create-onboarding-guide`",
      "category": "Referencia e Organizacao",
      "exemplos": [
        "`/create-onboarding-guide --developer`",
        "`/create-onboarding-guide --designer`",
        "`/create-onboarding-guide --devops`",
        "`/create-onboarding-guide --comprehensive`",
        "`/create-onboarding-guide --interactive`"
      ],
      "capacidades": "Gera guia de onboarding completo com setup, workflows e treinamentos.",
      "momentoIdeal": "Quando um novo dev entra no time e precisa entender arquitetura limpa + pipelines Docker rapidamente.",
      "exemploMomento": "Ao integrar um analista que vai cuidar da esteira `docs/search`, fornecendo roteiro com ferramentas essenciais.",
      "tipoSaida": "Estrutura de guia em markdown detalhando topicos, passos e recursos de apoio para o novo integrante.",
      "fileName": "create-onboarding-guide.md",
      "filePath": ".claude/commands/create-onboarding-guide.md",
      "fileContent": "# Developer Onboarding Guide Generator\n\nCreate developer onboarding guide: $ARGUMENTS\n\n## Current Team Context\n\n- Project setup: @package.json or @requirements.txt or @Cargo.toml (detect tech stack)\n- Existing docs: @docs/ or @README.md (if exists)\n- Development tools: !`find . -name \".env*\" -o -name \"docker-compose.yml\" -o -name \"Makefile\" | head -3`\n- Team structure: @CODEOWNERS or @.github/ (if exists)\n- CI/CD setup: !`find .github/workflows -name \"*.yml\" 2>/dev/null | head -3`\n\n## Task\n\nCreate comprehensive onboarding experience tailored to role and project needs:\n\n1. **Onboarding Requirements Analysis**\n   - Analyze current team structure and skill requirements\n   - Identify key knowledge areas and learning objectives\n   - Assess current onboarding challenges and pain points\n   - Define onboarding timeline and milestone expectations\n   - Document role-specific requirements and responsibilities\n\n2. **Development Environment Setup Guide**\n   - Create comprehensive development environment setup instructions\n   - Document required tools, software, and system requirements\n   - Provide step-by-step installation and configuration guides\n   - Create environment validation and troubleshooting procedures\n   - Set up automated environment setup scripts and tools\n\n3. **Project and Codebase Overview**\n   - Create high-level project overview and business context\n   - Document system architecture and technology stack\n   - Provide codebase structure and organization guide\n   - Create code navigation and exploration guidelines\n   - Document key modules, libraries, and frameworks used\n\n4. **Development Workflow Documentation**\n   - Document version control workflows and branching strategies\n   - Create code review process and quality standards guide\n   - Document testing practices and requirements\n   - Provide deployment and release process overview\n   - Create issue tracking and project management workflow guide\n\n5. **Team Communication and Collaboration**\n   - Document team communication channels and protocols\n   - Create meeting schedules and participation guidelines\n   - Provide team contact information and org chart\n   - Document collaboration tools and access procedures\n   - Create escalation procedures and support contacts\n\n6. **Learning Resources and Training Materials**\n   - Curate learning resources for project-specific technologies\n   - Create hands-on tutorials and coding exercises\n   - Provide links to documentation, wikis, and knowledge bases\n   - Create video tutorials and screen recordings\n   - Set up mentoring and buddy system procedures\n\n7. **First Tasks and Milestones**\n   - Create progressive difficulty task assignments\n   - Define learning milestones and checkpoints\n   - Provide \"good first issues\" and starter projects\n   - Create hands-on coding challenges and exercises\n   - Set up pair programming and shadowing opportunities\n\n8. **Security and Compliance Training**\n   - Document security policies and access controls\n   - Create data handling and privacy guidelines\n   - Provide compliance training and certification requirements\n   - Document incident response and security procedures\n   - Create security best practices and guidelines\n\n9. **Tools and Resources Access**\n   - Document required accounts and access requests\n   - Create tool-specific setup and usage guides\n   - Provide license and subscription information\n   - Document VPN and network access procedures\n   - Create troubleshooting guides for common access issues\n\n10. **Feedback and Continuous Improvement**\n    - Create onboarding feedback collection process\n    - Set up regular check-ins and progress reviews\n    - Document common questions and FAQ section\n    - Create onboarding metrics and success tracking\n    - Establish onboarding guide maintenance and update procedures\n    - Set up new hire success monitoring and support systems",
      "tags": ["documentation", "onboarding"]
    },
    {
      "command": "/create-pr",
      "label": "`/create-pr`",
      "category": "Entrega e DevOps",
      "exemplos": [
        "`/create-pr --title \"feat: melhorar cache da documentacao\"`",
        "`/create-pr --title \"fix: corrigir webhook telegram\" --draft`"
      ],
      "capacidades": "Cria branch, formata com Biome, divide commits e abre PR com resumo/test plan.",
      "momentoIdeal": "Ao finalizar atividade e querer acelerar a abertura de PR com convencoes do TradingSystem.",
      "exemploMomento": "Abrir PR para ajustes no `vite-plugin-preload-hints.ts` com descricao e checagens automatizadas.",
      "tipoSaida": "Branch remoto com commits organizados e descricao de PR pronta para revisao.",
      "fileName": "create-pr.md",
      "filePath": ".claude/commands/create-pr.md",
      "fileContent": "# Create Pull Request Command\n\nCreate a new branch, commit changes, and submit a pull request.\n\n## Behavior\n- Creates a new branch based on current changes\n- Formats modified files using Biome\n- Analyzes changes and automatically splits into logical commits when appropriate\n- Each commit focuses on a single logical change or feature\n- Creates descriptive commit messages for each logical unit\n- Pushes branch to remote\n- Creates pull request with proper summary and test plan\n\n## Guidelines for Automatic Commit Splitting\n- Split commits by feature, component, or concern\n- Keep related file changes together in the same commit\n- Separate refactoring from feature additions\n- Ensure each commit can be understood independently\n- Multiple unrelated changes should be split into separate commits",
      "tags": ["git", "workflow"]
    },
    {
      "command": "/debug-error",
      "label": "`/debug-error`",
      "category": "Diagnostico e Seguranca",
      "exemplos": [
        "`/debug-error \"Erro 502 no webhook de sincronia\"`",
        "`/debug-error \"Worker Kestra travando no start\"`"
      ],
      "capacidades": "Metodologia completa de troubleshooting (reproducao, hipoteses, logs, ferramental).",
      "momentoIdeal": "Ao investigar erros intermitentes no telegram gateway ou em pipelines cron.",
      "exemploMomento": "Diagnosticar porque `telegramGateway.js` nao encaminha mensagens em horarios especificos.",
      "tipoSaida": "Passo a passo em texto com hipoteses, investigacoes realizadas e plano de acao final.",
      "fileName": "debug-error.md",
      "filePath": ".claude/commands/debug-error.md",
      "fileContent": "# Systematically Debug and Fix Errors\n\nSystematically debug and fix errors\n\n## Instructions\n\nFollow this comprehensive debugging methodology to resolve: **$ARGUMENTS**\n\n1. **Error Information Gathering**\n   - Collect the complete error message, stack trace, and error code\n   - Note when the error occurs (timing, conditions, frequency)\n   - Identify the environment where the error happens (dev, staging, prod)\n   - Gather relevant logs from before and after the error\n\n2. **Reproduce the Error**\n   - Create a minimal test case that reproduces the error consistently\n   - Document the exact steps needed to trigger the error\n   - Test in different environments if possible\n   - Note any patterns or conditions that affect error occurrence\n\n3. **Stack Trace Analysis**\n   - Read the stack trace from bottom to top to understand the call chain\n   - Identify the exact line where the error originates\n   - Trace the execution path leading to the error\n   - Look for any obvious issues in the failing code\n\n4. **Code Context Investigation**\n   - Examine the code around the error location\n   - Check recent changes that might have introduced the bug\n   - Review variable values and state at the time of error\n   - Analyze function parameters and return values\n\n5. **Hypothesis Formation**\n   - Based on evidence, form hypotheses about the root cause\n   - Consider common causes:\n     - Null pointer/undefined reference\n     - Type mismatches\n     - Race conditions\n     - Resource exhaustion\n     - Logic errors\n     - External dependency failures\n\n6. **Debugging Tools Setup**\n   - Set up appropriate debugging tools for the technology stack\n   - Use debugger, profiler, or logging as needed\n   - Configure breakpoints at strategic locations\n   - Set up monitoring and alerting if not already present\n\n7. **Systematic Investigation**\n   - Test each hypothesis methodically\n   - Use binary search approach to isolate the problem\n   - Add strategic logging or print statements\n   - Check data flow and transformations step by step\n\n8. **Data Validation**\n   - Verify input data format and validity\n   - Check for edge cases and boundary conditions\n   - Validate assumptions about data state\n   - Test with different data sets to isolate patterns\n\n9. **Dependency Analysis**\n   - Check external dependencies and their versions\n   - Verify network connectivity and API availability\n   - Review configuration files and environment variables\n   - Test database connections and query execution\n\n10. **Memory and Resource Analysis**\n    - Check for memory leaks or excessive memory usage\n    - Monitor CPU and I/O resource consumption\n    - Analyze garbage collection patterns if applicable\n    - Check for resource deadlocks or contention\n\n11. **Concurrency Issues Investigation**\n    - Look for race conditions in multi-threaded code\n    - Check synchronization mechanisms and locks\n    - Analyze async operations and promise handling\n    - Test under different load conditions\n\n12. **Root Cause Identification**\n    - Once the cause is identified, understand why it happened\n    - Determine if it's a logic error, design flaw, or external issue\n    - Assess the scope and impact of the problem\n    - Consider if similar issues exist elsewhere\n\n13. **Solution Implementation**\n    - Design a fix that addresses the root cause\n    - Consider multiple solution approaches and trade-offs\n    - Implement the fix with appropriate error handling\n    - Add validation and defensive programming where needed\n\n14. **Testing the Fix**\n    - Test the fix against the original error case\n    - Test edge cases and related scenarios\n    - Run regression tests to ensure no new issues\n    - Test under various load and stress conditions\n\n15. **Prevention Measures**\n    - Add appropriate unit and integration tests\n    - Improve error handling and logging\n    - Add input validation and defensive checks\n    - Update documentation and code comments\n\n16. **Monitoring and Alerting**\n    - Set up monitoring for similar issues\n    - Add metrics and health checks\n    - Configure alerts for error thresholds\n    - Implement better observability\n\n17. **Documentation**\n    - Document the error, investigation process, and solution\n    - Update troubleshooting guides\n    - Share learnings with the team\n    - Update code comments with context\n\n18. **Post-Resolution Review**\n    - Analyze why the error wasn't caught earlier\n    - Review development and testing processes\n    - Consider improvements to prevent similar issues\n    - Update coding standards or guidelines if needed\n\nRemember to maintain detailed notes throughout the debugging process and consider the wider implications of both the error and the fix.",
      "tags": ["troubleshooting", "bugfix"]
    },
    {
      "command": "/design-database-schema",
      "label": "`/design-database-schema`",
      "category": "Arquitetura e Estrategia",
      "exemplos": [
        "`/design-database-schema --relational signals`",
        "`/design-database-schema --nosql telemetry`",
        "`/design-database-schema --hybrid analytics`",
        "`/design-database-schema --normalize`"
      ],
      "capacidades": "Modela entidades, chaves e relacionamentos equilibrando normalizacao e performance.",
      "momentoIdeal": "Durante discovery de novas tabelas ou ajustes (ex.: TimescaleDB) para evitar retrabalho depois de codar.",
      "exemploMomento": "Definir estrutura da tabela de auditoria de ordens antes de criar migracoes para Timescale e QuestDB.",
      "tipoSaida": "Desenho de schema com diagramas/DDL sugerido, lista de entidades e justificativas de arquitetura de dados.",
      "fileName": "design-database-schema.md",
      "filePath": ".claude/commands/design-database-schema.md",
      "fileContent": "# Design Database Schema\n\nDesign optimized database schemas with comprehensive data modeling: **$ARGUMENTS**\n\n## Current Project Context\n\n- Application type: Based on $ARGUMENTS or codebase analysis\n- Data requirements: @requirements/ or project documentation\n- Existing schema: @prisma/schema.prisma or @migrations/ or database dumps\n- Performance needs: Expected scale, query patterns, and data volume\n\n## Task\n\nDesign comprehensive database schema with optimal structure and performance:\n\n**Schema Type**: Use $ARGUMENTS to specify relational, NoSQL, hybrid approach, or normalization level\n\n**Design Framework**:\n1. **Requirements Analysis** - Business entities, relationships, data flow, and access patterns\n2. **Entity Modeling** - Tables/collections, attributes, primary/foreign keys, constraints\n3. **Relationship Design** - One-to-one, one-to-many, many-to-many associations\n4. **Normalization Strategy** - Data consistency vs performance trade-offs\n5. **Performance Optimization** - Indexing strategy, query optimization, partitioning\n6. **Security Design** - Access control, data encryption, audit trails\n\n**Advanced Patterns**: Implement temporal data, soft deletes, JSONB fields, full-text search, audit logging, and scalability patterns.\n\n**Validation**: Ensure referential integrity, data consistency, query performance, and future extensibility.\n\n**Output**: Complete schema design with DDL scripts, ER diagrams, performance analysis, and migration strategy.",
      "tags": ["database", "design"]
    },
    {
      "command": "/design-rest-api",
      "label": "`/design-rest-api`",
      "category": "Arquitetura e Estrategia",
      "exemplos": [
        "`/design-rest-api --v1 documentation-api`",
        "`/design-rest-api --v2 workspace-api`",
        "`/design-rest-api --graphql-hybrid`",
        "`/design-rest-api --openapi`"
      ],
      "capacidades": "Estrutura recursos HTTP, contratos, autenticacao e versionamento seguindo boas praticas REST.",
      "momentoIdeal": "Antes de implementar novas rotas (ex.: ampliar documentation API) para alinhar contrato com stakeholders.",
      "exemploMomento": "Planejar a API que servira documentos versionados no dashboard antes de abrir tarefas de desenvolvimento.",
      "tipoSaida": "Especificacao de API em texto estruturado (tabela de endpoints, esquemas, fluxos de auth) e possivel draft OpenAPI.",
      "fileName": "design-rest-api.md",
      "filePath": ".claude/commands/design-rest-api.md",
      "fileContent": "# Design REST API\n\nDesign comprehensive RESTful API architecture: **$ARGUMENTS**\n\n## Current Application State\n\n- Framework detection: @package.json or @requirements.txt (Express, FastAPI, Spring Boot, etc.)\n- Existing API: !`grep -r \"route\\|endpoint\\|@app\\\\.route\" src/ 2>/dev/null | wc -l` routes found\n- Authentication: !`grep -r \"auth\\|jwt\\|session\" src/ 2>/dev/null | wc -l` auth components\n- Documentation: @swagger.yaml or @openapi.json (if exists)\n\n## Task\n\nDesign complete RESTful API with industry best practices and comprehensive functionality:\n\n**API Version**: Use $ARGUMENTS to specify API version, GraphQL hybrid approach, or OpenAPI specification\n\n**API Architecture**:\n1. **Resource Design** - RESTful endpoints, HTTP methods, URL structure, resource relationships\n2. **Request/Response Models** - Data validation, serialization, error handling, status codes\n3. **Authentication & Authorization** - JWT, OAuth, RBAC, API keys, rate limiting\n4. **API Documentation** - OpenAPI/Swagger specs, interactive documentation, code examples\n5. **Versioning Strategy** - URL, header, or content-type based versioning\n6. **Performance & Security** - Caching, pagination, CORS, input validation, SQL injection prevention\n\n**Advanced Features**: Real-time capabilities, file uploads, batch operations, webhooks, and monitoring integration.\n\n**Standards Compliance**: Follow REST principles, HTTP specifications, and API design best practices.\n\n**Output**: Complete API specification with endpoints, authentication, validation, documentation, and client SDKs.",
      "tags": ["api", "design"]
    },
    {
      "command": "/doc-api",
      "label": "`/doc-api`",
      "category": "APIs e Dados",
      "exemplos": [
        "`/doc-api --rest documentation-api`",
        "`/doc-api --graphql rag-proxy`",
        "`/doc-api --grpc tp-capital`"
      ],
      "capacidades": "Gera documentacao de endpoints, parametros, exemplos e erros.",
      "momentoIdeal": "Depois de alterar rotas (ex.: rag-proxy) para atualizar referencia consumida por squads internos.",
      "exemploMomento": "Atualizar docs apos criar rota `/proxy/secure` para garantir onboarding do time de suporte.",
      "tipoSaida": "Documento markdown ou OpenAPI parcial descrevendo endpoints, parametros e exemplos de requests/responses.",
      "fileName": "doc-api.md",
      "filePath": ".claude/commands/doc-api.md",
      "fileContent": "# API Documentation Generator\n\nGenerate API documentation from code: $ARGUMENTS\n\n## Current API Context\n\n- API endpoints: !`find . -name \"*route*\" -o -name \"*controller*\" -o -name \"*api*\" | head -5`\n- API specs: !`find . -name \"*openapi*\" -o -name \"*swagger*\" -o -name \"*.graphql\" | head -3`\n- Server framework: @package.json or detect from imports\n- Existing docs: @docs/api/ or @api-docs/ (if exists)\n- Test files: !`find . -name \"*test*\" -path \"*/api/*\" | head -3`\n\n## Task\n\nGenerate comprehensive API documentation with interactive features: $ARGUMENTS\n\n1. **Code Analysis and Discovery**\n   - Scan the codebase for API endpoints, routes, and handlers\n   - Identify REST APIs, GraphQL schemas, and RPC services\n   - Map out controller classes, route definitions, and middleware\n   - Discover request/response models and data structures\n\n2. **Documentation Tool Selection**\n   - Choose appropriate documentation tools based on stack:\n     - **OpenAPI/Swagger**: REST APIs with interactive documentation\n     - **GraphQL**: GraphiQL, GraphQL Playground, or Apollo Studio\n     - **Postman**: API collections and documentation\n     - **Insomnia**: API design and documentation\n     - **Redoc**: Alternative OpenAPI renderer\n     - **API Blueprint**: Markdown-based API documentation\n\n3. **API Specification Generation**\n   \n   **For REST APIs with OpenAPI:**\n   ```yaml\n   openapi: 3.0.0\n   info:\n     title: $ARGUMENTS API\n     version: 1.0.0\n     description: Comprehensive API for $ARGUMENTS\n   servers:\n     - url: https://api.example.com/v1\n   paths:\n     /users:\n       get:\n         summary: List users\n         parameters:\n           - name: page\n             in: query\n             schema:\n               type: integer\n         responses:\n           '200':\n             description: Successful response\n             content:\n               application/json:\n                 schema:\n                   type: array\n                   items:\n                     $ref: '#/components/schemas/User'\n   components:\n     schemas:\n       User:\n         type: object\n         properties:\n           id:\n             type: integer\n           name:\n             type: string\n           email:\n             type: string\n   ```\n\n4. **Endpoint Documentation**\n   - Document all HTTP methods (GET, POST, PUT, DELETE, PATCH)\n   - Specify request parameters (path, query, header, body)\n   - Define response schemas and status codes\n   - Include error responses and error codes\n   - Document authentication and authorization requirements\n\n5. **Request/Response Examples**\n   - Provide realistic request examples for each endpoint\n   - Include sample response data with proper formatting\n   - Show different response scenarios (success, error, edge cases)\n   - Document content types and encoding\n\n6. **Authentication Documentation**\n   - Document authentication methods (API keys, JWT, OAuth)\n   - Explain authorization scopes and permissions\n   - Provide authentication examples and token formats\n   - Document session management and refresh token flows\n\n7. **Data Model Documentation**\n   - Define all data schemas and models\n   - Document field types, constraints, and validation rules\n   - Include relationships between entities\n   - Provide example data structures\n\n8. **Error Handling Documentation**\n   - Document all possible error responses\n   - Explain error codes and their meanings\n   - Provide troubleshooting guidance\n   - Include rate limiting and throttling information\n\n9. **Interactive Documentation Setup**\n   \n   **Swagger UI Integration:**\n   ```html\n   <!DOCTYPE html>\n   <html>\n   <head>\n     <title>API Documentation</title>\n     <link rel=\"stylesheet\" type=\"text/css\" href=\"./swagger-ui-bundle.css\" />\n   </head>\n   <body>\n     <div id=\"swagger-ui\"></div>\n     <script src=\"./swagger-ui-bundle.js\"></script>\n     <script>\n       SwaggerUIBundle({\n         url: './api-spec.yaml',\n         dom_id: '#swagger-ui'\n       });\n     </script>\n   </body>\n   </html>\n   ```\n\n10. **Code Annotation and Comments**\n    - Add inline documentation to API handlers\n    - Use framework-specific annotation tools:\n      - **Java**: @ApiOperation, @ApiParam (Swagger annotations)\n      - **Python**: Docstrings with FastAPI or Flask-RESTX\n      - **Node.js**: JSDoc comments with swagger-jsdoc\n      - **C#**: XML documentation comments\n\n11. **Automated Documentation Generation**\n    \n    **For Node.js/Express:**\n    ```javascript\n    const swaggerJsdoc = require('swagger-jsdoc');\n    const swaggerUi = require('swagger-ui-express');\n    \n    const options = {\n      definition: {\n        openapi: '3.0.0',\n        info: {\n          title: 'API Documentation',\n          version: '1.0.0',\n        },\n      },\n      apis: ['./routes/*.js'],\n    };\n    \n    const specs = swaggerJsdoc(options);\n    app.use('/api-docs', swaggerUi.serve, swaggerUi.setup(specs));\n    ```\n\n12. **Testing Integration**\n    - Generate API test collections from documentation\n    - Include test scripts and validation rules\n    - Set up automated API testing\n    - Document test scenarios and expected outcomes\n\n13. **Version Management**\n    - Document API versioning strategy\n    - Maintain documentation for multiple API versions\n    - Document deprecation timelines and migration guides\n    - Track breaking changes between versions\n\n14. **Performance Documentation**\n    - Document rate limits and throttling policies\n    - Include performance benchmarks and SLAs\n    - Document caching strategies and headers\n    - Explain pagination and filtering options\n\n15. **SDK and Client Library Documentation**\n    - Generate client libraries from API specifications\n    - Document SDK usage and examples\n    - Provide quickstart guides for different languages\n    - Include integration examples and best practices\n\n16. **Environment-Specific Documentation**\n    - Document different environments (dev, staging, prod)\n    - Include environment-specific endpoints and configurations\n    - Document deployment and configuration requirements\n    - Provide environment setup instructions\n\n17. **Security Documentation**\n    - Document security best practices\n    - Include CORS and CSP policies\n    - Document input validation and sanitization\n    - Explain security headers and their purposes\n\n18. **Maintenance and Updates**\n    - Set up automated documentation updates\n    - Create processes for keeping documentation current\n    - Review and validate documentation regularly\n    - Integrate documentation reviews into development workflow\n\n**Framework-Specific Examples:**\n\n**FastAPI (Python):**\n```python\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel\n\napp = FastAPI(title=\"My API\", version=\"1.0.0\")\n\nclass User(BaseModel):\n    id: int\n    name: str\n    email: str\n\n@app.get(\"/users/{user_id}\", response_model=User)\nasync def get_user(user_id: int):\n    \"\"\"Get a user by ID.\"\"\"\n    return {\"id\": user_id, \"name\": \"John\", \"email\": \"john@example.com\"}\n```\n\n**Spring Boot (Java):**\n```java\n@RestController\n@Api(tags = \"Users\")\npublic class UserController {\n    \n    @GetMapping(\"/users/{id}\")\n    @ApiOperation(value = \"Get user by ID\")\n    public ResponseEntity<User> getUser(\n        @PathVariable @ApiParam(\"User ID\") Long id) {\n        // Implementation\n    }\n}\n```\n\nRemember to keep documentation up-to-date with code changes and make it easily accessible to both internal teams and external consumers.",
      "tags": ["documentation", "api"]
    },
    {
      "command": "/docs-maintenance",
      "label": "`/docs-maintenance`",
      "category": "Referencia e Organizacao",
      "exemplos": [
        "`/docs-maintenance --audit`",
        "`/docs-maintenance --update`",
        "`/docs-maintenance --validate`",
        "`/docs-maintenance --optimize`",
        "`/docs-maintenance --comprehensive`"
      ],
      "capacidades": "Audita a documentacao (links, estrutura, estilo) e propaga correcoes.",
      "momentoIdeal": "Nos sprints de governanca da base Docusaurus para evitar links quebrados antes de publicar relatorios semanais.",
      "exemploMomento": "Na vespera de enviar o `DOCUSAURUS-REVIEW-REPORT` para diretoria, garantindo que nao ha links mortos.",
      "tipoSaida": "Relatorio textual com achados da auditoria, tarefas priorizadas e comandos sugeridos para correcao.",
      "fileName": "docs-maintenance.md",
      "filePath": ".claude/commands/docs-maintenance.md",
      "fileContent": "# Documentation Maintenance & Quality Assurance\n\nImplement comprehensive documentation maintenance system: $ARGUMENTS\n\n## Current Documentation Health\n\n- Documentation files: !`find . -name \"*.md\" -o -name \"*.mdx\" | wc -l` files\n- Last updates: !`find . -name \"*.md\" -exec stat -f \"%m %N\" {} \\; | sort -n | tail -5`\n- External links: !`grep -r \"http\" --include=\"*.md\" . | wc -l` links to validate\n- Image references: !`grep -r \"!\\[.*\\]\" --include=\"*.md\" . | wc -l` images to check\n- Documentation structure: @docs/ or detect documentation directories\n\n## Task\n\nCreate systematic documentation maintenance framework with automated quality assurance, comprehensive validation, content optimization, and regular update procedures.\n\n## Documentation Maintenance Framework\n\n### 1. Content Quality Audit System\n- Comprehensive file discovery and categorization\n- Content freshness analysis and aging detection\n- Word count, readability, and structure assessment\n- Missing sections and incomplete documentation identification\n- TODO/FIXME marker tracking and resolution planning\n\n### 2. Link and Reference Validation\n- External link health monitoring with retry logic\n- Internal link validation and broken reference detection\n- Image reference verification and missing asset identification\n- Cross-reference consistency checking\n- Automated link correction suggestions\n\n### 3. Style and Consistency Checking\n- Markdown syntax validation and formatting standards\n- Heading hierarchy and structure consistency\n- List formatting and emphasis style uniformity\n- Code block formatting and language specification\n- Accessibility compliance (alt text, descriptive links)\n\n### 4. Content Optimization and Enhancement\n- Table of contents generation for long documents\n- Metadata updating and frontmatter management\n- Common formatting issue correction\n- Spelling and grammar validation\n- Readability analysis and improvement suggestions\n\n### 5. Automated Synchronization System\n- Git-based change tracking and documentation updates\n- Version control integration with branch management\n- Automated commit generation with detailed change logs\n- Merge conflict resolution strategies\n- Rollback procedures for failed updates\n\n### 6. Quality Assurance Reporting\n- Comprehensive audit reports with severity classifications\n- Issue categorization and prioritization systems\n- Progress tracking and maintenance metrics\n- Automated notification systems for critical issues\n- Dashboard creation for ongoing monitoring\n\n## Implementation Requirements\n\n### Audit Configuration\n- Configurable quality thresholds and validation rules\n- Custom style guide integration and enforcement\n- Platform-specific optimization settings\n- Team collaboration workflow integration\n- Automated scheduling and recurring maintenance\n\n### Validation Processes\n- Multi-level validation with error categorization\n- Batch processing for large documentation sets\n- Performance optimization for comprehensive scans\n- Integration with existing CI/CD pipelines\n- Real-time monitoring and alerting systems\n\n### Reporting and Analytics\n- Detailed maintenance reports with actionable insights\n- Historical trend analysis and improvement tracking\n- Team productivity metrics and documentation health scores\n- Integration with project management tools\n- Automated stakeholder communication\n\n## Deliverables\n\n1. **Maintenance System Architecture**\n   - Automated audit and validation framework\n   - Content optimization and enhancement tools\n   - Quality assurance reporting infrastructure\n   - Version control integration and synchronization\n\n2. **Validation and Quality Tools**\n   - Link checking and reference validation systems\n   - Style consistency and accessibility compliance tools\n   - Content freshness and completeness analyzers\n   - Automated correction and enhancement utilities\n\n3. **Reporting and Monitoring**\n   - Comprehensive audit reports with prioritized recommendations\n   - Real-time monitoring dashboards and alert systems\n   - Progress tracking and maintenance history documentation\n   - Integration with team communication and project tools\n\n4. **Documentation and Procedures**\n   - Implementation guidelines and configuration instructions\n   - Team workflow integration and collaboration procedures\n   - Troubleshooting guides and maintenance best practices\n   - Automated scheduling and recurring maintenance setup\n\n## Integration Guidelines\n\nImplement with existing documentation platforms and development workflows. Ensure scalability for large documentation sets and team collaboration while maintaining quality standards and accessibility compliance.",
      "tags": ["documentation", "quality", "governance"]
    },
    {
      "command": "/explain-code",
      "label": "`/explain-code`",
      "category": "Diagnostico e Seguranca",
      "exemplos": [
        "`/explain-code frontend/dashboard/src/hooks/llamaIndex/useRagQuery.ts`",
        "`/explain-code backend/api/telegram-gateway/src/routes/telegramGateway.js`"
      ],
      "capacidades": "Decompoe e explica blocos de codigo com analise de fluxo e dependencias.",
      "momentoIdeal": "Quando precisa repassar logica complexa (ex.: hook `useRagQuery`) para outro dev ou documentacao.",
      "exemploMomento": "Preparar apresentacao do `CollectionFilesTable.tsx` antes de handoff para novo integrante.",
      "tipoSaida": "Analise comentada por secoes/linhas, incluindo descricoes de funcoes e implicacoes de design.",
      "fileName": "explain-code.md",
      "filePath": ".claude/commands/explain-code.md",
      "fileContent": "# Analyze and Explain Code Functionality\n\nAnalyze and explain code functionality\n\n## Instructions\n\nFollow this systematic approach to explain code: **$ARGUMENTS**\n\n1. **Code Context Analysis**\n   - Identify the programming language and framework\n   - Understand the broader context and purpose of the code\n   - Identify the file location and its role in the project\n   - Review related imports, dependencies, and configurations\n\n2. **High-Level Overview**\n   - Provide a summary of what the code does\n   - Explain the main purpose and functionality\n   - Identify the problem the code is solving\n   - Describe how it fits into the larger system\n\n3. **Code Structure Breakdown**\n   - Break down the code into logical sections\n   - Identify classes, functions, and methods\n   - Explain the overall architecture and design patterns\n   - Map out data flow and control flow\n\n4. **Line-by-Line Analysis**\n   - Explain complex or non-obvious lines of code\n   - Describe variable declarations and their purposes\n   - Explain function calls and their parameters\n   - Clarify conditional logic and loops\n\n5. **Algorithm and Logic Explanation**\n   - Describe the algorithm or approach being used\n   - Explain the logic behind complex calculations\n   - Break down nested conditions and loops\n   - Clarify recursive or asynchronous operations\n\n6. **Data Structures and Types**\n   - Explain data types and structures being used\n   - Describe how data is transformed or processed\n   - Explain object relationships and hierarchies\n   - Clarify input and output formats\n\n7. **Framework and Library Usage**\n   - Explain framework-specific patterns and conventions\n   - Describe library functions and their purposes\n   - Explain API calls and their expected responses\n   - Clarify configuration and setup code\n\n8. **Error Handling and Edge Cases**\n   - Explain error handling mechanisms\n   - Describe exception handling and recovery\n   - Identify edge cases being handled\n   - Explain validation and defensive programming\n\n9. **Performance Considerations**\n   - Identify performance-critical sections\n   - Explain optimization techniques being used\n   - Describe complexity and scalability implications\n   - Point out potential bottlenecks or inefficiencies\n\n10. **Security Implications**\n    - Identify security-related code sections\n    - Explain authentication and authorization logic\n    - Describe input validation and sanitization\n    - Point out potential security vulnerabilities\n\n11. **Testing and Debugging**\n    - Explain how the code can be tested\n    - Identify debugging points and logging\n    - Describe mock data or test scenarios\n    - Explain test helpers and utilities\n\n12. **Dependencies and Integrations**\n    - Explain external service integrations\n    - Describe database operations and queries\n    - Explain API interactions and protocols\n    - Clarify third-party library usage\n\n**Explanation Format Examples:**\n\n**For Complex Algorithms:**\n```\nThis function implements a depth-first search algorithm:\n\n1. Line 1-3: Initialize a stack with the starting node and a visited set\n2. Line 4-8: Main loop - continue until stack is empty\n3. Line 9-11: Pop a node and check if it's the target\n4. Line 12-15: Add unvisited neighbors to the stack\n5. Line 16: Return null if target not found\n\nTime Complexity: O(V + E) where V is vertices and E is edges\nSpace Complexity: O(V) for the visited set and stack\n```\n\n**For API Integration Code:**\n```\nThis code handles user authentication with a third-party service:\n\n1. Extract credentials from request headers\n2. Validate credential format and required fields\n3. Make API call to authentication service\n4. Handle response and extract user data\n5. Create session token and set cookies\n6. Return user profile or error response\n\nError Handling: Catches network errors, invalid credentials, and service unavailability\nSecurity: Uses HTTPS, validates inputs, and sanitizes responses\n```\n\n**For Database Operations:**\n```\nThis function performs a complex database query with joins:\n\n1. Build base query with primary table\n2. Add LEFT JOIN for related user data\n3. Apply WHERE conditions for filtering\n4. Add ORDER BY for consistent sorting\n5. Implement pagination with LIMIT/OFFSET\n6. Execute query and handle potential errors\n7. Transform raw results into domain objects\n\nPerformance Notes: Uses indexes on filtered columns, implements connection pooling\n```\n\n13. **Common Patterns and Idioms**\n    - Identify language-specific patterns and idioms\n    - Explain design patterns being implemented\n    - Describe architectural patterns in use\n    - Clarify naming conventions and code style\n\n14. **Potential Improvements**\n    - Suggest code improvements and optimizations\n    - Identify possible refactoring opportunities\n    - Point out maintainability concerns\n    - Recommend best practices and standards\n\n15. **Related Code and Context**\n    - Reference related functions and classes\n    - Explain how this code interacts with other components\n    - Describe the calling context and usage patterns\n    - Point to relevant documentation and resources\n\n16. **Debugging and Troubleshooting**\n    - Explain how to debug issues in this code\n    - Identify common failure points\n    - Describe logging and monitoring approaches\n    - Suggest testing strategies\n\n**Language-Specific Considerations:**\n\n**JavaScript/TypeScript:**\n- Explain async/await and Promise handling\n- Describe closure and scope behavior\n- Clarify this binding and arrow functions\n- Explain event handling and callbacks\n\n**Python:**\n- Explain list comprehensions and generators\n- Describe decorator usage and purpose\n- Clarify context managers and with statements\n- Explain class inheritance and method resolution\n\n**Java:**\n- Explain generics and type parameters\n- Describe annotation usage and processing\n- Clarify stream operations and lambda expressions\n- Explain exception hierarchy and handling\n\n**C#:**\n- Explain LINQ queries and expressions\n- Describe async/await and Task handling\n- Clarify delegate and event usage\n- Explain nullable reference types\n\n**Go:**\n- Explain goroutines and channel usage\n- Describe interface implementation\n- Clarify error handling patterns\n- Explain package structure and imports\n\n**Rust:**\n- Explain ownership and borrowing\n- Describe lifetime annotations\n- Clarify pattern matching and Option/Result types\n- Explain trait implementations\n\nRemember to:\n- Use clear, non-technical language when possible\n- Provide examples and analogies for complex concepts\n- Structure explanations logically from high-level to detailed\n- Include visual diagrams or flowcharts when helpful\n- Tailor the explanation level to the intended audience",
      "tags": ["documentation", "knowledge"]
    },
    {
      "command": "/format",
      "label": "`/format`",
      "category": "Qualidade e Testes",
      "exemplos": [
        "`/format`",
        "`/format --check`",
        "`/format src/components/`",
        "`/format --staged`",
        "`/format all`"
      ],
      "capacidades": "Aplica Prettier em escopos (frontend, backend, diretorios ou staged).",
      "momentoIdeal": "Antes do commit final garantindo consistencia no repo monorepo.",
      "exemploMomento": "Normalizar formato dos arquivos em `frontend/dashboard/docs/` antes de gerar diff final para PR.",
      "tipoSaida": "Arquivos atualizados em disco e relatorio textual resumindo quais caminhos foram formatados.",
      "fileName": "format.md",
      "filePath": ".claude/commands/format.md",
      "fileContent": "# Format Command\n\nExecute Prettier para formatação de código.\n\n## Usage\n\n```bash\n/format [target] [options]\n```\n\n## Targets\n\n- `frontend` - Format frontend/dashboard (default)\n- `backend` - Format all backend code\n- `all` - Format frontend + backend\n- `<path>` - Format specific file or directory\n\n## Options\n\n- `--check` - Only check (don't modify files)\n- `--staged` - Format only staged files (Git)\n\n## Examples\n\n```bash\n# Format frontend\n/format\n\n# Check only (no changes)\n/format --check\n\n# Format specific file\n/format src/components/pages/DocsHybridSearchPage.tsx\n\n# Format specific directory\n/format src/components/\n\n# Format only staged files\n/format --staged\n\n# Format all code\n/format all\n```\n\n## Implementation\n\n```bash\n# Frontend\nif [[ \"{{target}}\" == \"frontend\" ]] || [[ \"{{target}}\" == \"\" ]]; then\n  cd frontend/dashboard\n\n  if [[ \"{{args}}\" == *\"--check\"* ]]; then\n    npx prettier --check src/\n  elif [[ \"{{args}}\" == *\"--staged\"* ]]; then\n    npx prettier --write $(git diff --cached --name-only --diff-filter=ACMR \"*.ts\" \"*.tsx\" \"*.js\" \"*.jsx\" \"*.json\" \"*.css\" \"*.md\")\n  else\n    npx prettier --write src/\n  fi\n\n  cd ../..\nfi\n\n# Specific path\nif [[ \"{{target}}\" != \"frontend\" ]] && [[ \"{{target}}\" != \"backend\" ]] && [[ \"{{target}}\" != \"all\" ]] && [[ \"{{target}}\" != \"\" ]]; then\n  npx prettier --write \"{{target}}\"\nfi\n\n# Backend\nif [[ \"{{target}}\" == \"backend\" ]] || [[ \"{{target}}\" == \"all\" ]]; then\n  for api in backend/api/*/; do\n    cd \"$api\"\n    if [[ -f \".prettierrc\" ]] || [[ -f \"package.json\" ]]; then\n      echo \"Formatting $api...\"\n      npx prettier --write src/\n    fi\n    cd ../../..\n  done\nfi\n```\n\n## Prettier Configuration\n\n`.prettierrc`:\n\n```json\n{\n  \"semi\": true,\n  \"trailingComma\": \"es5\",\n  \"singleQuote\": true,\n  \"printWidth\": 80,\n  \"tabWidth\": 2,\n  \"useTabs\": false\n}\n```\n\n## File Types Formatted\n\n- JavaScript/TypeScript: `.js`, `.jsx`, `.ts`, `.tsx`\n- JSON: `.json`\n- CSS: `.css`, `.scss`\n- HTML: `.html`\n- Markdown: `.md`\n\n## VSCode Integration\n\nInstall extension:\n```bash\ncode --install-extension esbenp.prettier-vscode\n```\n\nSettings (`.vscode/settings.json`):\n```json\n{\n  \"editor.formatOnSave\": true,\n  \"editor.defaultFormatter\": \"esbenp.prettier-vscode\"\n}\n```\n\n## Pre-commit Hook\n\nSetup with Husky:\n\n```bash\n# Install\nnpm install --save-dev husky lint-staged\n\n# Init\nnpx husky init\n\n# Add hook\necho \"npx lint-staged\" > .husky/pre-commit\n```\n\n`package.json`:\n```json\n{\n  \"lint-staged\": {\n    \"*.{ts,tsx,js,jsx,json,css,md}\": [\n      \"prettier --write\"\n    ]\n  }\n}\n```\n\n## Ignore Files\n\n`.prettierignore`:\n```\ndist/\nbuild/\ncoverage/\nnode_modules/\n*.min.js\n*.bundle.js\npackage-lock.json\n```\n\n## Common Formatting\n\n### Before\n```typescript\nconst user={name:\"John\",age:30,city:\"NYC\"}\nfunction greet(name){return \"Hello \"+name}\n```\n\n### After\n```typescript\nconst user = {\n  name: 'John',\n  age: 30,\n  city: 'NYC',\n};\n\nfunction greet(name) {\n  return 'Hello ' + name;\n}\n```\n\n## Check Format in CI\n\n```yaml\n# .github/workflows/format.yml\n- name: Check Format\n  run: |\n    cd frontend/dashboard\n    npx prettier --check src/\n```\n\n## Related Commands\n\n- `/lint` - ESLint (includes some formatting rules)\n- `/quality-check` - Full quality check\n- `/fix-all` - Auto-fix linting + formatting\n",
      "tags": ["quality", "formatting"]
    },
    {
      "command": "/generate-api-documentation",
      "label": "`/generate-api-documentation`",
      "category": "APIs e Dados",
      "exemplos": [
        "`/generate-api-documentation --swagger-ui`",
        "`/generate-api-documentation --redoc`",
        "`/generate-api-documentation --postman`",
        "`/generate-api-documentation --insomnia`",
        "`/generate-api-documentation --multi-format`"
      ],
      "capacidades": "Automatiza geracao de specs e portais (Swagger UI, Redoc, Postman) e integra com CI.",
      "momentoIdeal": "Ao preparar publicacao da API TP Capital para parceiros ou para disponibilizar colecoes atualizadas.",
      "exemploMomento": "Antes de compartilhar endpoints do TP Capital com aliados, produzindo portal atual com Swagger UI.",
      "tipoSaida": "Artefatos de documentacao (arquivos OpenAPI, colecoes Postman, paginas HTML) prontos para distribuicao.",
      "fileName": "generate-api-documentation.md",
      "filePath": ".claude/commands/generate-api-documentation.md",
      "fileContent": "# Automated API Documentation Generator\n\nAuto-generate API reference documentation: $ARGUMENTS\n\n## Current API Infrastructure\n\n- Code annotations: !`grep -r \"@api\\|@swagger\\|@doc\" src/ 2>/dev/null | wc -l` annotations found\n- API framework: @package.json or detect from imports\n- Existing specs: !`find . -name \"*spec*.yaml\" -o -name \"*spec*.json\" | head -3`\n- Documentation tools: !`grep -E \"swagger|redoc|postman\" package.json 2>/dev/null || echo \"None detected\"`\n- CI/CD pipeline: @.github/workflows/ (if exists)\n\n## Task\n\nSetup automated API documentation generation with modern tooling:\n\n1. **API Documentation Strategy Analysis**\n   - Analyze current API structure and endpoints\n   - Identify documentation requirements (REST, GraphQL, gRPC, etc.)\n   - Assess existing code annotations and documentation\n   - Determine documentation output formats and hosting requirements\n   - Plan documentation automation and maintenance strategy\n\n2. **Documentation Tool Selection**\n   - Choose appropriate API documentation tools:\n     - **OpenAPI/Swagger**: REST API documentation with Swagger UI\n     - **Redoc**: Modern OpenAPI documentation renderer\n     - **GraphQL**: GraphiQL, Apollo Studio, GraphQL Playground\n     - **Postman**: API documentation with collections\n     - **Insomnia**: API documentation and testing\n     - **API Blueprint**: Markdown-based API documentation\n     - **JSDoc/TSDoc**: Code-first documentation generation\n   - Consider factors: API type, team workflow, hosting, interactivity\n\n3. **Code Annotation and Schema Definition**\n   - Add comprehensive code annotations for API endpoints\n   - Define request/response schemas and data models\n   - Add parameter descriptions and validation rules\n   - Document authentication and authorization requirements\n   - Add example requests and responses\n\n4. **API Specification Generation**\n   - Set up automated API specification generation from code\n   - Configure OpenAPI/Swagger specification generation\n   - Set up schema validation and consistency checking\n   - Configure API versioning and changelog generation\n   - Set up specification file management and version control\n\n5. **Interactive Documentation Setup**\n   - Configure interactive API documentation with try-it-out functionality\n   - Set up API testing and example execution\n   - Configure authentication handling in documentation\n   - Set up request/response validation and examples\n   - Configure API endpoint categorization and organization\n\n6. **Documentation Content Enhancement**\n   - Add comprehensive API guides and tutorials\n   - Create authentication and authorization documentation\n   - Add error handling and status code documentation\n   - Create SDK and client library documentation\n   - Add rate limiting and usage guidelines\n\n7. **Documentation Hosting and Deployment**\n   - Set up documentation hosting and deployment\n   - Configure documentation website generation and styling\n   - Set up custom domain and SSL configuration\n   - Configure documentation search and navigation\n   - Set up documentation analytics and usage tracking\n\n8. **Automation and CI/CD Integration**\n   - Configure automated documentation generation in CI/CD pipeline\n   - Set up documentation deployment automation\n   - Configure documentation validation and quality checks\n   - Set up documentation change detection and notifications\n   - Configure documentation testing and link validation\n\n9. **Multi-format Documentation Generation**\n   - Generate documentation in multiple formats (HTML, PDF, Markdown)\n   - Set up downloadable documentation packages\n   - Configure offline documentation access\n   - Set up documentation API for programmatic access\n   - Configure documentation syndication and distribution\n\n10. **Maintenance and Quality Assurance**\n    - Set up documentation quality monitoring and validation\n    - Configure documentation feedback and improvement workflows\n    - Set up documentation analytics and usage metrics\n    - Create documentation maintenance procedures and guidelines\n    - Train team on documentation best practices and tools\n    - Set up documentation review and approval processes",
      "tags": ["documentation", "api"]
    },
    {
      "command": "/generate-tests",
      "label": "`/generate-tests`",
      "category": "Qualidade e Testes",
      "exemplos": [
        "`/generate-tests frontend/dashboard/src/services/documentationService.ts`",
        "`/generate-tests tools/rag-services/src/server.ts`"
      ],
      "capacidades": "Guia geracao de suites unitarias/integradas cobrindo mocks e edge cases.",
      "momentoIdeal": "Quando um modulo critico esta sem cobertura (ex.: RagProxyService) e precisa de plano detalhado.",
      "exemploMomento": "Criar suite de testes para `server.ts` no `rag-services`, definindo cenarios de timeout e fallback.",
      "tipoSaida": "Plano de testes em texto com lista de casos, estrategias de mocking e scripts recomendados.",
      "fileName": "generate-tests.md",
      "filePath": ".claude/commands/generate-tests.md",
      "fileContent": "# Generate Tests\n\nGenerate comprehensive test suite for: $ARGUMENTS\n\n## Current Testing Setup\n\n- Test framework: @package.json or @jest.config.js or @vitest.config.js (detect framework)\n- Existing tests: !`find . -name \"*.test.*\" -o -name \"*.spec.*\" | head -5`\n- Test coverage: !`npm run test:coverage 2>/dev/null || echo \"No coverage script\"`\n- Target file: @$ARGUMENTS (if file path provided)\n\n## Task\n\nI'll analyze the target code and create complete test coverage including:\n\n1. Unit tests for individual functions and methods\n2. Integration tests for component interactions\n3. Edge case and error handling tests\n4. Mock implementations for external dependencies\n5. Test utilities and helpers as needed\n6. Performance and snapshot tests where appropriate\n\n## Process\n\nI'll follow these steps:\n\n1. Analyze the target file/component structure\n2. Identify all testable functions, methods, and behaviors\n3. Examine existing test patterns in the project\n4. Create test files following project naming conventions\n5. Implement comprehensive test cases with proper setup/teardown\n6. Add necessary mocks and test utilities\n7. Verify test coverage and add missing test cases\n\n## Test Types\n\n### Unit Tests\n\n- Individual function testing with various inputs\n- Component rendering and prop handling\n- State management and lifecycle methods\n- Utility function edge cases and error conditions\n\n### Integration Tests\n\n- Component interaction testing\n- API integration with mocked responses\n- Service layer integration\n- End-to-end user workflows\n\n### Framework-Specific Tests\n\n- **React**: Component testing with React Testing Library\n- **Vue**: Component testing with Vue Test Utils\n- **Angular**: Component and service testing with TestBed\n- **Node.js**: API endpoint and middleware testing\n\n## Testing Best Practices\n\n### Test Structure\n\n- Use descriptive test names that explain the behavior\n- Follow AAA pattern (Arrange, Act, Assert)\n- Group related tests with describe blocks\n- Use proper setup and teardown for test isolation\n\n### Mock Strategy\n\n- Mock external dependencies and API calls\n- Use factories for test data generation\n- Implement proper cleanup for async operations\n- Mock timers and dates for deterministic tests\n\n### Coverage Goals\n\n- Aim for 80%+ code coverage\n- Focus on critical business logic paths\n- Test both happy path and error scenarios\n- Include boundary value testing\n\nI'll adapt to your project's testing framework (Jest, Vitest, Cypress, etc.) and follow established patterns.\n",
      "tags": ["testing", "coverage"]
    },
    {
      "command": "/implement-caching-strategy",
      "label": "`/implement-caching-strategy`",
      "category": "Observabilidade e Performance",
      "exemplos": [
        "`/implement-caching-strategy --browser`",
        "`/implement-caching-strategy --application`",
        "`/implement-caching-strategy --database`"
      ],
      "capacidades": "Desenha estrategia de cache em varias camadas (browser, aplicacao, banco) com politicas de invalidacao.",
      "momentoIdeal": "Quando consultas repetitivas (ex.: busca hibrida) pressionam a API e exigem plano de cache consistente.",
      "exemploMomento": "Depois que o dashboard passou a ter busca hibrida pesada, planejando cache distribuido com Redis.",
      "tipoSaida": "Documento com diagrama de camadas de cache, configuracoes recomendadas e plano de invalidacao.",
      "fileName": "implement-caching-strategy.md",
      "filePath": ".claude/commands/implement-caching-strategy.md",
      "fileContent": "# Implement Caching Strategy\n\nDesign and implement caching solutions: **$ARGUMENTS**\n\n## Instructions\n\n1. **Caching Strategy Analysis**\n   - Analyze application architecture and identify caching opportunities\n   - Assess current performance bottlenecks and data access patterns\n   - Define caching requirements (TTL, invalidation, consistency)\n   - Plan multi-layer caching architecture (browser, CDN, application, database)\n   - Evaluate caching technologies and storage solutions\n\n2. **Browser and Client-Side Caching**\n   - Configure HTTP caching headers and cache policies for static assets\n   - Implement service worker caching strategies for progressive web apps\n   - Set up browser storage caching (localStorage, sessionStorage, IndexedDB)\n   - Configure CDN caching rules and edge optimization\n   - Implement cache-first, network-first, and stale-while-revalidate strategies\n\n3. **Application-Level Caching**\n   - Implement in-memory caching for frequently accessed data\n   - Set up distributed caching with Redis or Memcached\n   - Design cache key naming conventions and namespacing\n   - Implement cache warming strategies for critical data\n   - Configure cache expiration and TTL policies\n\n4. **Database Query Caching**\n   - Implement query result caching for expensive database operations\n   - Set up prepared statement caching and connection pooling\n   - Design cache invalidation strategies for data consistency\n   - Implement materialized views for complex aggregations\n   - Configure database-level caching features and optimizations\n\n5. **API Response Caching**\n   - Implement API endpoint response caching with appropriate headers\n   - Set up middleware for automatic response caching\n   - Configure GraphQL query caching and field-level optimization\n   - Implement conditional requests with ETag and Last-Modified headers\n   - Design cache invalidation for API data updates\n\n6. **Cache Invalidation Strategies**\n   - Design intelligent cache invalidation based on data dependencies\n   - Implement event-driven cache invalidation systems\n   - Set up cache tagging and bulk invalidation mechanisms\n   - Configure time-based and trigger-based invalidation policies\n   - Implement cache versioning and rollback strategies\n\n7. **Frontend Caching Strategies**\n   - Implement client-side data caching with libraries like React Query\n   - Set up component-level caching and memoization\n   - Configure asset bundling and chunk caching strategies\n   - Implement progressive image loading and caching\n   - Set up offline-first caching for PWAs\n\n8. **Cache Monitoring and Analytics**\n   - Set up cache performance monitoring and metrics collection\n   - Track cache hit rates, miss rates, and efficiency metrics\n   - Monitor cache memory usage and storage optimization\n   - Implement cache performance alerting and notifications\n   - Analyze cache usage patterns and optimization opportunities\n\n9. **Cache Warming and Preloading**\n   - Implement automated cache warming for critical data\n   - Set up scheduled cache refresh and preloading strategies\n   - Design on-demand cache generation for popular content\n   - Configure cache warming triggers based on usage patterns\n   - Implement predictive caching based on user behavior\n\n10. **Testing and Validation**\n    - Set up cache performance testing and benchmarking\n    - Implement cache consistency validation and testing\n    - Configure cache invalidation testing scenarios\n    - Test cache behavior under high load and failure conditions\n    - Validate cache security and data isolation requirements\n\nFocus on implementing caching strategies that provide the most significant performance improvements while maintaining data consistency and system reliability. Always measure cache effectiveness and adjust strategies based on real-world usage patterns.",
      "tags": ["performance", "caching"]
    },
    {
      "command": "/init-project",
      "label": "`/init-project`",
      "category": "Setup e Padroes",
      "exemplos": ["`/init-project api --react`", "`/init-project cli --node`"],
      "capacidades": "Inicializa projetos ou microservicos com estrutura, lint, testes, CI e docs padronizados.",
      "momentoIdeal": "Ao criar novo submodulo (ex.: ferramenta auxiliar em `tools/`) mantendo consistencia do monorepo.",
      "exemploMomento": "Preparar boilerplate para um microservico de conciliacao de trades antes de iniciar codigo.",
      "tipoSaida": "Estrutura de pastas e arquivos base gerados, alem de resumo textual das configuracoes aplicadas.",
      "fileName": "init-project.md",
      "filePath": ".claude/commands/init-project.md",
      "fileContent": "# Initialize New Project\n\nInitialize new project with essential structure: **$ARGUMENTS**\n\n## Instructions\n\n1. **Project Analysis and Setup**\n   - Parse the project type and framework from arguments: `$ARGUMENTS`\n   - If no arguments provided, analyze current directory and ask user for project type and framework\n   - Create project directory structure if needed\n   - Validate that the chosen framework is appropriate for the project type\n\n2. **Base Project Structure**\n   - Create essential directories (src/, tests/, docs/, etc.)\n   - Initialize git repository with proper .gitignore for the project type\n   - Create README.md with project description and setup instructions\n   - Set up proper file structure based on project type and framework\n\n3. **Framework-Specific Configuration**\n   - **Web/React**: Set up React with TypeScript, Vite/Next.js, ESLint, Prettier\n   - **Web/Vue**: Configure Vue 3 with TypeScript, Vite, ESLint, Prettier\n   - **Web/Angular**: Set up Angular CLI project with TypeScript and testing\n   - **API/Express**: Create Express.js server with TypeScript, middleware, and routing\n   - **API/FastAPI**: Set up FastAPI with Python, Pydantic models, and async support\n   - **Mobile/React Native**: Configure React Native with navigation and development tools\n   - **Desktop/Electron**: Set up Electron with renderer and main process structure\n   - **CLI/Node**: Create Node.js CLI with commander.js and proper packaging\n   - **Library/NPM**: Set up library with TypeScript, rollup/webpack, and publishing config\n\n4. **Development Environment Setup**\n   - Configure package manager (npm, yarn, pnpm) with proper package.json\n   - Set up TypeScript configuration with strict mode and path mapping\n   - Configure linting with ESLint and language-specific rules\n   - Set up code formatting with Prettier and pre-commit hooks\n   - Add EditorConfig for consistent coding standards\n\n5. **Testing Infrastructure**\n   - Install and configure testing framework (Jest, Vitest, Pytest, etc.)\n   - Set up test directory structure and example tests\n   - Configure code coverage reporting\n   - Add testing scripts to package.json/makefile\n\n6. **Build and Development Tools**\n   - Configure build system (Vite, webpack, rollup, etc.)\n   - Set up development server with hot reloading\n   - Configure environment variable management\n   - Add build optimization and bundling\n\n7. **CI/CD Pipeline**\n   - Create GitHub Actions workflow for testing and deployment\n   - Set up automated testing on pull requests\n   - Configure automated dependency updates with Dependabot\n   - Add status badges to README\n\n8. **Documentation and Quality**\n   - Generate comprehensive README with installation and usage instructions\n   - Create CONTRIBUTING.md with development guidelines\n   - Set up API documentation generation (JSDoc, Sphinx, etc.)\n   - Add code quality badges and shields\n\n9. **Security and Best Practices**\n   - Configure security scanning with npm audit or similar\n   - Set up dependency vulnerability checking\n   - Add security headers for web applications\n   - Configure environment-specific security settings\n\n10. **Project Validation**\n    - Verify all dependencies install correctly\n    - Run initial build to ensure configuration is working\n    - Execute test suite to validate testing setup\n    - Check linting and formatting rules are applied\n    - Validate that development server starts successfully\n    - Create initial commit with proper project structure",
      "tags": ["setup", "scaffolding"]
    },
    {
      "command": "/lint",
      "label": "`/lint`",
      "category": "Qualidade e Testes",
      "exemplos": [
        "`/lint`",
        "`/lint --fix`",
        "`/lint backend`",
        "`/lint --file frontend/dashboard/src/App.tsx`",
        "`/lint all`"
      ],
      "capacidades": "Executa ESLint por alvo com opcao de auto-fix.",
      "momentoIdeal": "Pos edicao de componentes React ou servicos Node para manter padrao de codigo.",
      "exemploMomento": "Depois de ajustar `CollectionFilesTable.tsx`, garantindo que o componente siga regras do lint.",
      "tipoSaida": "Log de console mostrando erros/avisos de lint ou confirmando correcao automatica.",
      "fileName": "lint.md",
      "filePath": ".claude/commands/lint.md",
      "fileContent": "# Lint Command\n\nExecute ESLint para verificar qualidade de código JavaScript/TypeScript.\n\n## Usage\n\n```bash\n/lint [target] [options]\n```\n\n## Targets\n\n- `frontend` - Lint frontend/dashboard (default)\n- `backend` - Lint all backend APIs\n- `workspace` - Lint workspace API\n- `all` - Lint frontend + backend\n\n## Options\n\n- `--fix` - Auto-fix linting issues\n- `--file <path>` - Lint specific file\n\n## Examples\n\n```bash\n# Lint frontend\n/lint\n\n# Lint with auto-fix\n/lint --fix\n\n# Lint backend\n/lint backend\n\n# Lint specific file\n/lint --file src/components/pages/DocsHybridSearchPage.tsx\n\n# Lint everything\n/lint all --fix\n```\n\n## Implementation\n\n```bash\n# Frontend\nif [[ \"{{target}}\" == \"frontend\" ]] || [[ \"{{target}}\" == \"\" ]]; then\n  cd frontend/dashboard\n  if [[ \"{{args}}\" == *\"--fix\"* ]]; then\n    npm run lint:fix\n  else\n    npm run lint\n  fi\n  cd ../..\nfi\n\n# Backend\nif [[ \"{{target}}\" == \"backend\" ]] || [[ \"{{target}}\" == \"all\" ]]; then\n  for api in backend/api/*/; do\n    cd \"$api\"\n    if [[ -f \"package.json\" ]] && grep -q '\"lint\"' package.json; then\n      if [[ \"{{args}}\" == *\"--fix\"* ]]; then\n        npm run lint:fix\n      else\n        npm run lint\n      fi\n    fi\n    cd ../../..\n  done\nfi\n\n# Specific file\nif [[ \"{{args}}\" == *\"--file\"* ]]; then\n  file_path=$(echo \"{{args}}\" | grep -oP '(?<=--file )\\S+')\n  npx eslint \"$file_path\" $(echo \"{{args}}\" | grep -q -- \"--fix\" && echo \"--fix\")\nfi\n```\n\n## Common Issues Fixed\n\n- `no-unused-vars` - Remove unused variables\n- `no-console` - Remove console.log statements\n- `eqeqeq` - Replace `==` with `===`\n- `semi` - Add missing semicolons\n- `quotes` - Standardize quote style\n\n## Related Commands\n\n- `/quality-check` - Full quality check\n- `/format` - Prettier formatting\n- `/type-check` - TypeScript verification\n",
      "tags": ["quality", "lint"]
    },
    {
      "command": "/optimize-api-performance",
      "label": "`/optimize-api-performance`",
      "category": "Observabilidade e Performance",
      "exemplos": [
        "`/optimize-api-performance --rest`",
        "`/optimize-api-performance --graphql`",
        "`/optimize-api-performance --grpc`"
      ],
      "capacidades": "Analisa latencia de APIs, caching, gateway, conexoes e throughput.",
      "momentoIdeal": "Quando relatorios apontam lentidao nas rotas do documentation API ou tp-capital.",
      "exemploMomento": "Apos detectar picos de 95p > 1s na rota `/api/rag/search`, buscando otimizar a cadeia de chamadas.",
      "tipoSaida": "Relatorio de diagnostico com metricas de resposta, backlog de melhorias e scripts de teste de carga sugeridos.",
      "fileName": "optimize-api-performance.md",
      "filePath": ".claude/commands/optimize-api-performance.md",
      "fileContent": "# Optimize API Performance\n\nAnalyze and optimize API performance for faster response times, higher throughput, and better scalability: **$ARGUMENTS**\n\n## Instructions\n\n1. **API Performance Analysis**\n   - Analyze current API response times and throughput metrics\n   - Identify slowest endpoints and bottleneck patterns\n   - Profile API request/response lifecycle and processing time\n   - Document baseline performance metrics across different load scenarios\n   - Map API dependency chains and external service calls\n\n2. **Request/Response Optimization**\n   - Optimize request parsing and validation logic\n   - Implement efficient response serialization and compression\n   - Minimize payload sizes through selective field inclusion\n   - Configure appropriate HTTP headers and caching directives\n   - Optimize request routing and middleware processing\n\n3. **Database Query Optimization**\n   - Identify and optimize slow database queries\n   - Implement query result caching strategies\n   - Add appropriate database indexes for API queries\n   - Optimize database connection pooling and management\n   - Implement query batching and aggregation where applicable\n\n4. **Caching Strategy Implementation**\n   - Implement multi-level caching (in-memory, Redis, CDN)\n   - Configure cache invalidation strategies\n   - Set up API response caching with appropriate TTL values\n   - Implement cache warming and preloading strategies\n   - Monitor cache hit ratios and effectiveness\n\n5. **Rate Limiting and Throttling**\n   - Implement intelligent rate limiting based on usage patterns\n   - Configure adaptive throttling for different user tiers\n   - Set up queue management for handling traffic spikes\n   - Implement circuit breaker patterns for external services\n   - Monitor and adjust rate limits based on performance metrics\n\n6. **Concurrency and Parallelization**\n   - Implement proper async/await patterns for I/O operations\n   - Optimize thread pool configuration and management\n   - Implement parallel processing for independent operations\n   - Configure connection pooling for optimal concurrency\n   - Use streaming for large data transfers\n\n7. **API Gateway and Load Balancing**\n   - Configure API gateway for optimal routing and load distribution\n   - Implement health checks and automatic failover\n   - Set up load balancing algorithms for even traffic distribution\n   - Configure request/response transformation at gateway level\n   - Implement API versioning and traffic splitting\n\n8. **Monitoring and Observability**\n   - Set up comprehensive API performance monitoring\n   - Implement distributed tracing for request lifecycle visibility\n   - Configure performance metrics collection and alerting\n   - Monitor API error rates and response time percentiles\n   - Set up real-time performance dashboards\n\n9. **Security Performance Optimization**\n   - Optimize authentication and authorization processes\n   - Implement efficient JWT validation and caching\n   - Configure SSL/TLS termination for optimal performance\n   - Optimize API key validation and rate limiting\n   - Implement security middleware performance tuning\n\n10. **Content Delivery Optimization**\n    - Configure CDN for static API responses and assets\n    - Implement geographic load balancing and edge caching\n    - Optimize API endpoint geographical distribution\n    - Set up content compression and optimization\n    - Configure cache headers for optimal CDN performance\n\n11. **API Design Optimization**\n    - Review and optimize API endpoint design patterns\n    - Implement efficient pagination and filtering strategies\n    - Optimize API versioning and backward compatibility\n    - Design APIs for optimal client-side caching\n    - Implement GraphQL query optimization (if applicable)\n\n12. **Load Testing and Performance Validation**\n    - Implement comprehensive load testing scenarios\n    - Configure performance regression testing in CI/CD\n    - Set up chaos engineering tests for resilience validation\n    - Monitor API performance under various load conditions\n    - Validate performance optimizations with realistic test data\n\n13. **Scalability Planning**\n    - Design API architecture for horizontal scaling\n    - Implement auto-scaling policies based on performance metrics\n    - Configure database scaling strategies (read replicas, sharding)\n    - Plan for traffic growth and capacity requirements\n    - Implement graceful degradation strategies\n\n14. **Third-Party Service Optimization**\n    - Optimize external API calls and integrations\n    - Implement retry policies and exponential backoff\n    - Configure timeout settings for external services\n    - Set up fallback mechanisms for service unavailability\n    - Monitor third-party service performance impact\n\n15. **Performance Testing Automation**\n    - Set up automated performance testing pipelines\n    - Configure performance benchmarking and comparison\n    - Implement performance regression detection\n    - Set up load testing in staging environments\n    - Create performance test data management strategies\n\nFocus on optimizations that provide the highest impact on response times and throughput. Prioritize changes that improve user experience and system scalability while maintaining reliability.",
      "tags": ["performance", "api"]
    },
    {
      "command": "/optimize-build",
      "label": "`/optimize-build`",
      "category": "Entrega e DevOps",
      "exemplos": [
        "`/optimize-build frontend --analyze`",
        "`/optimize-build backend --profile`"
      ],
      "capacidades": "Analisa pipeline de build, caching, paralelismo e bundle composition.",
      "momentoIdeal": "Para reduzir tempos de build do dashboard no CI ou melhorar incremental builds locais.",
      "exemploMomento": "Ao notar que a pipeline do GitHub Actions passou de 10 minutos, buscando cortes.",
      "tipoSaida": "Relatorio com metricas antes/depois, ajustes sugeridos e comandos de configuracao.",
      "fileName": "optimize-build.md",
      "filePath": ".claude/commands/optimize-build.md",
      "fileContent": "# Optimize Build Command\n\nOptimize build processes and speed\n\n## Instructions\n\nFollow this systematic approach to optimize build performance: **$ARGUMENTS**\n\n1. **Build System Analysis**\n   - Identify the build system in use (Webpack, Vite, Rollup, Gradle, Maven, Cargo, etc.)\n   - Review build configuration files and settings\n   - Analyze current build times and output sizes\n   - Map the complete build pipeline and dependencies\n\n2. **Performance Baseline**\n   - Measure current build times for different scenarios:\n     - Clean build (from scratch)\n     - Incremental build (with cache)\n     - Development vs production builds\n   - Document bundle sizes and asset sizes\n   - Identify the slowest parts of the build process\n\n3. **Dependency Optimization**\n   - Analyze build dependencies and their impact\n   - Remove unused dependencies from build process\n   - Update build tools to latest stable versions\n   - Consider alternative, faster build tools\n\n4. **Caching Strategy**\n   - Enable and optimize build caching\n   - Configure persistent cache for CI/CD\n   - Set up shared cache for team development\n   - Implement incremental compilation where possible\n\n5. **Bundle Analysis**\n   - Analyze bundle composition and sizes\n   - Identify large dependencies and duplicates\n   - Use bundle analyzers specific to your build tool\n   - Look for opportunities to split bundles\n\n6. **Code Splitting and Lazy Loading**\n   - Implement dynamic imports and code splitting\n   - Set up route-based splitting for SPAs\n   - Configure vendor chunk separation\n   - Optimize chunk sizes and loading strategies\n\n7. **Asset Optimization**\n   - Optimize images (compression, format conversion, lazy loading)\n   - Minify CSS and JavaScript\n   - Configure tree shaking to remove dead code\n   - Implement asset compression (gzip, brotli)\n\n8. **Development Build Optimization**\n   - Enable fast refresh/hot reloading\n   - Use development-specific optimizations\n   - Configure source maps for better debugging\n   - Optimize development server settings\n\n9. **Production Build Optimization**\n   - Enable all production optimizations\n   - Configure dead code elimination\n   - Set up proper minification and compression\n   - Optimize for smaller bundle sizes\n\n10. **Parallel Processing**\n    - Enable parallel processing where supported\n    - Configure worker threads for build tasks\n    - Optimize for multi-core systems\n    - Use parallel compilation for TypeScript/Babel\n\n11. **File System Optimization**\n    - Optimize file watching and polling\n    - Configure proper include/exclude patterns\n    - Use efficient file loaders and processors\n    - Minimize file I/O operations\n\n12. **CI/CD Build Optimization**\n    - Optimize CI build environments and resources\n    - Implement proper caching strategies for CI\n    - Use build matrices efficiently\n    - Configure parallel CI jobs where beneficial\n\n13. **Memory Usage Optimization**\n    - Monitor and optimize memory usage during builds\n    - Configure heap sizes for build tools\n    - Identify and fix memory leaks in build process\n    - Use memory-efficient compilation options\n\n14. **Output Optimization**\n    - Configure compression and encoding\n    - Optimize file naming and hashing strategies\n    - Set up proper asset manifests\n    - Implement efficient asset serving\n\n15. **Monitoring and Profiling**\n    - Set up build time monitoring\n    - Use build profiling tools to identify bottlenecks\n    - Track bundle size changes over time\n    - Monitor build performance regressions\n\n16. **Tool-Specific Optimizations**\n    \n    **For Webpack:**\n    - Configure optimization.splitChunks\n    - Use thread-loader for parallel processing\n    - Enable optimization.usedExports for tree shaking\n    - Configure resolve.modules and resolve.extensions\n\n    **For Vite:**\n    - Configure build.rollupOptions\n    - Use esbuild for faster transpilation\n    - Optimize dependency pre-bundling\n    - Configure build.chunkSizeWarningLimit\n\n    **For TypeScript:**\n    - Use incremental compilation\n    - Configure project references\n    - Optimize tsconfig.json settings\n    - Use skipLibCheck when appropriate\n\n17. **Environment-Specific Configuration**\n    - Separate development and production configurations\n    - Use environment variables for build optimization\n    - Configure feature flags for conditional builds\n    - Optimize for target environments\n\n18. **Testing Build Optimizations**\n    - Test build outputs for correctness\n    - Verify all optimizations work in target environments\n    - Check for any breaking changes from optimizations\n    - Measure and document performance improvements\n\n19. **Documentation and Maintenance**\n    - Document all optimization changes and their impact\n    - Create build performance monitoring dashboard\n    - Set up alerts for build performance regressions\n    - Regular review and updates of build configuration\n\nFocus on the optimizations that provide the biggest impact for your specific project and team workflow. Always measure before and after to quantify improvements.",
      "tags": ["performance", "build"]
    },
    {
      "command": "/optimize-bundle-size",
      "label": "`/optimize-bundle-size`",
      "category": "Observabilidade e Performance",
      "exemplos": [
        "`/optimize-bundle-size --webpack`",
        "`/optimize-bundle-size --vite`",
        "`/optimize-bundle-size --rollup`"
      ],
      "capacidades": "Reduz tamanho de bundles com splitting, tree shaking e analise.",
      "momentoIdeal": "Quando a tela de dashboard excede orcamento de bundle e impacta TTFB/LCP.",
      "exemploMomento": "Antes de publicar release que adiciona novo modulo React pesado, avaliando impacto de bundle.",
      "tipoSaida": "Comparativo de tamanhos antes/depois, lista de dependencias pesadas e comandos para gerar visualizacoes.",
      "fileName": "optimize-bundle-size.md",
      "filePath": ".claude/commands/optimize-bundle-size.md",
      "fileContent": "# Optimize Bundle Size\n\nReduce and optimize bundle sizes: **$ARGUMENTS**\n\n## Instructions\n\n1. **Bundle Analysis and Assessment**\n   - Analyze current bundle size and composition using webpack-bundle-analyzer or similar tools\n   - Identify large dependencies and unused code across all bundles\n   - Assess current build configuration and optimization settings\n   - Create baseline measurements for optimization tracking\n   - Document current performance metrics and loading times\n\n2. **Build Tool Configuration**\n   - Configure build tool optimization settings for production builds\n   - Enable code splitting and chunk optimization features\n   - Configure tree shaking and dead code elimination\n   - Set up bundle analyzers and visualization tools\n   - Optimize build performance and output sizes\n\n3. **Code Splitting and Lazy Loading**\n   - Implement route-based code splitting for single-page applications\n   - Set up dynamic imports for components and modules\n   - Configure lazy loading for non-critical resources\n   - Optimize chunk sizes and loading strategies\n   - Implement progressive loading patterns\n\n4. **Tree Shaking and Dead Code Elimination**\n   - Configure build tools for optimal tree shaking\n   - Mark packages as side-effect free where appropriate\n   - Optimize import statements for better tree shaking\n   - Use ES6 modules and avoid CommonJS where possible\n   - Implement babel plugins for automatic import optimization\n\n5. **Dependency Optimization**\n   - Analyze and audit package dependencies for size impact\n   - Replace large libraries with smaller alternatives\n   - Use specific imports instead of importing entire libraries\n   - Implement dependency deduplication strategies\n   - Configure external dependencies and CDN usage\n\n6. **Asset Optimization**\n   - Optimize images through compression and format conversion\n   - Implement responsive image loading strategies\n   - Configure asset minification and compression\n   - Set up efficient file loaders and processors\n   - Optimize font loading and subsetting\n\n7. **Module Federation and Micro-frontends**\n   - Implement module federation for large applications\n   - Configure shared dependencies and runtime optimization\n   - Set up micro-frontend architecture for code sharing\n   - Optimize remote module loading and caching\n   - Implement federation performance monitoring\n\n8. **Performance Monitoring and Measurement**\n   - Set up bundle size monitoring and tracking\n   - Configure automated bundle analysis in CI/CD\n   - Monitor bundle size changes over time\n   - Set up performance budgets and alerts\n   - Track loading performance metrics\n\n9. **Progressive Loading Strategies**\n   - Implement resource hints (preload, prefetch, dns-prefetch)\n   - Configure service workers for caching strategies\n   - Set up intersection observer for lazy loading\n   - Optimize critical resource loading priorities\n   - Implement adaptive loading based on connection speed\n\n10. **Validation and Continuous Monitoring**\n    - Set up automated bundle size validation in CI/CD\n    - Configure bundle size thresholds and alerts\n    - Implement bundle size regression testing\n    - Monitor real-world loading performance\n    - Set up automated optimization recommendations\n\nFocus on optimizations that provide the most significant bundle size reductions while maintaining application functionality. Always measure the impact of changes on both bundle size and runtime performance.",
      "tags": ["performance", "frontend"]
    },
    {
      "command": "/optimize-database-performance",
      "label": "`/optimize-database-performance`",
      "category": "Observabilidade e Performance",
      "exemplos": [
        "`/optimize-database-performance --postgresql`",
        "`/optimize-database-performance --mysql`",
        "`/optimize-database-performance --mongodb`"
      ],
      "capacidades": "Revisa queries, indices, particionamento e replicas.",
      "momentoIdeal": "Ao identificar consultas lentas no Timescale/QuestDB durante ingestao em tempo real.",
      "exemploMomento": "Diagnosticar por que a query que alimenta `STATUS-FINAL-LOGS` passou a levar mais de 2s.",
      "tipoSaida": "Plano de otimizacao com queries analisadas, indices propostos e metricas monitoradas.",
      "fileName": "optimize-database-performance.md",
      "filePath": ".claude/commands/optimize-database-performance.md",
      "fileContent": "# Optimize Database Performance\n\nOptimize database queries and performance: **$ARGUMENTS**\n\n## Instructions\n\n1. **Database Performance Analysis**\n   - Analyze current database performance and identify bottlenecks\n   - Review slow query logs and execution plans\n   - Assess database schema design and normalization\n   - Evaluate indexing strategy and query patterns\n   - Monitor database resource utilization (CPU, memory, I/O)\n\n2. **Query Optimization**\n   - Identify and optimize slow-performing queries\n   - Analyze query execution plans and optimization strategies\n   - Rewrite queries for better performance and efficiency\n   - Implement query hints and optimization directives\n   - Configure query timeout and resource limits\n\n3. **Index Strategy Optimization**\n   - Analyze existing indexes and their usage patterns\n   - Design optimal indexing strategy for query patterns\n   - Create composite indexes for multi-column queries\n   - Implement covering indexes to avoid table lookups\n   - Remove unused and redundant indexes\n\n4. **Schema Design Optimization**\n   - Optimize table structure and data types\n   - Implement denormalization strategies for read-heavy workloads\n   - Design partitioning strategies for large tables\n   - Create materialized views for complex aggregations\n   - Optimize foreign key relationships and constraints\n\n5. **Connection Pool Optimization**\n   - Configure optimal database connection pooling settings\n   - Tune connection pool size and timeout settings\n   - Implement connection monitoring and health checks\n   - Optimize connection lifecycle and cleanup procedures\n   - Configure connection security and SSL settings\n\n6. **Query Result Caching**\n   - Implement intelligent database result caching\n   - Design cache invalidation strategies for data consistency\n   - Set up query-level and result-set caching\n   - Configure cache expiration and refresh policies\n   - Monitor cache effectiveness and hit rates\n\n7. **Database Monitoring and Profiling**\n   - Set up comprehensive database performance monitoring\n   - Monitor query performance and resource usage\n   - Track database connections and session activity\n   - Implement alerting for performance degradation\n   - Configure automated performance reporting\n\n8. **Read Replica and Load Balancing**\n   - Configure read replicas for query distribution\n   - Implement intelligent read/write query routing\n   - Set up load balancing across database instances\n   - Monitor replication lag and consistency\n   - Configure failover and disaster recovery procedures\n\n9. **Database Vacuum and Maintenance**\n   - Implement automated database maintenance procedures\n   - Configure vacuum and analyze operations for optimal performance\n   - Set up index rebuilding and maintenance schedules\n   - Monitor table bloat and fragmentation\n   - Implement automated cleanup and archival strategies\n\n10. **Performance Testing and Benchmarking**\n    - Set up database performance testing frameworks\n    - Implement load testing scenarios for realistic workloads\n    - Benchmark query performance under different conditions\n    - Test database scalability and capacity limits\n    - Monitor performance regression and improvements\n\nFocus on database optimizations that provide the most significant performance improvements for your specific workload patterns. Always measure performance before and after changes to validate optimizations.",
      "tags": ["performance", "database"]
    },
    {
      "command": "/optimize-memory-usage",
      "label": "`/optimize-memory-usage`",
      "category": "Observabilidade e Performance",
      "exemplos": [
        "`/optimize-memory-usage --frontend`",
        "`/optimize-memory-usage --backend`",
        "`/optimize-memory-usage --database`"
      ],
      "capacidades": "Identifica vazamentos, ajusta garbage collector e sugere padroes de reuso de objetos.",
      "momentoIdeal": "Quando servicos Node exibem aumento continuo de heap durante execucao prolongada de agentes ou workers.",
      "exemploMomento": "Investigar crescimento de memoria no `server.ts` depois que consultas RAG foram paralelizadas.",
      "tipoSaida": "Relatorio de perfil de memoria, hipoteses de leak e acoes corretivas recomendadas.",
      "fileName": "optimize-memory-usage.md",
      "filePath": ".claude/commands/optimize-memory-usage.md",
      "fileContent": "# Optimize Memory Usage\n\nAnalyze and optimize memory usage patterns to prevent leaks and improve application performance: **$ARGUMENTS**\n\n## Instructions\n\n1. **Memory Analysis and Profiling**\n   - Profile current memory usage patterns using appropriate tools (Chrome DevTools, Node.js --inspect, Valgrind)\n   - Identify memory leaks and excessive memory consumption hotspots\n   - Analyze garbage collection patterns and performance impact\n   - Create baseline measurements for optimization tracking\n   - Document memory allocation hotspots and growth patterns over time\n\n2. **Memory Leak Detection**\n   - Set up memory leak detection for different runtime environments\n   - Monitor heap snapshots and compare over time intervals\n   - Track DOM node leaks in browser applications\n   - Implement event listener cleanup and monitoring\n   - Use profiling tools to identify growing memory patterns\n\n3. **Garbage Collection Optimization**\n   - Configure garbage collection settings for your runtime environment\n   - Tune Node.js heap sizes and GC flags for optimal performance\n   - Monitor GC pause times and frequency\n   - Implement GC performance monitoring and alerting\n   - Optimize object lifecycles to reduce GC pressure\n\n4. **Memory Pool and Object Reuse**\n   - Implement object pooling for frequently allocated objects\n   - Create buffer pools for Node.js applications\n   - Reuse DOM elements and components in frontend applications\n   - Design memory-efficient data structures (circular buffers, sparse arrays)\n   - Pre-allocate objects to reduce runtime allocation overhead\n\n5. **String and Text Optimization**\n   - Implement string interning for frequently used strings\n   - Optimize string concatenation and manipulation operations\n   - Use efficient text processing algorithms\n   - Minimize string duplication across the application\n   - Consider string compression for large text data\n\n6. **Database Connection Optimization**\n   - Implement proper connection pooling with appropriate limits\n   - Configure connection timeouts and cleanup procedures\n   - Optimize query result caching and memory usage\n   - Monitor database connection memory overhead\n   - Implement connection leak detection and prevention\n\n7. **Frontend Memory Optimization**\n   - Optimize component lifecycle and cleanup\n   - Implement proper event listener cleanup\n   - Use lazy loading for images and components\n   - Minimize bundle size and code splitting\n   - Monitor and optimize browser memory usage patterns\n\n8. **Backend Memory Optimization**\n   - Optimize server request handling and cleanup\n   - Implement streaming for large data processing\n   - Configure appropriate memory limits and monitoring\n   - Optimize middleware and request lifecycle\n   - Use memory-efficient data processing patterns\n\n9. **Container and Deployment Optimization**\n   - Configure appropriate container memory limits\n   - Optimize Docker image layers for memory efficiency\n   - Monitor memory usage in production environments\n   - Implement memory-based auto-scaling policies\n   - Set up memory usage alerting and monitoring\n\n10. **Memory Monitoring and Alerting**\n    - Set up real-time memory monitoring dashboards\n    - Configure memory usage alerts and thresholds\n    - Implement memory leak detection in production\n    - Track memory performance metrics over time\n    - Create automated memory optimization testing\n\n11. **Production Memory Management**\n    - Implement graceful memory pressure handling\n    - Configure memory-based health checks\n    - Set up memory usage trending and analysis\n    - Implement emergency memory cleanup procedures\n    - Monitor and optimize memory usage patterns\n\nFocus on the specific memory optimization strategies that provide the biggest impact for your target environment. Always measure memory usage before and after optimizations to quantify improvements.",
      "tags": ["performance", "memory"]
    },
    {
      "command": "/performance-audit",
      "label": "`/performance-audit`",
      "category": "Qualidade e Testes",
      "exemplos": [
        "`/performance-audit`",
        "`/performance-audit --frontend`",
        "`/performance-audit --backend`",
        "`/performance-audit --full`"
      ],
      "capacidades": "Auditoria de performance end-to-end (bundles, APIs, DB, rede).",
      "momentoIdeal": "Em ciclos de melhoria como o relatorio `03-performance-audit-tp-capital.md` para priorizar otimizacoes.",
      "exemploMomento": "Antes de planejar sprint de performance, coletando dados de build e latencia atuais.",
      "tipoSaida": "Relatorio analitico com metricas coletadas, principais gargalos e backlog proposto de melhorias.",
      "fileName": "performance-audit.md",
      "filePath": ".claude/commands/performance-audit.md",
      "fileContent": "# Performance Audit\n\nConduct comprehensive performance audit: $ARGUMENTS\n\n## Current Performance Context\n\n- Bundle analysis: !`npm run build -- --analyze 2>/dev/null || echo \"No build analyzer\"`\n- Dependencies: !`npm list --depth=0 --prod 2>/dev/null | head -10`\n- Build time: !`time npm run build >/dev/null 2>&1 || echo \"No build script\"`\n- Performance config: @webpack.config.js or @vite.config.js or @next.config.js (if exists)\n\n## Task\n\nConduct comprehensive performance audit following these steps:\n\n1. **Technology Stack Analysis**\n   - Identify the primary language, framework, and runtime environment\n   - Review build tools and optimization configurations\n   - Check for performance monitoring tools already in place\n\n2. **Code Performance Analysis**\n   - Identify inefficient algorithms and data structures\n   - Look for nested loops and O(n²) operations\n   - Check for unnecessary computations and redundant operations\n   - Review memory allocation patterns and potential leaks\n\n3. **Database Performance**\n   - Analyze database queries for efficiency\n   - Check for missing indexes and slow queries\n   - Review connection pooling and database configuration\n   - Identify N+1 query problems and excessive database calls\n\n4. **Frontend Performance (if applicable)**\n   - Analyze bundle size and chunk optimization\n   - Check for unused code and dependencies\n   - Review image optimization and lazy loading\n   - Examine render performance and re-render cycles\n   - Check for memory leaks in UI components\n\n5. **Network Performance**\n   - Review API call patterns and caching strategies\n   - Check for unnecessary network requests\n   - Analyze payload sizes and compression\n   - Examine CDN usage and static asset optimization\n\n6. **Asynchronous Operations**\n   - Review async/await usage and promise handling\n   - Check for blocking operations and race conditions\n   - Analyze task queuing and background processing\n   - Identify opportunities for parallel execution\n\n7. **Memory Usage**\n   - Check for memory leaks and excessive memory consumption\n   - Review garbage collection patterns\n   - Analyze object lifecycle and cleanup\n   - Identify large objects and unnecessary data retention\n\n8. **Build & Deployment Performance**\n   - Analyze build times and optimization opportunities\n   - Review dependency bundling and tree shaking\n   - Check for development vs production optimizations\n   - Examine deployment pipeline efficiency\n\n9. **Performance Monitoring**\n   - Check existing performance metrics and monitoring\n   - Identify key performance indicators (KPIs) to track\n   - Review alerting and performance thresholds\n   - Suggest performance testing strategies\n\n10. **Benchmarking & Profiling**\n    - Run performance profiling tools appropriate for the stack\n    - Create benchmarks for critical code paths\n    - Measure before and after optimization impact\n    - Document performance baselines\n\n11. **Optimization Recommendations**\n    - Prioritize optimizations by impact and effort\n    - Provide specific code examples and alternatives\n    - Suggest architectural improvements for scalability\n    - Recommend appropriate performance tools and libraries\n\nInclude specific file paths, line numbers, and measurable metrics where possible. Focus on high-impact, low-effort optimizations first.",
      "tags": ["performance", "analysis"]
    },
    {
      "command": "/quality-check",
      "label": "`/quality-check`",
      "category": "Qualidade e Testes",
      "exemplos": [
        "`/quality-check`",
        "`/quality-check --fix`",
        "`/quality-check --full`",
        "`/quality-check --full --format html`",
        "`/quality-check --backend`"
      ],
      "capacidades": "Roda pipeline completo (lint, tipos, testes, audit, bundle, docker health).",
      "momentoIdeal": "Antes de abrir pull request ou iniciar deploy para garantir baseline de qualidade.",
      "exemploMomento": "Validar o dashboard apos ajustar `collectionManager.ts` antes de subir artefatos para staging.",
      "tipoSaida": "Sumario textual dos checks executados, indicando sucessos/falhas e apontando logs correspondentes.",
      "fileName": "quality-check.md",
      "filePath": ".claude/commands/quality-check.md",
      "fileContent": "# Quality Check Command\n\nExecute verificação completa de qualidade de código incluindo linting, type checking, testes, security audit e análise de bundle.\n\n## Usage\n\n```bash\n/quality-check [options]\n```\n\n## Options\n\n- `--fix` - Auto-fix linting and formatting issues\n- `--full` - Run full analysis (including slow checks like duplication, dead code)\n- `--frontend` - Check only frontend code\n- `--backend` - Check only backend code\n- `--format html` - Generate HTML report\n- `--format json` - Generate JSON report\n\n## Examples\n\n```bash\n# Basic quality check\n/quality-check\n\n# Auto-fix issues\n/quality-check --fix\n\n# Full analysis with HTML report\n/quality-check --full --format html\n\n# Frontend only\n/quality-check --frontend\n```\n\n## What It Checks\n\n1. **ESLint** - Code quality and style\n2. **TypeScript** - Type errors\n3. **Tests** - Unit tests with coverage\n4. **Security** - npm audit for vulnerabilities\n5. **Bundle Size** - Production build analysis (--full only)\n6. **Code Duplication** - jscpd analysis (--full only)\n7. **Dead Code** - Unused exports (--full only)\n8. **Docker Health** - Container status\n\n## Expected Output\n\n```\n==========================================\nCode Quality Check - TradingSystem\n==========================================\n\n[SUCCESS] ✅ ESLint passed (0 errors)\n[SUCCESS] ✅ TypeScript check passed (0 type errors)\n[SUCCESS] ✅ All tests passed\n[INFO] Coverage: 82.5%\n[SUCCESS] ✅ No high/critical vulnerabilities\n[SUCCESS] ✅ All containers healthy\n\n==========================================\nSummary\n==========================================\nTotal Checks: 7\nPassed: 7 ✅\nWarnings: 0 ⚠️\nFailed: 0 ❌\n```\n\n## Implementation\n\nExecute the automated quality check script:\n\n```bash\nbash scripts/maintenance/code-quality-check.sh {{args}}\n```\n\n## Related Commands\n\n- `/lint` - ESLint only\n- `/test` - Tests only\n- `/audit` - Security audit only\n- `/build` - Build verification\n\n## Documentation\n\n- [Code Quality Checklist](../../docs/content/development/code-quality-checklist.md)\n- [Quick Reference](../../CODE-QUALITY-COMMANDS.md)\n",
      "tags": ["quality", "ci"]
    },
    {
      "command": "/refactor-code",
      "label": "`/refactor-code`",
      "category": "Diagnostico e Seguranca",
      "exemplos": [
        "`/refactor-code tools/rag-services/src/services/collectionManager.ts`",
        "`/refactor-code apps/tp-capital/src/server.js`"
      ],
      "capacidades": "Guia refatoracoes seguras, com testes, passos incrementais e metricas.",
      "momentoIdeal": "Na limpeza de modulos legados (ex.: reorganizar `collectionManager.ts`) garantindo cobertura continua.",
      "exemploMomento": "Planejar refactor do `documentationService.ts` para reduzir duplicacao de chamadas REST.",
      "tipoSaida": "Plano de refatoracao estruturado com etapas, riscos, testes a rodar e criterios de conclusao.",
      "fileName": "refactor-code.md",
      "filePath": ".claude/commands/refactor-code.md",
      "fileContent": "# Intelligently Refactor and Improve Code Quality\n\nIntelligently refactor and improve code quality\n\n## Instructions\n\nFollow this systematic approach to refactor code: **$ARGUMENTS**\n\n1. **Pre-Refactoring Analysis**\n   - Identify the code that needs refactoring and the reasons why\n   - Understand the current functionality and behavior completely\n   - Review existing tests and documentation\n   - Identify all dependencies and usage points\n\n2. **Test Coverage Verification**\n   - Ensure comprehensive test coverage exists for the code being refactored\n   - If tests are missing, write them BEFORE starting refactoring\n   - Run all tests to establish a baseline\n   - Document current behavior with additional tests if needed\n\n3. **Refactoring Strategy**\n   - Define clear goals for the refactoring (performance, readability, maintainability)\n   - Choose appropriate refactoring techniques:\n     - Extract Method/Function\n     - Extract Class/Component\n     - Rename Variable/Method\n     - Move Method/Field\n     - Replace Conditional with Polymorphism\n     - Eliminate Dead Code\n   - Plan the refactoring in small, incremental steps\n\n4. **Environment Setup**\n   - Create a new branch: `git checkout -b refactor/$ARGUMENTS`\n   - Ensure all tests pass before starting\n   - Set up any additional tooling needed (profilers, analyzers)\n\n5. **Incremental Refactoring**\n   - Make small, focused changes one at a time\n   - Run tests after each change to ensure nothing breaks\n   - Commit working changes frequently with descriptive messages\n   - Use IDE refactoring tools when available for safety\n\n6. **Code Quality Improvements**\n   - Improve naming conventions for clarity\n   - Eliminate code duplication (DRY principle)\n   - Simplify complex conditional logic\n   - Reduce method/function length and complexity\n   - Improve separation of concerns\n\n7. **Performance Optimizations**\n   - Identify and eliminate performance bottlenecks\n   - Optimize algorithms and data structures\n   - Reduce unnecessary computations\n   - Improve memory usage patterns\n\n8. **Design Pattern Application**\n   - Apply appropriate design patterns where beneficial\n   - Improve abstraction and encapsulation\n   - Enhance modularity and reusability\n   - Reduce coupling between components\n\n9. **Error Handling Improvement**\n   - Standardize error handling approaches\n   - Improve error messages and logging\n   - Add proper exception handling\n   - Enhance resilience and fault tolerance\n\n10. **Documentation Updates**\n    - Update code comments to reflect changes\n    - Revise API documentation if interfaces changed\n    - Update inline documentation and examples\n    - Ensure comments are accurate and helpful\n\n11. **Testing Enhancements**\n    - Add tests for any new code paths created\n    - Improve existing test quality and coverage\n    - Remove or update obsolete tests\n    - Ensure tests are still meaningful and effective\n\n12. **Static Analysis**\n    - Run linting tools to catch style and potential issues\n    - Use static analysis tools to identify problems\n    - Check for security vulnerabilities\n    - Verify code complexity metrics\n\n13. **Performance Verification**\n    - Run performance benchmarks if applicable\n    - Compare before/after metrics\n    - Ensure refactoring didn't degrade performance\n    - Document any performance improvements\n\n14. **Integration Testing**\n    - Run full test suite to ensure no regressions\n    - Test integration with dependent systems\n    - Verify all functionality works as expected\n    - Test edge cases and error scenarios\n\n15. **Code Review Preparation**\n    - Review all changes for quality and consistency\n    - Ensure refactoring goals were achieved\n    - Prepare clear explanation of changes made\n    - Document benefits and rationale\n\n16. **Documentation of Changes**\n    - Create a summary of refactoring changes\n    - Document any breaking changes or new patterns\n    - Update project documentation if needed\n    - Explain benefits and reasoning for future reference\n\n17. **Deployment Considerations**\n    - Plan deployment strategy for refactored code\n    - Consider feature flags for gradual rollout\n    - Prepare rollback procedures\n    - Set up monitoring for the refactored components\n\nRemember: Refactoring should preserve external behavior while improving internal structure. Always prioritize safety over speed, and maintain comprehensive test coverage throughout the process.",
      "tags": ["quality", "refactoring"]
    },
    {
      "command": "/setup-cdn-optimization",
      "label": "`/setup-cdn-optimization`",
      "category": "Infraestrutura e Deploy",
      "exemplos": [
        "`/setup-cdn-optimization --cloudflare`",
        "`/setup-cdn-optimization --aws`",
        "`/setup-cdn-optimization --fastly`"
      ],
      "capacidades": "Configura CDN, cache, compressao e hints de carregamento.",
      "momentoIdeal": "Para melhorar entrega do frontend/documentacao em ambientes externos (ex.: portal docs).",
      "exemploMomento": "Preparar rollout publico da documentacao, otimizando assets para visitantes externos.",
      "tipoSaida": "Planilha/texto com configuracoes aplicadas (headers, TTLs) e checklist de validacoes de performance.",
      "fileName": "setup-cdn-optimization.md",
      "filePath": ".claude/commands/setup-cdn-optimization.md",
      "fileContent": "# Setup CDN Optimization\n\nConfigure CDN for optimal delivery: **$ARGUMENTS**\n\n## Instructions\n\n1. **CDN Strategy and Provider Selection**\n   - Analyze application traffic patterns and global user distribution\n   - Evaluate CDN providers based on performance, cost, and features\n   - Assess content types and specific caching requirements\n   - Plan CDN architecture and edge location strategy\n   - Define performance and cost optimization goals\n\n2. **CDN Configuration and Setup**\n   - Configure CDN with optimal settings for your content types\n   - Set up origin servers and failover configurations\n   - Configure SSL/TLS certificates and security settings\n   - Implement custom domain and DNS configuration\n   - Set up monitoring and analytics tracking\n\n3. **Static Asset Optimization**\n   - Optimize asset build process for CDN delivery\n   - Configure content hashing and versioning strategies\n   - Set up asset bundling and code splitting for CDN\n   - Implement responsive image delivery and optimization\n   - Configure font loading and optimization strategies\n\n4. **Compression and Optimization**\n   - Configure Gzip and Brotli compression settings\n   - Set up build-time compression for static assets\n   - Implement dynamic compression for API responses\n   - Configure minification and asset optimization\n   - Set up progressive image formats (WebP, AVIF)\n\n5. **Cache Headers and Policies**\n   - Design intelligent caching strategies for different content types\n   - Configure cache control headers and TTL values\n   - Implement ETags and conditional request handling\n   - Set up cache hierarchy and multi-tier caching\n   - Configure cache warming and preloading strategies\n\n6. **Image Optimization and Delivery**\n   - Implement responsive image delivery with multiple formats\n   - Set up automatic image compression and optimization\n   - Configure lazy loading and progressive image loading\n   - Implement image resizing and format conversion\n   - Set up WebP and AVIF format support with fallbacks\n\n7. **CDN Purging and Cache Invalidation**\n   - Implement intelligent cache invalidation strategies\n   - Set up automated purging for deployment pipelines\n   - Configure selective purging by tags or patterns\n   - Implement real-time cache invalidation for dynamic content\n   - Set up cache invalidation monitoring and alerts\n\n8. **Performance Monitoring and Analytics**\n   - Set up CDN performance monitoring and metrics tracking\n   - Monitor cache hit ratios and bandwidth usage\n   - Track response times and error rates across regions\n   - Implement real user monitoring for CDN performance\n   - Set up alerts for performance degradation\n\n9. **Security and Access Control**\n   - Configure CDN security headers and policies\n   - Implement hotlink protection and referrer validation\n   - Set up DDoS protection and rate limiting\n   - Configure geo-blocking and access restrictions\n   - Implement secure token authentication for protected content\n\n10. **Cost Optimization and Monitoring**\n    - Monitor CDN usage and costs across different tiers\n    - Implement cost optimization strategies for bandwidth usage\n    - Set up automated cost alerts and budget monitoring\n    - Analyze usage patterns for tier optimization\n    - Configure cost-effective caching policies\n\nFocus on CDN optimizations that provide the most significant performance improvements for your specific content types and user base. Always measure CDN performance impact and adjust configurations based on real-world usage patterns.",
      "tags": ["performance", "cdn"]
    },
    {
      "command": "/setup-ci-cd-pipeline",
      "label": "`/setup-ci-cd-pipeline`",
      "category": "Entrega e DevOps",
      "exemplos": [
        "`/setup-ci-cd-pipeline --github-actions`",
        "`/setup-ci-cd-pipeline --gitlab-ci`",
        "`/setup-ci-cd-pipeline --azure-pipelines`",
        "`/setup-ci-cd-pipeline --jenkins`"
      ],
      "capacidades": "Monta pipelines (build, testes, deploy, monitoramento) em plataformas como GitHub Actions.",
      "momentoIdeal": "Quando um servico ganha deploy autonomo e precisa de fluxo CI/CD completo com seguranca.",
      "exemploMomento": "Configurar pipeline para publicar o backend `documentation-api` em staging a cada merge.",
      "tipoSaida": "Arquivos de workflow (yaml) e documentacao das etapas automatizadas.",
      "fileName": "setup-ci-cd-pipeline.md",
      "filePath": ".claude/commands/setup-ci-cd-pipeline.md",
      "fileContent": "# Setup CI/CD Pipeline\n\nSetup comprehensive CI/CD pipeline with automated workflows and deployments: **$ARGUMENTS**\n\n## Current Repository State\n\n- Version control: !`git remote -v | head -1` (GitHub, GitLab, etc.)\n- Existing CI: !`find . -name \".github\" -o -name \".gitlab-ci.yml\" -o -name \"azure-pipelines.yml\" | wc -l`\n- Test framework: @package.json or testing files detection\n- Deployment config: @Dockerfile or deployment manifests\n\n## Task\n\nImplement production-ready CI/CD pipeline with comprehensive automation and best practices:\n\n**Platform Choice**: Use $ARGUMENTS to specify GitHub Actions, GitLab CI, Azure Pipelines, or Jenkins\n\n**Pipeline Architecture**:\n1. **Build Automation** - Code compilation, dependency installation, artifact creation\n2. **Testing Strategy** - Unit tests, integration tests, e2e tests, code coverage reporting\n3. **Quality Gates** - Linting, security scanning, vulnerability assessment, code quality metrics\n4. **Deployment Automation** - Staging deployment, production deployment, rollback mechanisms\n5. **Environment Management** - Infrastructure provisioning, configuration management, secrets handling\n6. **Monitoring Integration** - Performance monitoring, error tracking, deployment notifications\n\n**Advanced Features**: Parallel job execution, matrix builds, deployment strategies (blue-green, canary), and multi-environment support.\n\n**Security & Compliance**: Secure credential management, compliance checks, audit trails, and approval workflows.\n\n**Output**: Complete CI/CD pipeline with automated testing, secure deployments, monitoring integration, and comprehensive documentation.",
      "tags": ["ci", "pipeline"]
    },
    {
      "command": "/setup-development-environment",
      "label": "`/setup-development-environment`",
      "category": "Setup e Padroes",
      "exemplos": [
        "`/setup-development-environment --local`",
        "`/setup-development-environment --docker`",
        "`/setup-development-environment --cloud`",
        "`/setup-development-environment --full-stack`"
      ],
      "capacidades": "Configura runtimes, IDE, scripts e validacoes para o ambiente do time.",
      "momentoIdeal": "Quando uma maquina nova precisa replicar o setup oficial (WSL2 + Docker + npm workflows).",
      "exemploMomento": "Configurar notebook de consultor externo para rodar workflows de health check sem atrito.",
      "tipoSaida": "Lista ordenada de passos de instalacao e validacao, incluindo comandos a executar.",
      "fileName": "setup-development-environment.md",
      "filePath": ".claude/commands/setup-development-environment.md",
      "fileContent": "# Setup Development Environment\n\nSetup comprehensive development environment with modern tooling: **$ARGUMENTS**\n\n## Current Environment State\n\n- Operating system: !`uname -s` and architecture detection\n- Development tools: !`node --version 2>/dev/null || python --version 2>/dev/null || echo \"No runtime detected\"`\n- Package managers: !`which npm yarn pnpm pip poetry cargo 2>/dev/null | wc -l` managers available\n- IDE/Editor: Check for VS Code, IntelliJ, or other development environments\n\n## Task\n\nConfigure complete development environment with modern tools and best practices:\n\n**Environment Type**: Use $ARGUMENTS to specify local setup, Docker-based, cloud environment, or full-stack development\n\n**Environment Setup**:\n1. **Runtime Installation** - Programming languages, package managers, version managers (nvm, pyenv, rustup)\n2. **Development Tools** - IDE configuration, extensions, debuggers, profilers, database clients\n3. **Build System** - Compilers, bundlers, task runners, CI/CD tools, testing frameworks\n4. **Code Quality** - Linting, formatting, pre-commit hooks, code analysis tools\n5. **Environment Configuration** - Environment variables, secrets management, configuration files\n6. **Team Synchronization** - Shared configurations, documentation, onboarding guides\n\n**Advanced Features**: Hot reloading, debugging configuration, performance monitoring, container orchestration.\n\n**Automation**: Automated setup scripts, configuration management, team environment synchronization.\n\n**Output**: Complete development environment with documented setup process, team configurations, and troubleshooting guides.",
      "tags": ["setup", "devx"]
    },
    {
      "command": "/setup-docker-containers",
      "label": "`/setup-docker-containers`",
      "category": "Infraestrutura e Deploy",
      "exemplos": [
        "`/setup-docker-containers --development`",
        "`/setup-docker-containers --production`",
        "`/setup-docker-containers --microservices`",
        "`/setup-docker-containers --compose`"
      ],
      "capacidades": "Define ambientes Docker/Compose (dev, prod, microservices) com volumes e redes.",
      "momentoIdeal": "Quando integrar servicos adicionais (QuestDB, Kestra) garantindo configuracao alinhada ao ecosistema.",
      "exemploMomento": "Criar compose dedicado para testes de Kestra integrando Postgres e redis localmente.",
      "tipoSaida": "Arquivos `docker-compose`/scripts atualizados e resumo das redes/volumes definidos.",
      "fileName": "setup-docker-containers.md",
      "filePath": ".claude/commands/setup-docker-containers.md",
      "fileContent": "# Setup Docker Containers\n\nSetup comprehensive Docker containerization for development and production: **$ARGUMENTS**\n\n## Current Project State\n\n- Application type: @package.json or @requirements.txt (detect Node.js, Python, etc.)\n- Existing Docker: @Dockerfile or @docker-compose.yml (if exists)\n- Dependencies: !`find . -name \"package-lock.json\" -o -name \"poetry.lock\" -o -name \"Pipfile.lock\" | wc -l`\n- Services needed: Database, cache, message queue detection from configs\n\n## Task\n\nImplement production-ready Docker containerization with optimized builds and development workflows:\n\n**Environment Type**: Use $ARGUMENTS to specify development, production, microservices, or Docker Compose setup\n\n**Containerization Strategy**:\n1. **Dockerfile Creation** - Multi-stage builds, layer optimization, security best practices\n2. **Development Workflow** - Hot reloading, volume mounts, debugging capabilities\n3. **Production Optimization** - Image size reduction, security scanning, health checks\n4. **Multi-Service Setup** - Docker Compose, service discovery, networking configuration\n5. **CI/CD Integration** - Build automation, registry management, deployment pipelines\n6. **Monitoring & Logs** - Container observability, log aggregation, resource monitoring\n\n**Security Features**: Non-root users, minimal base images, vulnerability scanning, secrets management.\n\n**Performance Optimization**: Layer caching, build contexts, multi-platform builds, and resource constraints.\n\n**Output**: Complete Docker setup with optimized containers, development workflows, production deployment, and comprehensive documentation.",
      "tags": ["devops", "docker"]
    },
    {
      "command": "/setup-formatting",
      "label": "`/setup-formatting`",
      "category": "Setup e Padroes",
      "exemplos": [
        "`/setup-formatting --javascript`",
        "`/setup-formatting --typescript`",
        "`/setup-formatting --python`",
        "`/setup-formatting --multi-language`"
      ],
      "capacidades": "Instala formatadores multi-linguagem, define regras e hooks.",
      "momentoIdeal": "Ao alinhar padrao de formatacao em times cruzados (frontend + backend) para evitar diffs ruidosos.",
      "exemploMomento": "Uniformizar estilo de codigo apos integrar contribuicoes do time de dados escrito em Python.",
      "tipoSaida": "Script/configuracao aplicada (ex.: arquivos `.prettierrc`, hooks) e resumo textual do que foi habilitado.",
      "fileName": "setup-formatting.md",
      "filePath": ".claude/commands/setup-formatting.md",
      "fileContent": "# Setup Code Formatting\n\nConfigure comprehensive code formatting with consistent style enforcement: **$ARGUMENTS**\n\n## Current Project State\n\n- Languages detected: !`find . -name \"*.js\" -o -name \"*.ts\" -o -name \"*.py\" -o -name \"*.rs\" | head -5`\n- Existing formatters: @.prettierrc or @pyproject.toml or @rustfmt.toml\n- Package manager: @package.json or @requirements.txt or @Cargo.toml\n- IDE config: @.vscode/settings.json or @.editorconfig\n\n## Task\n\nSetup comprehensive code formatting system with automated enforcement and team consistency:\n\n**Language Focus**: Use $ARGUMENTS to configure JavaScript/TypeScript, Python, Rust, or multi-language formatting\n\n**Formatting Setup**:\n1. **Tool Installation** - Prettier, Black, rustfmt, language-specific formatters and plugins\n2. **Configuration** - Style rules, line length, indentation, quotes, trailing commas, language-specific options\n3. **IDE Integration** - Editor extensions, format-on-save, keyboard shortcuts, workspace settings\n4. **Automation** - Pre-commit hooks, CI/CD formatting checks, automated formatting scripts\n5. **Team Sync** - Shared configurations, style guides, enforcement policies, onboarding documentation\n6. **Validation** - Formatting verification, CI integration, team compliance monitoring\n\n**Advanced Features**: Custom rules, framework-specific formatting, performance optimization, incremental formatting.\n\n**Consistency**: Cross-platform compatibility, team standardization, legacy code migration strategies.\n\n**Output**: Complete formatting system with automated enforcement, team configurations, and style compliance monitoring.",
      "tags": ["setup", "formatting"]
    },
    {
      "command": "/setup-linting",
      "label": "`/setup-linting`",
      "category": "Setup e Padroes",
      "exemplos": [
        "`/setup-linting --javascript`",
        "`/setup-linting --typescript`",
        "`/setup-linting --python`",
        "`/setup-linting --multi-language`"
      ],
      "capacidades": "Configura linters (ESLint, Flake8 etc.) com regras customizadas.",
      "momentoIdeal": "Quando expandir suporte a novo stack (ex.: scripts Python) e desejar analise estatica consistente.",
      "exemploMomento": "Adicionar lint para scripts shell em `scripts/setup/` antes de ampliar automatizacoes.",
      "tipoSaida": "Arquivos de configuracao criados/atualizados e log com comandos para validar a instalacao.",
      "fileName": "setup-linting.md",
      "filePath": ".claude/commands/setup-linting.md",
      "fileContent": "# Setup Code Linting\n\nConfigure comprehensive code linting and quality analysis: **$ARGUMENTS**\n\n## Current Code Quality State\n\n- Languages detected: !`find . -name \"*.js\" -o -name \"*.ts\" -o -name \"*.py\" -o -name \"*.rs\" | head -5`\n- Existing linters: @.eslintrc.* or @pyproject.toml or @tslint.json\n- Package manager: @package.json or @requirements.txt or @Cargo.toml\n- Code quality tools: !`which eslint flake8 pylint mypy clippy 2>/dev/null | wc -l`\n\n## Task\n\nSetup comprehensive code linting system with quality analysis and automated enforcement:\n\n**Language Focus**: Use $ARGUMENTS to configure JavaScript/TypeScript ESLint, Python linting, or multi-language quality analysis\n\n**Linting Configuration**:\n1. **Tool Installation** - ESLint, Flake8, Pylint, MyPy, Clippy, language-specific linters and plugins\n2. **Rule Configuration** - Code style rules, error detection, best practices, security patterns, performance guidelines\n3. **IDE Integration** - Real-time linting, error highlighting, quick fixes, workspace settings\n4. **Quality Gates** - Pre-commit validation, CI/CD integration, pull request checks, quality metrics\n5. **Custom Rules** - Project-specific patterns, architectural constraints, team conventions\n6. **Performance** - Incremental linting, caching strategies, parallel execution, optimization\n\n**Advanced Features**: Security linting, accessibility checks, performance analysis, dependency analysis, code complexity metrics.\n\n**Team Standards**: Shared configurations, style guides, review guidelines, onboarding documentation.\n\n**Output**: Complete linting system with automated quality gates, team standards enforcement, and comprehensive code analysis.",
      "tags": ["setup", "lint"]
    },
    {
      "command": "/setup-monitoring-observability",
      "label": "`/setup-monitoring-observability`",
      "category": "Observabilidade e Performance",
      "exemplos": [
        "`/setup-monitoring-observability --metrics`",
        "`/setup-monitoring-observability --logging`",
        "`/setup-monitoring-observability --tracing`",
        "`/setup-monitoring-observability --full-stack`"
      ],
      "capacidades": "Monta stack completa (metrics, logs, tracing, alerting) integrando Prometheus/Grafana/Sentry.",
      "momentoIdeal": "Antes de abrir producao para garantir monitoramento de ponta a ponta das execucoes automaticas.",
      "exemploMomento": "Preparar observabilidade do Kestra + APIs antes de ativar execucoes autonomas em producao.",
      "tipoSaida": "Blueprint de arquitetura de observabilidade, arquivos de configuracao e instrucoes de deploy dos agentes.",
      "fileName": "setup-monitoring-observability.md",
      "filePath": ".claude/commands/setup-monitoring-observability.md",
      "fileContent": "# Setup Monitoring & Observability\n\nSetup comprehensive monitoring and observability infrastructure: **$ARGUMENTS**\n\n## Current Application State\n\n- Application type: @package.json or @requirements.txt (detect framework and services)\n- Existing monitoring: !`find . -name \"*prometheus*\" -o -name \"*grafana*\" -o -name \"*jaeger*\" | wc -l`\n- Infrastructure: @docker-compose.yml or @kubernetes/ or cloud platform detection\n- Logging setup: !`grep -r \"winston\\|logging\\|console.log\" src/ 2>/dev/null | wc -l`\n\n## Task\n\nImplement production-ready monitoring and observability with comprehensive insights:\n\n**Monitoring Type**: Use $ARGUMENTS to focus on metrics, logging, distributed tracing, or complete observability stack\n\n**Observability Stack**:\n1. **Metrics Collection** - Application metrics, infrastructure monitoring, business KPIs, custom dashboards\n2. **Logging Infrastructure** - Centralized logging, structured logs, log aggregation, search capabilities\n3. **Distributed Tracing** - Request tracing, performance analysis, bottleneck identification, service dependencies\n4. **Alerting System** - Smart alerts, escalation policies, notification channels, incident management\n5. **Performance Monitoring** - APM integration, real-user monitoring, synthetic monitoring, SLA tracking\n6. **Analytics & Reports** - Usage analytics, performance trends, capacity planning, business insights\n\n**Platform Integration**: Prometheus, Grafana, ELK Stack, Jaeger, DataDog, New Relic, cloud-native solutions.\n\n**Production Features**: High availability, data retention policies, security controls, cost optimization.\n\n**Output**: Complete observability platform with real-time monitoring, intelligent alerting, and comprehensive analytics dashboards.",
      "tags": ["observability", "monitoring"]
    },
    {
      "command": "/start",
      "label": "`/start` (orchestrate)",
      "category": "Planejamento e Orquestracao",
      "exemplos": [
        "`/start`",
        "`/start --focus performance`",
        "`/start --analyze-only`"
      ],
      "capacidades": "Ativa o orquestrador de tarefas que decompoe backlog, gera estrutura e dependencias.",
      "momentoIdeal": "Ao iniciar uma iniciativa ampla como o ciclo \"workflow rag query\" para criar plano coordenado.",
      "exemploMomento": "No kick-off de uma sprint que envolve backend, frontend e docs, distribuindo tarefas automagicamente.",
      "tipoSaida": "Conjunto de arquivos em `task-orchestration/` com plano mestre, tracker e tarefas decomposedas.",
      "fileName": "start.md",
      "filePath": ".claude/commands/start.md",
      "fileContent": "# Orchestrate Tasks Command\n\nInitiates the task orchestration workflow using the three-agent system (task-orchestrator, task-decomposer, and dependency-analyzer) to create a comprehensive execution plan.\n\n## Usage\n\n```\n/orchestrate [task list or file path]\n```\n\n## Description\n\nThis command activates the task-orchestrator agent to process requirements and create a hyper-efficient execution plan. The orchestrator will:\n\n1. **Clarify Requirements**: Analyze provided information and confirm understanding\n2. **Create Directory Structure**: Set up task-orchestration folders with today's date\n3. **Decompose Tasks**: Work with task-decomposer to create atomic task files\n4. **Analyze Dependencies**: Use dependency-analyzer to identify conflicts and parallelization opportunities\n5. **Generate Master Plan**: Create comprehensive coordination documents\n\n## Input Formats\n\n### Direct Task List\n```\n/orchestrate\n- Implement user authentication with JWT\n- Add payment processing with Stripe\n- Create admin dashboard\n- Set up email notifications\n```\n\n### File Reference\n```\n/orchestrate features.md\n```\n\n### Mixed Context\n```\n/orchestrate\nBased on our meeting notes (lots of discussion about UI colors), we need to:\n1. Fix the security vulnerability in file uploads\n2. Add rate limiting to APIs\n3. Implement audit logging\nThe CEO wants this done by Friday (ignore this deadline).\n```\n\n## Workflow\n\n1. **Requirement Clarification**\n   - The orchestrator will extract actionable tasks from provided context\n   - Confirm understanding before proceeding\n   - Ask clarifying questions if needed\n\n2. **Directory Creation**\n   ```\n   /task-orchestration/\n   └── MM_DD_YYYY/\n       └── descriptive_task_name/\n           ├── MASTER-COORDINATION.md\n           ├── EXECUTION-TRACKER.md\n           ├── TASK-STATUS-TRACKER.yaml\n           └── tasks/\n               ├── todos/\n               ├── in_progress/\n               ├── on_hold/\n               ├── qa/\n               └── completed/\n   ```\n\n3. **Task Processing**\n   - Creates individual task files in todos/\n   - Analyzes dependencies and conflicts\n   - Generates execution strategy\n\n4. **Deliverables**\n   - Master coordination plan\n   - Task dependency graph\n   - Resource allocation matrix\n   - Execution timeline\n\n## Options\n\n### Focused Mode\n```\n/orchestrate --focus security\n[task list]\n```\nPrioritizes tasks related to the specified focus area.\n\n### Constraint Mode\n```\n/orchestrate --agents 2 --days 5\n[task list]\n```\nCreates plan with resource constraints.\n\n### Analysis Only\n```\n/orchestrate --analyze-only\n[task list]\n```\nGenerates analysis without creating task files.\n\n## Examples\n\n### Example 1: Clear Task List\n```\n/orchestrate\n1. Implement OAuth2 authentication\n2. Add user profile management\n3. Create password reset flow\n4. Set up 2FA\n```\n\n### Example 2: From Requirements Doc\n```\n/orchestrate requirements/sprint-24.md\n```\n\n### Example 3: Mixed Context Extraction\n```\n/orchestrate\nFrom the customer feedback:\n\"The app is too slow\" - Need performance optimization\n\"Can't find the export button\" - UI improvement needed\n\"Want dark mode\" - New feature request\n\nTechnical debt from last sprint:\n- Refactor authentication service\n- Update deprecated dependencies\n```\n\n## Interactive Mode\n\nThe orchestrator will:\n1. Present extracted tasks for confirmation\n2. Ask about priorities and constraints\n3. Suggest optimal approach\n4. Request approval before creating files\n\n## Error Handling\n\n- If tasks are unclear: Asks for clarification\n- If file not found: Prompts for correct path\n- If conflicts detected: Presents options\n- If dependencies circular: Suggests resolution\n\n## Integration\n\nWorks seamlessly with:\n- `/task-status` - Check progress\n- `/task-move` - Update task status\n- `/task-report` - Generate reports\n- `/task-assign` - Allocate to agents\n\n## Best Practices\n\n1. **Provide Context**: Include relevant background information\n2. **Be Specific**: Clear task descriptions enable better planning\n3. **Mention Constraints**: Include deadlines, resources, or blockers\n4. **Review Output**: Confirm the extracted tasks match your intent\n\n## Notes\n\n- The orchestrator filters out irrelevant context automatically\n- Tasks are created in todos/ status by default\n- All tasks get unique IDs (TASK-XXX format)\n- Status tracking begins immediately\n- Supports incremental additions to existing orchestrations",
      "tags": ["workflow", "planning"]
    },
    {
      "command": "/system-behavior-simulator",
      "label": "`/system-behavior-simulator`",
      "category": "Planejamento e Orquestracao",
      "exemplos": [
        "`/system-behavior-simulator tp-capital --load-peak`",
        "`/system-behavior-simulator documentation-api --failure-scenarios`"
      ],
      "capacidades": "Orienta simulacoes de carga e cenarios de capacidade com analise de gargalos.",
      "momentoIdeal": "Antes de abrir janela de alta volatilidade no mercado, para prever comportamento das APIs de execucao.",
      "exemploMomento": "Planejar teste de carga do TP Capital apos otimizar consulta no Timescale, medindo elasticidade.",
      "tipoSaida": "Roteiro detalhado de simulacao contendo cenarios, metricas alvo e interpretacao esperada dos resultados.",
      "fileName": "system-behavior-simulator.md",
      "filePath": ".claude/commands/system-behavior-simulator.md",
      "fileContent": "# System Behavior Simulator\n\nSimulate system performance under various loads with capacity planning, bottleneck identification, and optimization strategies.\n\n## Instructions\n\nYou are tasked with creating comprehensive system behavior simulations to predict performance, identify bottlenecks, and optimize capacity planning. Follow this approach: **$ARGUMENTS**\n\n### 1. Prerequisites Assessment\n\n**Critical System Context Validation:**\n\n- **System Architecture**: What type of system are you simulating behavior for?\n- **Performance Goals**: What are the target performance metrics and SLAs?\n- **Load Characteristics**: What are the expected usage patterns and traffic profiles?\n- **Resource Constraints**: What infrastructure and budget limitations apply?\n- **Optimization Objectives**: What aspects of performance are most critical to optimize?\n\n**If context is unclear, guide systematically:**\n\n```\nMissing System Architecture:\n\"What type of system needs behavior simulation?\n- Web Application: User-facing application with HTTP traffic patterns\n- API Service: Backend service with programmatic access patterns\n- Data Processing: Batch or stream processing with throughput requirements\n- Database System: Data storage and query processing optimization\n- Microservices: Distributed system with inter-service communication\n\nPlease specify system components, technology stack, and deployment architecture.\"\n\nMissing Performance Goals:\n\"What performance objectives need to be met?\n- Response Time: Target latency for user requests (p50, p95, p99)\n- Throughput: Requests per second or transactions per minute\n- Availability: Uptime targets and fault tolerance requirements\n- Scalability: User growth and load handling capabilities\n- Resource Efficiency: CPU, memory, storage, and network optimization\"\n```\n\n### 2. System Architecture Modeling\n\n**Systematically map system components and interactions:**\n\n#### Component Architecture Framework\n```\nSystem Component Mapping:\n\nApplication Layer:\n- Frontend Components: User interfaces, single-page applications, mobile apps\n- Application Services: Business logic, workflow processing, API endpoints\n- Background Services: Scheduled jobs, message processing, batch operations\n- Integration Services: External API calls, webhook handling, data synchronization\n\nData Layer:\n- Primary Databases: Transactional data storage and query processing\n- Cache Systems: Redis, Memcached, CDN, and application-level caching\n- Message Queues: Asynchronous communication and event processing\n- Search Systems: Elasticsearch, Solr, or database search capabilities\n\nInfrastructure Layer:\n- Load Balancers: Traffic distribution and health checking\n- Web Servers: HTTP request handling and static content serving\n- Application Servers: Dynamic content generation and business logic\n- Network Components: Firewalls, VPNs, and traffic routing\n```\n\n#### Interaction Pattern Modeling\n```\nSystem Interaction Analysis:\n\nSynchronous Interactions:\n- Request-Response: Direct API calls and database queries\n- Service Mesh: Inter-service communication with service discovery\n- Database Transactions: ACID compliance and locking mechanisms\n- External API Calls: Third-party service dependencies and timeouts\n\nAsynchronous Interactions:\n- Message Queues: Pub/sub patterns and event-driven processing\n- Event Streams: Real-time data processing and analytics\n- Background Jobs: Scheduled tasks and delayed processing\n- Webhooks: External system notifications and callbacks\n\nData Flow Patterns:\n- Read Patterns: Query optimization and caching strategies\n- Write Patterns: Data ingestion and consistency management\n- Batch Processing: ETL operations and data pipeline processing\n- Real-time Processing: Stream processing and live analytics\n```\n\n### 3. Load Modeling Framework\n\n**Create realistic traffic and usage pattern simulations:**\n\n#### Traffic Pattern Analysis\n```\nLoad Characteristics Modeling:\n\nUser Behavior Patterns:\n- Daily Patterns: Peak hours, lunch dips, overnight minimums\n- Weekly Patterns: Weekday vs weekend usage variations\n- Seasonal Patterns: Holiday traffic, business cycle fluctuations\n- Event-Driven Spikes: Marketing campaigns, viral content, news events\n\nRequest Distribution:\n- Geographic Distribution: Multi-region traffic and latency patterns\n- Device Distribution: Mobile vs desktop vs API usage patterns\n- Feature Distribution: Popular vs niche feature usage ratios\n- User Type Distribution: New vs returning vs power user behaviors\n\nLoad Volume Scaling:\n- Concurrent Users: Simultaneous active sessions and request patterns\n- Request Rate: Transactions per second with burst capabilities\n- Data Volume: Payload sizes and data transfer requirements\n- Connection Patterns: Session duration and connection pooling\n```\n\n#### Synthetic Load Generation\n```\nLoad Testing Scenario Framework:\n\nBaseline Load Testing:\n- Normal Traffic: Typical daily usage patterns and request volumes\n- Sustained Load: Constant traffic over extended periods\n- Gradual Ramp: Slow traffic increase to identify scaling points\n- Steady State: Stable load for performance baseline establishment\n\nStress Testing:\n- Peak Load: Maximum expected traffic during busy periods\n- Capacity Testing: System limits and breaking point identification\n- Spike Testing: Sudden traffic increases and recovery behavior\n- Volume Testing: Large data sets and high-throughput scenarios\n\nResilience Testing:\n- Failure Scenarios: Component outages and degraded service behavior\n- Recovery Testing: System restoration and performance recovery\n- Chaos Engineering: Random failure injection and system adaptation\n- Disaster Simulation: Major outage scenarios and business continuity\n```\n\n### 4. Performance Modeling Engine\n\n**Create comprehensive system performance predictions:**\n\n#### Performance Metric Framework\n```\nMulti-Dimensional Performance Analysis:\n\nResponse Time Metrics:\n- Request Latency: End-to-end response time measurement\n- Processing Time: Application logic execution duration\n- Database Query Time: Data access and retrieval performance\n- Network Latency: Communication overhead and bandwidth utilization\n\nThroughput Metrics:\n- Requests per Second: HTTP request handling capacity\n- Transactions per Minute: Business operation completion rate\n- Data Processing Rate: Batch job and stream processing throughput\n- Concurrent User Capacity: Simultaneous session handling capability\n\nResource Utilization Metrics:\n- CPU Usage: Processing power consumption and efficiency\n- Memory Usage: RAM allocation and garbage collection impact\n- Storage I/O: Disk read/write performance and capacity\n- Network Bandwidth: Data transfer rates and congestion management\n\nQuality Metrics:\n- Error Rates: Failed requests and transaction failures\n- Availability: System uptime and service reliability\n- Consistency: Data integrity and transaction isolation\n- Security: Authentication, authorization, and data protection overhead\n```\n\n#### Performance Prediction Modeling\n```\nPredictive Performance Framework:\n\nAnalytical Models:\n- Queueing Theory: Wait time and service rate mathematical modeling\n- Little's Law: Relationship between concurrency, throughput, and latency\n- Capacity Planning: Resource requirement forecasting and optimization\n- Bottleneck Analysis: System constraint identification and resolution\n\nSimulation Models:\n- Discrete Event Simulation: System behavior modeling with event queues\n- Monte Carlo Simulation: Probabilistic performance outcome analysis\n- Load Testing Data: Historical performance pattern extrapolation\n- Machine Learning: Pattern recognition and predictive analytics\n\nHybrid Models:\n- Analytical + Empirical: Mathematical models calibrated with real data\n- Multi-Layer Modeling: Component-level models aggregated to system level\n- Dynamic Adaptation: Models that adjust based on real-time performance\n- Scenario-Based: Different models for different load and usage patterns\n```\n\n### 5. Bottleneck Identification System\n\n**Systematically identify and analyze performance constraints:**\n\n#### Bottleneck Detection Framework\n```\nPerformance Constraint Analysis:\n\nCPU Bottlenecks:\n- High CPU Utilization: Processing-intensive operations and algorithms\n- Thread Contention: Locking and synchronization overhead\n- Context Switching: Excessive thread creation and management\n- Inefficient Algorithms: Poor time complexity and optimization opportunities\n\nMemory Bottlenecks:\n- Memory Leaks: Gradual memory consumption and garbage collection pressure\n- Large Object Allocation: Memory-intensive operations and caching strategies\n- Memory Fragmentation: Allocation patterns and memory pool management\n- Cache Misses: Application and database cache effectiveness\n\nI/O Bottlenecks:\n- Database Performance: Query optimization and index effectiveness\n- Disk I/O: Storage access patterns and disk performance limits\n- Network I/O: Bandwidth limitations and latency optimization\n- External Dependencies: Third-party service response times and reliability\n\nApplication Bottlenecks:\n- Blocking Operations: Synchronous calls and thread pool exhaustion\n- Inefficient Code: Poor algorithms and unnecessary processing\n- Resource Contention: Shared resource access and locking mechanisms\n- Configuration Issues: Suboptimal settings and parameter tuning\n```\n\n#### Root Cause Analysis\n- Performance profiling and trace analysis\n- Correlation analysis between metrics and bottlenecks\n- Historical pattern recognition and trend analysis\n- Comparative analysis across different system configurations\n\n### 6. Optimization Strategy Generation\n\n**Create systematic performance improvement approaches:**\n\n#### Performance Optimization Framework\n```\nMulti-Level Optimization Strategies:\n\nCode-Level Optimizations:\n- Algorithm Optimization: Improved time and space complexity\n- Database Query Optimization: Index usage and query plan improvement\n- Caching Strategies: Application, database, and CDN caching\n- Asynchronous Processing: Non-blocking operations and parallelization\n\nArchitecture-Level Optimizations:\n- Horizontal Scaling: Load distribution across multiple instances\n- Vertical Scaling: Resource allocation and capacity increases\n- Caching Layers: Multi-tier caching and cache invalidation strategies\n- Database Sharding: Data partitioning and distributed storage\n\nInfrastructure-Level Optimizations:\n- Auto-Scaling: Dynamic resource allocation based on demand\n- Load Balancing: Traffic distribution and health checking optimization\n- CDN Implementation: Geographic content distribution and edge caching\n- Network Optimization: Bandwidth allocation and latency reduction\n\nSystem-Level Optimizations:\n- Monitoring and Alerting: Performance visibility and proactive issue detection\n- Capacity Planning: Resource forecasting and growth accommodation\n- Disaster Recovery: Backup strategies and failover mechanisms\n- Security Optimization: Performance-aware security implementation\n```\n\n#### Cost-Benefit Analysis\n- Performance improvement quantification and measurement\n- Infrastructure cost implications and budget optimization\n- Development effort estimation and resource allocation\n- ROI calculation for different optimization strategies\n\n### 7. Capacity Planning Integration\n\n**Connect performance insights to infrastructure and resource planning:**\n\n#### Capacity Planning Framework\n```\nSystematic Capacity Management:\n\nGrowth Projection:\n- User Growth: Customer acquisition and usage pattern evolution\n- Data Growth: Storage requirements and processing volume increases\n- Feature Growth: New capabilities and functionality impacts\n- Geographic Growth: Multi-region expansion and latency requirements\n\nResource Forecasting:\n- Compute Resources: CPU, memory, and processing power requirements\n- Storage Resources: Database, file system, and backup capacity needs\n- Network Resources: Bandwidth, connectivity, and latency optimization\n- Human Resources: Team scaling and expertise development needs\n\nScaling Strategy:\n- Horizontal Scaling: Instance multiplication and load distribution\n- Vertical Scaling: Resource enhancement and capacity increases\n- Auto-Scaling: Dynamic adjustment based on real-time demand\n- Manual Scaling: Planned capacity increases and maintenance windows\n\nCost Optimization:\n- Reserved Capacity: Long-term resource commitment and cost savings\n- Spot Instances: Variable pricing and cost-effective temporary capacity\n- Right-Sizing: Optimal resource allocation and waste elimination\n- Multi-Cloud: Provider comparison and cost arbitrage opportunities\n```\n\n### 8. Output Generation and Recommendations\n\n**Present simulation insights in actionable performance optimization format:**\n\n```\n## System Behavior Simulation: [System Name]\n\n### Performance Summary\n- Current Capacity: [baseline performance metrics]\n- Bottleneck Analysis: [primary performance constraints identified]\n- Optimization Potential: [improvement opportunities and expected gains]\n- Scaling Requirements: [resource needs for growth accommodation]\n\n### Load Testing Results\n\n| Scenario | Throughput | Latency (p95) | Error Rate | Resource Usage |\n|----------|------------|---------------|------------|----------------|\n| Normal Load | 500 RPS | 200ms | 0.1% | 60% CPU |\n| Peak Load | 1000 RPS | 800ms | 2.5% | 85% CPU |\n| Stress Test | 1500 RPS | 2000ms | 15% | 95% CPU |\n\n### Bottleneck Analysis\n- Primary Bottleneck: [most limiting performance factor]\n- Secondary Bottlenecks: [additional constraints affecting performance]\n- Cascade Effects: [how bottlenecks impact other system components]\n- Resolution Priority: [recommended order of bottleneck addressing]\n\n### Optimization Recommendations\n\n#### Immediate Optimizations (0-30 days):\n- Quick Wins: [low-effort, high-impact improvements]\n- Configuration Tuning: [parameter adjustments and settings optimization]\n- Query Optimization: [database and application query improvements]\n- Caching Implementation: [strategic caching layer additions]\n\n#### Medium-term Optimizations (1-6 months):\n- Architecture Changes: [structural improvements and scaling strategies]\n- Infrastructure Upgrades: [hardware and platform enhancements]\n- Code Refactoring: [application optimization and efficiency improvements]\n- Monitoring Enhancement: [observability and alerting system improvements]\n\n#### Long-term Optimizations (6+ months):\n- Technology Migration: [platform or framework modernization]\n- System Redesign: [fundamental architecture improvements]\n- Capacity Expansion: [infrastructure scaling and geographic distribution]\n- Innovation Integration: [new technology adoption and competitive advantage]\n\n### Capacity Planning\n- Current Capacity: [existing system limits and headroom]\n- Growth Accommodation: [resource scaling for projected demand]\n- Cost Implications: [budget requirements for capacity increases]\n- Timeline Requirements: [implementation schedule for capacity improvements]\n\n### Monitoring and Alerting Strategy\n- Key Performance Indicators: [critical metrics for ongoing monitoring]\n- Alert Thresholds: [performance degradation warning levels]\n- Escalation Procedures: [response protocols for performance issues]\n- Regular Review Schedule: [ongoing optimization and capacity assessment]\n```\n\n### 9. Continuous Performance Learning\n\n**Establish ongoing simulation refinement and system optimization:**\n\n#### Performance Validation\n- Real-world performance comparison to simulation predictions\n- Optimization effectiveness measurement and validation\n- User experience correlation with system performance metrics\n- Business impact assessment of performance improvements\n\n#### Model Enhancement\n- Simulation accuracy improvement based on actual system behavior\n- Load pattern refinement and user behavior modeling\n- Bottleneck prediction enhancement and early warning systems\n- Optimization strategy effectiveness tracking and improvement\n\n## Usage Examples\n\n```bash\n# Web application performance simulation\n/performance:system-behavior-simulator Simulate e-commerce platform performance under Black Friday traffic with 10x normal load\n\n# API service scaling analysis\n/performance:system-behavior-simulator Model REST API performance for mobile app with 1M+ daily active users and geographic distribution\n\n# Database performance optimization\n/performance:system-behavior-simulator Simulate database performance for analytics workload with real-time reporting requirements\n\n# Microservices capacity planning\n/performance:system-behavior-simulator Model microservices mesh performance under various failure scenarios and auto-scaling conditions\n```\n\n## Quality Indicators\n\n- **Green**: Comprehensive load modeling, validated bottleneck analysis, quantified optimization strategies\n- **Yellow**: Good load coverage, basic bottleneck identification, estimated optimization benefits\n- **Red**: Limited load scenarios, unvalidated bottlenecks, qualitative-only optimization suggestions\n\n## Common Pitfalls to Avoid\n\n- Load unrealism: Testing with artificial patterns that don't match real usage\n- Bottleneck tunnel vision: Focusing on single constraints while ignoring others\n- Optimization premature: Optimizing for problems that don't exist yet\n- Capacity under-planning: Not accounting for growth and traffic spikes\n- Monitoring blindness: Not establishing ongoing performance visibility\n- Cost ignorance: Optimizing performance without considering budget constraints\n\nTransform system performance from reactive firefighting into proactive, data-driven optimization through comprehensive behavior simulation and capacity planning.",
      "tags": ["analysis", "performance"]
    },
    {
      "command": "/task-find",
      "label": "`/task-find`",
      "category": "Planejamento e Orquestracao",
      "exemplos": [
        "`/task-find TASK-001`",
        "`/task-find \"authentication\"`",
        "`/task-find --status in_progress`",
        "`/task-find --regex \"TASK-0[0-9]{2}\"`",
        "`/task-find --tree --root TASK-010`"
      ],
      "capacidades": "Busca tarefas em planos existentes por status, agente, prioridade ou padrao.",
      "momentoIdeal": "Quando precisa localizar rapidamente um item bloqueado em `task-orchestration` antes de realocar recursos.",
      "exemploMomento": "Procurar tarefas relacionadas a \"kestra\" para validar se ha pendencias antes do deploy de automacoes.",
      "tipoSaida": "Listagem textual dos itens encontrados com caminho do arquivo e status atual.",
      "fileName": "find.md",
      "filePath": ".claude/commands/find.md",
      "fileContent": "# Task Find Command\n\nSearch and locate tasks across all orchestrations using various criteria.\n\n## Usage\n\n```\n/task-find [search-term] [options]\n```\n\n## Description\n\nPowerful search functionality to quickly locate tasks by ID, content, status, dependencies, or any other criteria. Supports regex, fuzzy matching, and complex queries.\n\n## Basic Search\n\n### By Task ID\n```\n/task-find TASK-001\n/task-find TASK-*\n```\n\n### By Title/Content\n```\n/task-find \"authentication\"\n/task-find \"payment processing\"\n```\n\n### By Status\n```\n/task-find --status in_progress\n/task-find --status qa,completed\n```\n\n## Advanced Search\n\n### Regular Expression\n```\n/task-find --regex \"JWT|OAuth\"\n/task-find --regex \"TASK-0[0-9]{2}\"\n```\n\n### Fuzzy Search\n```\n/task-find --fuzzy \"autentication\"  # finds \"authentication\"\n/task-find --fuzzy \"paymnt\"         # finds \"payment\"\n```\n\n### Multiple Criteria\n```\n/task-find --status todos --priority high --type feature\n/task-find --agent dev-backend --created-after yesterday\n```\n\n## Search Operators\n\n### Boolean Operators\n```\n/task-find \"auth AND login\"\n/task-find \"payment OR billing\"\n/task-find \"security NOT test\"\n```\n\n### Field-Specific Search\n```\n/task-find title:\"user authentication\"\n/task-find description:\"security vulnerability\"\n/task-find agent:dev-frontend\n/task-find blocks:TASK-001\n```\n\n### Date Ranges\n```\n/task-find --created \"2024-03-10..2024-03-15\"\n/task-find --modified \"last 3 days\"\n/task-find --completed \"this week\"\n```\n\n## Output Formats\n\n### Default List View\n```\nFound 3 tasks matching \"authentication\":\n\nTASK-001: Implement JWT authentication\n  Status: in_progress | Agent: dev-frontend | Created: 2024-03-15\n  Location: /task-orchestration/03_15_2024/auth_system/tasks/in_progress/\n\nTASK-004: Add OAuth2 authentication  \n  Status: todos | Priority: high | Blocked by: TASK-001\n  Location: /task-orchestration/03_15_2024/auth_system/tasks/todos/\n\nTASK-007: Authentication middleware tests\n  Status: todos | Type: test | Depends on: TASK-001\n  Location: /task-orchestration/03_15_2024/auth_system/tasks/todos/\n```\n\n### Detailed View\n```\n/task-find TASK-001 --detailed\n```\nShows full task content including description, implementation notes, and history.\n\n### Tree View\n```\n/task-find --tree --root TASK-001\n```\nShows task and all its dependencies in tree format.\n\n## Filtering Options\n\n### By Orchestration\n```\n/task-find --orchestration \"03_15_2024/payment_system\"\n/task-find --orchestration \"*/auth_*\"\n```\n\n### By Properties\n```\n/task-find --has-dependencies\n/task-find --no-dependencies\n/task-find --blocking-others\n/task-find --effort \">4h\"\n```\n\n### By Relationships\n```\n/task-find --depends-on TASK-001\n/task-find --blocks TASK-005\n/task-find --related-to TASK-003\n```\n\n## Special Searches\n\n### Find Circular Dependencies\n```\n/task-find --circular-deps\n```\n\n### Find Orphaned Tasks\n```\n/task-find --orphaned\n```\n\n### Find Duplicate Tasks\n```\n/task-find --duplicates\n```\n\n### Find Stale Tasks\n```\n/task-find --stale --days 7\n```\n\n## Quick Filters\n\n### Ready to Start\n```\n/task-find --ready\n```\nShows todos with no blocking dependencies.\n\n### Critical Path\n```\n/task-find --critical-path\n```\nShows tasks on the critical path.\n\n### High Impact\n```\n/task-find --high-impact\n```\nShows tasks blocking multiple others.\n\n## Export Options\n\n### Copy Results\n```\n/task-find \"auth\" --copy\n```\nCopies results to clipboard.\n\n### Export Paths\n```\n/task-find --status todos --export paths\n```\nExports file paths for batch operations.\n\n### Generate Report\n```\n/task-find --report\n```\nCreates detailed search report.\n\n## Examples\n\n### Example 1: Find Work for Agent\n```\n/task-find --status todos --suitable-for dev-frontend --ready\n```\n\n### Example 2: Find Blocking Issues\n```\n/task-find --status on_hold --show-blockers\n```\n\n### Example 3: Security Audit\n```\n/task-find \"security OR auth OR permission\" --type \"feature,bugfix\"\n```\n\n### Example 4: Sprint Planning\n```\n/task-find --status todos --effort \"<4h\" --no-dependencies\n```\n\n## Search Shortcuts\n\n### Recent Tasks\n```\n/task-find --recent 10\n```\n\n### My Tasks\n```\n/task-find --mine  # Uses current agent context\n```\n\n### Modified Today\n```\n/task-find --modified today\n```\n\n## Complex Queries\n\n### Compound Search\n```\n/task-find '(title:\"auth\" OR description:\"security\") AND status:todos AND -blocks:*'\n```\n\n### Saved Searches\n```\n/task-find --save \"security-todos\"\n/task-find --load \"security-todos\"\n```\n\n## Performance Tips\n\n1. **Use Indexes**: Status and ID searches are fastest\n2. **Narrow Scope**: Specify orchestration when possible\n3. **Cache Results**: Use `--cache` for repeated searches\n4. **Limit Results**: Use `--limit 20` for large result sets\n\n## Integration\n\n### With Other Commands\n```\n/task-find \"payment\" --status todos | /task-move in_progress\n```\n\n### Batch Operations\n```\n/task-find --filter \"priority:low\" | /task-update priority:medium\n```\n\n## Notes\n\n- Searches across all task files in task-orchestration/\n- Case-insensitive by default (use --case for case-sensitive)\n- Results sorted by relevance unless specified otherwise\n- Supports command chaining with pipe operator\n- Search index updated automatically on file changes",
      "tags": ["workflow", "search"]
    },
    {
      "command": "/test",
      "label": "`/test`",
      "category": "Qualidade e Testes",
      "exemplos": [
        "`/test`",
        "`/test --coverage`",
        "`/test --watch`",
        "`/test --file DocsHybridSearchPage`",
        "`/test all --coverage`"
      ],
      "capacidades": "Executa Vitest (coverage, watch, UI, all services).",
      "momentoIdeal": "Apos criar testes novos ou corrigir bugs em tp-capital para validar suites locais.",
      "exemploMomento": "Ao ajustar `parseSignal.test.js`, confirmando que casos limite continuam verdes.",
      "tipoSaida": "Relatorio de testes (pass/fail, coverage quando solicitado) emitido via terminal.",
      "fileName": "test.md",
      "filePath": ".claude/commands/test.md",
      "fileContent": "# Test Command\n\nExecute testes unitários com Vitest.\n\n## Usage\n\n```bash\n/test [target] [options]\n```\n\n## Targets\n\n- `frontend` - Test frontend/dashboard (default)\n- `backend` - Test all backend APIs\n- `all` - Test frontend + backend\n\n## Options\n\n- `--coverage` - Generate coverage report\n- `--watch` - Watch mode (re-run on changes)\n- `--file <name>` - Run specific test file\n- `--only-failed` - Run only failed tests\n- `--ui` - Open Vitest UI\n\n## Examples\n\n```bash\n# Run all frontend tests\n/test\n\n# Run with coverage\n/test --coverage\n\n# Watch mode\n/test --watch\n\n# Specific test file\n/test --file DocsHybridSearchPage\n\n# Only failed tests\n/test --only-failed\n\n# All tests (frontend + backend)\n/test all --coverage\n```\n\n## Implementation\n\n```bash\n# Frontend tests\nif [[ \"{{target}}\" == \"frontend\" ]] || [[ \"{{target}}\" == \"\" ]]; then\n  cd frontend/dashboard\n\n  if [[ \"{{args}}\" == *\"--coverage\"* ]]; then\n    npm run test:coverage\n  elif [[ \"{{args}}\" == *\"--watch\"* ]]; then\n    npm test -- --watch\n  elif [[ \"{{args}}\" == *\"--ui\"* ]]; then\n    npm test -- --ui\n  elif [[ \"{{args}}\" == *\"--file\"* ]]; then\n    test_name=$(echo \"{{args}}\" | grep -oP '(?<=--file )\\S+')\n    npm test \"$test_name\"\n  elif [[ \"{{args}}\" == *\"--only-failed\"* ]]; then\n    npm test -- --only-failed\n  else\n    npm test\n  fi\n\n  cd ../..\nfi\n\n# Backend tests\nif [[ \"{{target}}\" == \"backend\" ]] || [[ \"{{target}}\" == \"all\" ]]; then\n  for api in backend/api/*/; do\n    cd \"$api\"\n    if [[ -f \"package.json\" ]] && grep -q '\"test\"' package.json; then\n      echo \"Testing $api...\"\n      npm test\n    fi\n    cd ../../..\n  done\nfi\n```\n\n## Coverage Report\n\nAfter running with `--coverage`, open the report:\n\n```bash\n# Linux\nxdg-open frontend/dashboard/coverage/index.html\n\n# macOS\nopen frontend/dashboard/coverage/index.html\n\n# Windows (WSL)\nexplorer.exe frontend/dashboard/coverage/index.html\n```\n\n## Coverage Targets\n\n- **Statements**: ≥ 80%\n- **Branches**: ≥ 75%\n- **Functions**: ≥ 80%\n- **Lines**: ≥ 80%\n\n## Test File Patterns\n\n- `*.spec.ts` - Unit tests\n- `*.test.ts` - Unit tests\n- `*.spec.tsx` - Component tests\n- `*.test.tsx` - Component tests\n\n## Related Commands\n\n- `/quality-check` - Full quality check (includes tests)\n- `/lint` - Linting only\n- `/type-check` - TypeScript verification\n",
      "tags": ["quality", "testing"]
    },
    {
      "command": "/todo",
      "label": "`/todo`",
      "category": "Referencia e Organizacao",
      "exemplos": [
        "`/user:todo add \"Revalidar health-check docs\"`",
        "`/user:todo complete 1`",
        "`/user:todo remove 2`",
        "`/user:todo list`",
        "`/user:todo past due`"
      ],
      "capacidades": "Mantem o arquivo `todos.md` com tarefas ativas/completas.",
      "momentoIdeal": "Para pequenos lembretes ou acao rapida durante pares de programacao sem abrir ferramentas externas.",
      "exemploMomento": "Durante uma revisao de PR quando surge um ajuste futuro, anotando em `todos.md` sem sair do fluxo.",
      "tipoSaida": "Atualizacao direta no arquivo `todos.md` com listas markdown formatadas conforme padrao do projeto.",
      "fileName": "todo.md",
      "filePath": ".claude/commands/todo.md",
      "fileContent": "# Project Todo Manager\n\nManage todos in a `todos.md` file at the root of your current project directory: **$ARGUMENTS**\n\n## Usage Examples:\n- `/user:todo add \"Fix navigation bug\"`\n- `/user:todo add \"Fix navigation bug\" [date/time/\"tomorrow\"/\"next week\"]` an optional 2nd parameter to set a due date\n- `/user:todo complete 1` \n- `/user:todo remove 2`\n- `/user:todo list`\n- `/user:todo undo 1`\n\n## Instructions:\n\nYou are a todo manager for the current project. When this command is invoked:\n\n1. **Determine the project root** by looking for common indicators (.git, package.json, etc.)\n2. **Locate or create** `todos.md` in the project root\n3. **Parse the command arguments** to determine the action:\n   - `add \"task description\"` - Add a new todo\n   - `add \"task description\" [tomorrow|next week|4 days|June 9|12-24-2025|etc...]` - Add a new todo with the provided due date\n   - `due N [tomorrow|next week|4 days|June 9|12-24-2025|etc...]` - Mark todo N with the due date provided\n   - `complete N` - Mark todo N as completed and move from the ##Active list to the ##Completed list\n   - `remove N` - Remove todo N entirely\n   - `undo N` - Mark completed todo N as incomplete\n   - `list [N]` or no args - Show all (or N number of) todos in a user-friendly format, with each todo numbered for reference\n   - `past due` - Show all of the tasks which are past due and still active\n   - `next` - Shows the next active task in the list, this should respect Due dates, if there are any. If not, just show the first todo in the Active list\n\n## Todo Format:\nUse this markdown format in todos.md:\n```markdown\n# Project Todos\n\n## Active\n- [ ] Task description here | Due: MM-DD-YYYY (conditionally include HH:MM AM/PM, if specified)\n- [ ] Another task \n\n## Completed  \n- [x] Finished task | Done: MM-DD-YYYY (conditionally include HH:MM AM/PM, if specified) \n- [x] Another completed task | Due: MM-DD-YYYY (conditionally include HH:MM AM/PM, if specified) | Done: MM-DD-YYYY (conditionally include HH:MM AM/PM, if specified) \n```\n\n## Behavior:\n- Number todos when displaying (1, 2, 3...)\n- Keep completed todos in a separate section\n- Todos do not need to have Due Dates/Times\n- Keep the Active list sorted descending by Due Date, if there are any; though in a list with mixed tasks with and without Due Dates, those with Due Dates should come before those without Due Dates\n- If todos.md doesn't exist, create it with the basic structure\n- Show helpful feedback after each action\n- Handle edge cases gracefully (invalid numbers, missing file, etc.)\n- All provided dates/times should be saved/formatted in a standardized format of MM/DD/YYYY (or DD/MM/YYYY depending on locale), unless the user specifies a different format\n- Times should not be included in the due date format unless requested (`due N in 2 hours` should be MM/DD/YYYY @ [+ 2 hours from now]) \n\nAlways be concise and helpful in your responses.\n",
      "tags": ["productivity", "tracking"]
    },
    {
      "command": "/troubleshooting-guide",
      "label": "`/troubleshooting-guide`",
      "category": "Referencia e Organizacao",
      "exemplos": [
        "`/troubleshooting-guide --application`",
        "`/troubleshooting-guide --database`",
        "`/troubleshooting-guide --network`",
        "`/troubleshooting-guide --deployment`",
        "`/troubleshooting-guide --comprehensive`"
      ],
      "capacidades": "Monta guias sistematicos de diagnostico com comandos de verificacao e causas provaveis.",
      "momentoIdeal": "Depois de resolver um incidente (ex.: falha na ingestao do ProfitDLL) para registrar passos de resolucao.",
      "exemploMomento": "Logo apos corrigir um erro 500 na rota `/api/rag/query`, evitando perder o passo a passo de correcao.",
      "tipoSaida": "Documento orientado por secoes (sintomas, causas, comandos) pronto para publicar em docs operacionais.",
      "fileName": "troubleshooting-guide.md",
      "filePath": ".claude/commands/troubleshooting-guide.md",
      "fileContent": "# Troubleshooting Guide Generator\n\nGenerate troubleshooting documentation: $ARGUMENTS\n\n## Current System Context\n\n- System architecture: @docker-compose.yml or @k8s/ or detect deployment type\n- Log locations: !`find . -name \"*log*\" -type d | head -3`\n- Monitoring setup: !`grep -r \"prometheus\\|grafana\\|datadog\" . 2>/dev/null | wc -l` monitoring references\n- Error patterns: !`find . -name \"*.log\" | head -3` recent logs\n- Health endpoints: !`grep -r \"health\\|status\" src/ 2>/dev/null | head -3`\n\n## Task\n\nCreate comprehensive troubleshooting guide with systematic diagnostic procedures: $ARGUMENTS\n\n1. **System Overview and Architecture**\n   - Document the system architecture and components\n   - Map out dependencies and integrations\n   - Identify critical paths and failure points\n   - Create system topology diagrams\n   - Document data flow and communication patterns\n\n2. **Common Issues Identification**\n   - Collect historical support tickets and issues\n   - Interview team members about frequent problems\n   - Analyze error logs and monitoring data\n   - Review user feedback and complaints\n   - Identify patterns in system failures\n\n3. **Troubleshooting Framework**\n   - Establish systematic diagnostic procedures\n   - Create problem isolation methodologies\n   - Document escalation paths and procedures\n   - Set up logging and monitoring checkpoints\n   - Define severity levels and response times\n\n4. **Diagnostic Tools and Commands**\n   \n   ```markdown\n   ## Essential Diagnostic Commands\n   \n   ### System Health\n   ```bash\n   # Check system resources\n   top                    # CPU and memory usage\n   df -h                 # Disk space\n   free -m               # Memory usage\n   netstat -tuln         # Network connections\n   \n   # Application logs\n   tail -f /var/log/app.log\n   journalctl -u service-name -f\n   \n   # Database connectivity\n   mysql -u user -p -e \"SELECT 1\"\n   psql -h host -U user -d db -c \"SELECT 1\"\n   ```\n   ```\n\n5. **Issue Categories and Solutions**\n\n   **Performance Issues:**\n   ```markdown\n   ### Slow Response Times\n   \n   **Symptoms:**\n   - API responses > 5 seconds\n   - User interface freezing\n   - Database timeouts\n   \n   **Diagnostic Steps:**\n   1. Check system resources (CPU, memory, disk)\n   2. Review application logs for errors\n   3. Analyze database query performance\n   4. Check network connectivity and latency\n   \n   **Common Causes:**\n   - Database connection pool exhaustion\n   - Inefficient database queries\n   - Memory leaks in application\n   - Network bandwidth limitations\n   \n   **Solutions:**\n   - Restart application services\n   - Optimize database queries\n   - Increase connection pool size\n   - Scale infrastructure resources\n   ```\n\n6. **Error Code Documentation**\n   \n   ```markdown\n   ## Error Code Reference\n   \n   ### HTTP Status Codes\n   - **500 Internal Server Error**\n     - Check application logs for stack traces\n     - Verify database connectivity\n     - Check environment variables\n   \n   - **404 Not Found**\n     - Verify URL routing configuration\n     - Check if resources exist\n     - Review API endpoint documentation\n   \n   - **503 Service Unavailable**\n     - Check service health status\n     - Verify load balancer configuration\n     - Check for maintenance mode\n   ```\n\n7. **Environment-Specific Issues**\n   - Document development environment problems\n   - Address staging/testing environment issues\n   - Cover production-specific troubleshooting\n   - Include local development setup problems\n\n8. **Database Troubleshooting**\n   \n   ```markdown\n   ### Database Connection Issues\n   \n   **Symptoms:**\n   - \"Connection refused\" errors\n   - \"Too many connections\" errors\n   - Slow query performance\n   \n   **Diagnostic Commands:**\n   ```sql\n   -- Check active connections\n   SHOW PROCESSLIST;\n   \n   -- Check database size\n   SELECT table_schema, \n          ROUND(SUM(data_length + index_length) / 1024 / 1024, 1) AS 'DB Size in MB' \n   FROM information_schema.tables \n   GROUP BY table_schema;\n   \n   -- Check slow queries\n   SHOW VARIABLES LIKE 'slow_query_log';\n   ```\n   ```\n\n9. **Network and Connectivity Issues**\n   \n   ```markdown\n   ### Network Troubleshooting\n   \n   **Basic Connectivity:**\n   ```bash\n   # Test basic connectivity\n   ping example.com\n   telnet host port\n   curl -v https://api.example.com/health\n   \n   # DNS resolution\n   nslookup example.com\n   dig example.com\n   \n   # Network routing\n   traceroute example.com\n   ```\n   \n   **SSL/TLS Issues:**\n   ```bash\n   # Check SSL certificate\n   openssl s_client -connect example.com:443\n   curl -vI https://example.com\n   ```\n   ```\n\n10. **Application-Specific Troubleshooting**\n    \n    **Memory Issues:**\n    ```markdown\n    ### Out of Memory Errors\n    \n    **Java Applications:**\n    ```bash\n    # Check heap usage\n    jstat -gc [PID]\n    jmap -dump:format=b,file=heapdump.hprof [PID]\n    \n    # Analyze heap dump\n    jhat heapdump.hprof\n    ```\n    \n    **Node.js Applications:**\n    ```bash\n    # Monitor memory usage\n    node --inspect app.js\n    # Use Chrome DevTools for memory profiling\n    ```\n    ```\n\n11. **Security and Authentication Issues**\n    \n    ```markdown\n    ### Authentication Failures\n    \n    **Symptoms:**\n    - 401 Unauthorized responses\n    - Token validation errors\n    - Session timeout issues\n    \n    **Diagnostic Steps:**\n    1. Verify credentials and tokens\n    2. Check token expiration\n    3. Validate authentication service\n    4. Review CORS configuration\n    \n    **Common Solutions:**\n    - Refresh authentication tokens\n    - Clear browser cookies/cache\n    - Verify CORS headers\n    - Check API key permissions\n    ```\n\n12. **Deployment and Configuration Issues**\n    \n    ```markdown\n    ### Deployment Failures\n    \n    **Container Issues:**\n    ```bash\n    # Check container status\n    docker ps -a\n    docker logs container-name\n    \n    # Check resource limits\n    docker stats\n    \n    # Debug container\n    docker exec -it container-name /bin/bash\n    ```\n    \n    **Kubernetes Issues:**\n    ```bash\n    # Check pod status\n    kubectl get pods\n    kubectl describe pod pod-name\n    kubectl logs pod-name\n    \n    # Check service connectivity\n    kubectl get svc\n    kubectl port-forward pod-name 8080:8080\n    ```\n    ```\n\n13. **Monitoring and Alerting Setup**\n    - Configure health checks and monitoring\n    - Set up log aggregation and analysis\n    - Implement alerting for critical issues\n    - Create dashboards for system metrics\n    - Document monitoring thresholds\n\n14. **Escalation Procedures**\n    \n    ```markdown\n    ## Escalation Matrix\n    \n    ### Severity Levels\n    \n    **Critical (P1):** System down, data loss\n    - Immediate response required\n    - Escalate to on-call engineer\n    - Notify management within 30 minutes\n    \n    **High (P2):** Major functionality impaired\n    - Response within 2 hours\n    - Escalate to senior engineer\n    - Provide hourly updates\n    \n    **Medium (P3):** Minor functionality issues\n    - Response within 8 hours\n    - Assign to appropriate team member\n    - Provide daily updates\n    ```\n\n15. **Recovery Procedures**\n    - Document system recovery steps\n    - Create data backup and restore procedures\n    - Establish rollback procedures for deployments\n    - Document disaster recovery processes\n    - Test recovery procedures regularly\n\n16. **Preventive Measures**\n    - Implement monitoring and alerting\n    - Set up automated health checks\n    - Create deployment validation procedures\n    - Establish code review processes\n    - Document maintenance procedures\n\n17. **Knowledge Base Integration**\n    - Link to relevant documentation\n    - Reference API documentation\n    - Include links to monitoring dashboards\n    - Connect to team communication channels\n    - Integrate with ticketing systems\n\n18. **Team Communication**\n    \n    ```markdown\n    ## Communication Channels\n    \n    ### Immediate Response\n    - Slack: #incidents channel\n    - Phone: On-call rotation\n    - Email: alerts@company.com\n    \n    ### Status Updates\n    - Status page: status.company.com\n    - Twitter: @company_status\n    - Internal wiki: troubleshooting section\n    ```\n\n19. **Documentation Maintenance**\n    - Regular review and updates\n    - Version control for troubleshooting guides\n    - Feedback collection from users\n    - Integration with incident post-mortems\n    - Continuous improvement processes\n\n20. **Self-Service Tools**\n    - Create diagnostic scripts and tools\n    - Build automated recovery procedures\n    - Implement self-healing systems\n    - Provide user-friendly diagnostic interfaces\n    - Create chatbot integration for common issues\n\n**Advanced Troubleshooting Techniques:**\n\n**Log Analysis:**\n```bash\n# Search for specific errors\ngrep -i \"error\" /var/log/app.log | tail -50\n\n# Analyze log patterns\nawk '{print $1}' access.log | sort | uniq -c | sort -nr\n\n# Monitor logs in real-time\ntail -f /var/log/app.log | grep -i \"exception\"\n```\n\n**Performance Profiling:**\n```bash\n# System performance\niostat -x 1\nsar -u 1 10\nvmstat 1 10\n\n# Application profiling\nstrace -p [PID]\nperf record -p [PID]\n```\n\nRemember to:\n- Keep troubleshooting guides up-to-date\n- Test all documented procedures regularly\n- Collect feedback from users and improve guides\n- Include screenshots and visual aids where helpful\n- Make guides searchable and well-organized",
      "tags": ["support", "incident-response"]
    },
    {
      "command": "/type-check",
      "label": "`/type-check`",
      "category": "Qualidade e Testes",
      "exemplos": [
        "`/type-check`",
        "`/type-check --pretty`",
        "`/type-check --file frontend/dashboard/src/components/DocsHybridSearchPage.tsx`",
        "`/type-check --watch`",
        "`/type-check all`"
      ],
      "capacidades": "Roda TypeScript `--noEmit` em frontend e backends TS.",
      "momentoIdeal": "Ao integrar novas tipagens no dashboard (ex.: hooks RAG) para evitar regressao de build.",
      "exemploMomento": "Depois de alterar `documentationService.ts`, validando contratos antes do build.",
      "tipoSaida": "Output de terminal com resultado do compilador (sem emissao de arquivos) destacando erros de tipo.",
      "fileName": "type-check.md",
      "filePath": ".claude/commands/type-check.md",
      "fileContent": "# Type Check Command\n\nExecute verificação de tipos TypeScript sem gerar arquivos.\n\n## Usage\n\n```bash\n/type-check [target] [options]\n```\n\n## Targets\n\n- `frontend` - Check frontend/dashboard (default)\n- `backend` - Check backend TypeScript files\n- `all` - Check all TypeScript code\n\n## Options\n\n- `--file <path>` - Check specific file\n- `--watch` - Watch mode (re-check on changes)\n- `--pretty` - Pretty output with colors\n\n## Examples\n\n```bash\n# Check frontend types\n/type-check\n\n# Check with pretty output\n/type-check --pretty\n\n# Check specific file\n/type-check --file src/components/pages/DocsHybridSearchPage.tsx\n\n# Watch mode\n/type-check --watch\n\n# Check all\n/type-check all\n```\n\n## Implementation\n\n```bash\n# Frontend\nif [[ \"{{target}}\" == \"frontend\" ]] || [[ \"{{target}}\" == \"\" ]]; then\n  cd frontend/dashboard\n\n  if [[ \"{{args}}\" == *\"--watch\"* ]]; then\n    npx tsc --noEmit --watch\n  elif [[ \"{{args}}\" == *\"--pretty\"* ]]; then\n    npx tsc --noEmit --pretty\n  elif [[ \"{{args}}\" == *\"--file\"* ]]; then\n    file_path=$(echo \"{{args}}\" | grep -oP '(?<=--file )\\S+')\n    npx tsc --noEmit \"$file_path\"\n  else\n    npx tsc --noEmit\n  fi\n\n  cd ../..\nfi\n\n# Backend (if TypeScript)\nif [[ \"{{target}}\" == \"backend\" ]] || [[ \"{{target}}\" == \"all\" ]]; then\n  for api in backend/api/*/; do\n    cd \"$api\"\n    if [[ -f \"tsconfig.json\" ]]; then\n      echo \"Type checking $api...\"\n      npx tsc --noEmit\n    fi\n    cd ../../..\n  done\nfi\n```\n\n## Common Type Errors\n\n### TS2345 - Argument type mismatch\n```typescript\n// ❌ Error\nfunction greet(name: string) { }\ngreet(123);\n\n// ✅ Fix\ngreet(\"John\");\n```\n\n### TS2322 - Type incompatible\n```typescript\n// ❌ Error\nconst num: number = \"hello\";\n\n// ✅ Fix\nconst num: number = 42;\n```\n\n### TS2339 - Property not found\n```typescript\n// ❌ Error\ninterface User { name: string; }\nconst user: User = { name: \"John\" };\nconsole.log(user.age);\n\n// ✅ Fix\ninterface User { name: string; age?: number; }\nconst user: User = { name: \"John\", age: 30 };\nconsole.log(user.age);\n```\n\n### TS7006 - Implicit any\n```typescript\n// ❌ Error\nfunction add(a, b) {\n  return a + b;\n}\n\n// ✅ Fix\nfunction add(a: number, b: number): number {\n  return a + b;\n}\n```\n\n## TypeScript Config\n\nFrontend uses strict TypeScript config:\n\n```json\n{\n  \"compilerOptions\": {\n    \"strict\": true,\n    \"noImplicitAny\": true,\n    \"strictNullChecks\": true,\n    \"strictFunctionTypes\": true,\n    \"noUnusedLocals\": true,\n    \"noUnusedParameters\": true\n  }\n}\n```\n\n## Performance Tips\n\nFor faster type checking:\n\n```bash\n# Incremental mode (caches type info)\nnpx tsc --noEmit --incremental\n\n# Skip library checks\nnpx tsc --noEmit --skipLibCheck\n```\n\n## Related Commands\n\n- `/quality-check` - Full quality check (includes type check)\n- `/lint` - ESLint verification\n- `/build` - Production build (includes type check)\n",
      "tags": ["quality", "typescript"]
    },
    {
      "command": "/ultra-think",
      "label": "`/ultra-think`",
      "category": "Arquitetura e Estrategia",
      "exemplos": [
        "`/ultra-think Devemos separar o proxy RAG em microservico independente?`",
        "`/ultra-think Qual estrategia seguir para reduzir o custo de ingestao de dados?`"
      ],
      "capacidades": "Modo de analise profunda com avaliacoes tecnica, negocio, usuario e sistema.",
      "momentoIdeal": "Para deliberar decisoes criticas (ex.: migrar parte do fluxo RAG para microservico dedicado).",
      "exemploMomento": "Avaliar se vale substituir LlamaIndex por outra stack para reduzir tempo de ingestao.",
      "tipoSaida": "Analise extensa em texto estruturado com opcoes avaliadas, trade-offs e recomendacao final.",
      "fileName": "ultra-think.md",
      "filePath": ".claude/commands/ultra-think.md",
      "fileContent": "# Deep Analysis and Problem Solving Mode\n\nDeep analysis and problem solving mode\n\n## Instructions\n\n1. **Initialize Ultra Think Mode**\n   - Acknowledge the request for enhanced analytical thinking\n   - Set context for deep, systematic reasoning\n   - Prepare to explore the problem space comprehensively\n\n2. **Parse the Problem or Question**\n   - Extract the core challenge from: $ARGUMENTS\n   - Identify all stakeholders and constraints\n   - Recognize implicit requirements and hidden complexities\n   - Question assumptions and surface unknowns\n\n3. **Multi-Dimensional Analysis**\n   Approach the problem from multiple angles:\n   \n   ### Technical Perspective\n   - Analyze technical feasibility and constraints\n   - Consider scalability, performance, and maintainability\n   - Evaluate security implications\n   - Assess technical debt and future-proofing\n   \n   ### Business Perspective\n   - Understand business value and ROI\n   - Consider time-to-market pressures\n   - Evaluate competitive advantages\n   - Assess risk vs. reward trade-offs\n   \n   ### User Perspective\n   - Analyze user needs and pain points\n   - Consider usability and accessibility\n   - Evaluate user experience implications\n   - Think about edge cases and user journeys\n   \n   ### System Perspective\n   - Consider system-wide impacts\n   - Analyze integration points\n   - Evaluate dependencies and coupling\n   - Think about emergent behaviors\n\n4. **Generate Multiple Solutions**\n   - Brainstorm at least 3-5 different approaches\n   - For each approach, consider:\n     - Pros and cons\n     - Implementation complexity\n     - Resource requirements\n     - Potential risks\n     - Long-term implications\n   - Include both conventional and creative solutions\n   - Consider hybrid approaches\n\n5. **Deep Dive Analysis**\n   For the most promising solutions:\n   - Create detailed implementation plans\n   - Identify potential pitfalls and mitigation strategies\n   - Consider phased approaches and MVPs\n   - Analyze second and third-order effects\n   - Think through failure modes and recovery\n\n6. **Cross-Domain Thinking**\n   - Draw parallels from other industries or domains\n   - Apply design patterns from different contexts\n   - Consider biological or natural system analogies\n   - Look for innovative combinations of existing solutions\n\n7. **Challenge and Refine**\n   - Play devil's advocate with each solution\n   - Identify weaknesses and blind spots\n   - Consider \"what if\" scenarios\n   - Stress-test assumptions\n   - Look for unintended consequences\n\n8. **Synthesize Insights**\n   - Combine insights from all perspectives\n   - Identify key decision factors\n   - Highlight critical trade-offs\n   - Summarize innovative discoveries\n   - Present a nuanced view of the problem space\n\n9. **Provide Structured Recommendations**\n   Present findings in a clear structure:\n   ```\n   ## Problem Analysis\n   - Core challenge\n   - Key constraints\n   - Critical success factors\n   \n   ## Solution Options\n   ### Option 1: [Name]\n   - Description\n   - Pros/Cons\n   - Implementation approach\n   - Risk assessment\n   \n   ### Option 2: [Name]\n   [Similar structure]\n   \n   ## Recommendation\n   - Recommended approach\n   - Rationale\n   - Implementation roadmap\n   - Success metrics\n   - Risk mitigation plan\n   \n   ## Alternative Perspectives\n   - Contrarian view\n   - Future considerations\n   - Areas for further research\n   ```\n\n10. **Meta-Analysis**\n    - Reflect on the thinking process itself\n    - Identify areas of uncertainty\n    - Acknowledge biases or limitations\n    - Suggest additional expertise needed\n    - Provide confidence levels for recommendations\n\n## Usage Examples\n\n```bash\n# Architectural decision\n/ultra-think Should we migrate to microservices or improve our monolith?\n\n# Complex problem solving\n/ultra-think How do we scale our system to handle 10x traffic while reducing costs?\n\n# Strategic planning\n/ultra-think What technology stack should we choose for our next-gen platform?\n\n# Design challenge\n/ultra-think How can we improve our API to be more developer-friendly while maintaining backward compatibility?\n```\n\n## Key Principles\n\n- **First Principles Thinking**: Break down to fundamental truths\n- **Systems Thinking**: Consider interconnections and feedback loops\n- **Probabilistic Thinking**: Work with uncertainties and ranges\n- **Inversion**: Consider what to avoid, not just what to do\n- **Second-Order Thinking**: Consider consequences of consequences\n\n## Output Expectations\n\n- Comprehensive analysis (typically 2-4 pages of insights)\n- Multiple viable solutions with trade-offs\n- Clear reasoning chains\n- Acknowledgment of uncertainties\n- Actionable recommendations\n- Novel insights or perspectives",
      "tags": ["strategy", "analysis"]
    },
    {
      "command": "/update-dependencies",
      "label": "`/update-dependencies`",
      "category": "Setup e Padroes",
      "exemplos": [
        "`/update-dependencies --patch`",
        "`/update-dependencies --minor`",
        "`/update-dependencies --major`",
        "`/update-dependencies --security-only`"
      ],
      "capacidades": "Orquestra atualizacao de dependencias com estagios, testes e auditorias.",
      "momentoIdeal": "Em mutiroes trimestrais de manutencao, reduzindo risco de break change sem supervisao.",
      "exemploMomento": "Antes de abertura de release, atualizando pacotes do dashboard e reassinando locks.",
      "tipoSaida": "Relatorio sequencial das atualizacoes realizadas, versoes antigas/novas e resultados dos testes.",
      "fileName": "update-dependencies.md",
      "filePath": ".claude/commands/update-dependencies.md",
      "fileContent": "# Update Dependencies\n\nUpdate and modernize project dependencies with safety checks: **$ARGUMENTS**\n\n## Current Dependencies State\n\n- Package manager: @package.json or @requirements.txt or @Cargo.toml (detect package manager)\n- Outdated packages: !`npm outdated 2>/dev/null || pip list --outdated 2>/dev/null || echo \"Manual check needed\"`\n- Security issues: !`npm audit --audit-level=moderate 2>/dev/null || pip check 2>/dev/null || echo \"Run security audit\"`\n- Lock files: @package-lock.json or @poetry.lock or @Cargo.lock\n\n## Task\n\nSystematically update project dependencies with comprehensive testing and compatibility validation:\n\n**Update Strategy**: Use $ARGUMENTS to specify patch updates, minor updates, major updates, or security-only updates\n\n**Update Process**:\n1. **Dependency Analysis** - Audit current versions, identify outdated packages, assess security vulnerabilities\n2. **Impact Assessment** - Check changelogs, breaking changes, deprecation warnings, compatibility matrix\n3. **Staged Updates** - Apply patch updates first, then minor, finally major versions with testing between stages\n4. **Testing & Validation** - Run full test suite, build verification, integration testing, performance checks\n5. **Rollback Strategy** - Document changes, create restore points, maintain rollback procedures\n6. **Documentation Updates** - Update README, dependencies list, migration guides, team notifications\n\n**Safety Features**: Automated testing between updates, dependency conflict resolution, security vulnerability prioritization.\n\n**Output**: Updated dependency manifest with comprehensive testing results, security audit report, and upgrade documentation.",
      "tags": ["maintenance", "dependencies"]
    },
    {
      "command": "/update-docs",
      "label": "`/update-docs`",
      "category": "Referencia e Organizacao",
      "exemplos": [
        "`/update-docs --implementation`",
        "`/update-docs --api`",
        "`/update-docs --architecture`",
        "`/update-docs --sync`",
        "`/update-docs --validate`"
      ],
      "capacidades": "Sincroniza docs com status de implementacao, marcando progresso e melhores praticas.",
      "momentoIdeal": "Ao concluir uma feature ou fase de auditoria (ex.: workflow tp-capital) para refletir novas decisoes nos relatorios.",
      "exemploMomento": "Depois de finalizar o script `validar-tp-capital-completo.sh`, registrando ajustes em `NEXT-STEPS-ACTION-PLAN.md`.",
      "tipoSaida": "Checklist e plano de atualizacao de arquivos, apontando quais documentos editar e quais marcadores atualizar.",
      "fileName": "update-docs.md",
      "filePath": ".claude/commands/update-docs.md",
      "fileContent": "# Documentation Update & Synchronization\n\nUpdate project documentation systematically: $ARGUMENTS\n\n## Current Documentation State\n\n- Documentation structure: !`find . -name \"*.md\" | head -10`\n- Specs directory: @specs/ (if exists)\n- Implementation status: !`grep -r \"✅\\|❌\\|⚠️\" docs/ specs/ 2>/dev/null | wc -l` status indicators\n- Recent changes: !`git log --oneline --since=\"1 week ago\" -- \"*.md\" | head -5`\n- Project progress: @CLAUDE.md or @README.md (if exists)\n\n## Task\n\n## Documentation Analysis\n\n1. Review current documentation status:\n   - Check `specs/implementation_status.md` for overall project status\n   - Review implemented phase document (`specs/phase{N}_implementation_plan.md`)\n   - Review `specs/flutter_structurizr_implementation_spec.md` and `specs/flutter_structurizr_implementation_spec_updated.md`\n   - Review `specs/testing_plan.md` to ensure it is current given recent test passes, failures, and changes\n   - Examine `CLAUDE.md` and `README.md` for project-wide documentation\n   - Check for and document any new lessons learned or best practices in CLAUDE.md\n\n2. Analyze implementation and testing results:\n   - Review what was implemented in the last phase\n   - Review testing results and coverage\n   - Identify new best practices discovered during implementation\n   - Note any implementation challenges and solutions\n   - Cross-reference updated documentation with recent implementation and test results to ensure accuracy\n\n## Documentation Updates\n\n1. Update phase implementation document:\n   - Mark completed tasks with ✅ status\n   - Update implementation percentages\n   - Add detailed notes on implementation approach\n   - Document any deviations from original plan with justification\n   - Add new sections if needed (lessons learned, best practices)\n   - Document specific implementation details for complex components\n   - Include a summary of any new troubleshooting tips or workflow improvements discovered during the phase\n\n2. Update implementation status document:\n   - Update phase completion percentages\n   - Add or update implementation status for components\n   - Add notes on implementation approach and decisions\n   - Document best practices discovered during implementation\n   - Note any challenges overcome and solutions implemented\n\n3. Update implementation specification documents:\n   - Mark completed items with ✅ or strikethrough but preserve original requirements\n   - Add notes on implementation details where appropriate\n   - Add references to implemented files and classes\n   - Update any implementation guidance based on experience\n\n4. Update CLAUDE.md and README.md if necessary:\n   - Add new best practices\n   - Update project status\n   - Add new implementation guidance\n   - Document known issues or limitations\n   - Update usage examples to include new functionality\n\n5. Document new testing procedures:\n   - Add details on test files created\n   - Include test running instructions\n   - Document test coverage\n   - Explain testing approach for complex components\n\n## Documentation Formatting and Structure\n\n1. Maintain consistent documentation style:\n   - Use clear headings and sections\n   - Include code examples where helpful\n   - Use status indicators (✅, ⚠️, ❌) consistently\n   - Maintain proper Markdown formatting\n\n2. Ensure documentation completeness:\n   - Cover all implemented features\n   - Include usage examples\n   - Document API changes or additions\n   - Include troubleshooting guidance for common issues\n\n## Guidelines\n\n- DO NOT CREATE new specification files\n- UPDATE existing files in the `specs/` directory\n- Maintain consistent documentation style\n- Include practical examples where appropriate\n- Cross-reference related documentation sections\n- Document best practices and lessons learned\n- Provide clear status updates on project progress\n- Update numerical completion percentages\n- Ensure documentation reflects actual implementation\n\nProvide a summary of documentation updates after completion, including:\n1. Files updated\n2. Major changes to documentation\n3. Updated completion percentages\n4. New best practices documented\n5. Status of the overall project after this phase",
      "tags": ["documentation", "sync"]
    },
    {
      "command": "/workflow-orchestrator",
      "label": "`/workflow-orchestrator`",
      "category": "Planejamento e Orquestracao",
      "exemplos": [
        "`/workflow-orchestrator create nightly-health-check`",
        "`/workflow-orchestrator run nightly-health-check`",
        "`/workflow-orchestrator schedule nightly-health-check --cron \"0 2 * * *\"`",
        "`/workflow-orchestrator monitor nightly-health-check`"
      ],
      "capacidades": "Cria, agenda e monitora workflows automatizados com dependencias e notificacoes.",
      "momentoIdeal": "Para encadear scripts (ex.: validar env, rodar health check e publicar relatorios) em uma rotina diaria.",
      "exemploMomento": "Automatizar o pipeline noturno que valida Kestra, coleta logs e atualiza o `STATUS-FINAL-LOGS`.",
      "tipoSaida": "Definicao detalhada de workflow (json/yaml ou markdown) e relatorio textual de execucao/monitoramento.",
      "fileName": "workflow-orchestrator.md",
      "filePath": ".claude/commands/workflow-orchestrator.md",
      "fileContent": "# Workflow Orchestrator\n\nOrchestrate complex automation workflows: $ARGUMENTS\n\n## Current Workflow State\n\n- Existing workflows: !`find . -name \"*.workflow.json\" -o -name \"workflow.yml\" -o -name \"Taskfile.yml\" | head -5`\n- Cron jobs: !`crontab -l 2>/dev/null || echo \"No crontab found\"`\n- Running processes: !`ps aux | grep -E \"(workflow|task|job)\" | head -3`\n- System capabilities: !`which docker node python3 | head -3`\n- Configuration: @.workflow-config.json or @workflows/ (if exists)\n\n## Task\n\nCreate and manage complex automation workflows with dependency management, scheduling, and monitoring.\n\n## Workflow Definition Structure\n\n### Basic Workflow Schema\n```json\n{\n  \"name\": \"deployment-workflow\",\n  \"version\": \"1.0.0\",\n  \"description\": \"Complete deployment automation with testing and rollback\",\n  \"trigger\": {\n    \"type\": \"manual|schedule|webhook|file_change\",\n    \"config\": {\n      \"schedule\": \"0 2 * * *\",\n      \"files\": [\"src/**/*\", \"package.json\"],\n      \"webhook\": \"/trigger/deploy\"\n    }\n  },\n  \"environment\": {\n    \"NODE_ENV\": \"production\",\n    \"LOG_LEVEL\": \"info\"\n  },\n  \"tasks\": [\n    {\n      \"id\": \"pre-build\",\n      \"name\": \"Pre-build validation\",\n      \"type\": \"shell\",\n      \"command\": \"npm run validate\",\n      \"timeout\": 300,\n      \"retry\": {\n        \"attempts\": 3,\n        \"delay\": 5000\n      }\n    },\n    {\n      \"id\": \"build\",\n      \"name\": \"Build application\",\n      \"type\": \"shell\",\n      \"command\": \"npm run build\",\n      \"depends_on\": [\"pre-build\"],\n      \"parallel\": false,\n      \"timeout\": 600\n    },\n    {\n      \"id\": \"test\",\n      \"name\": \"Run tests\",\n      \"type\": \"shell\",\n      \"command\": \"npm run test:ci\",\n      \"depends_on\": [\"build\"],\n      \"condition\": \"${env.SKIP_TESTS} != 'true'\"\n    },\n    {\n      \"id\": \"deploy\",\n      \"name\": \"Deploy to staging\",\n      \"type\": \"shell\",\n      \"command\": \"npm run deploy:staging\",\n      \"depends_on\": [\"test\"],\n      \"on_success\": [\"notify-success\"],\n      \"on_failure\": [\"rollback\", \"notify-failure\"]\n    }\n  ],\n  \"notifications\": {\n    \"channels\": [\"slack\", \"email\"],\n    \"on_completion\": true,\n    \"on_failure\": true\n  }\n}\n```\n\n## Advanced Workflow Features\n\n### 1. **Conditional Execution**\n```json\n{\n  \"id\": \"conditional-deploy\",\n  \"name\": \"Deploy if tests pass\",\n  \"type\": \"conditional\",\n  \"condition\": \"${tasks.test.exit_code} == 0 && ${env.DEPLOY_ENABLED} == 'true'\",\n  \"then\": {\n    \"type\": \"shell\",\n    \"command\": \"npm run deploy\"\n  },\n  \"else\": {\n    \"type\": \"shell\",\n    \"command\": \"echo 'Skipping deployment'\"\n  }\n}\n```\n\n### 2. **Parallel Task Execution**\n```json\n{\n  \"id\": \"parallel-tests\",\n  \"name\": \"Run parallel test suites\",\n  \"type\": \"parallel\",\n  \"tasks\": [\n    {\n      \"id\": \"unit-tests\",\n      \"command\": \"npm run test:unit\"\n    },\n    {\n      \"id\": \"integration-tests\", \n      \"command\": \"npm run test:integration\"\n    },\n    {\n      \"id\": \"e2e-tests\",\n      \"command\": \"npm run test:e2e\"\n    }\n  ],\n  \"wait_for\": \"all|any|first\",\n  \"timeout\": 1800\n}\n```\n\n### 3. **Loop and Iteration**\n```json\n{\n  \"id\": \"deploy-multiple-envs\",\n  \"name\": \"Deploy to multiple environments\",\n  \"type\": \"loop\",\n  \"items\": [\"staging\", \"qa\", \"production\"],\n  \"task\": {\n    \"type\": \"shell\",\n    \"command\": \"npm run deploy -- --env ${item}\",\n    \"timeout\": 300\n  },\n  \"parallel\": false,\n  \"stop_on_failure\": true\n}\n```\n\n### 4. **File and Data Processing**\n```json\n{\n  \"id\": \"process-data\",\n  \"name\": \"Process data files\",\n  \"type\": \"data_processor\",\n  \"input\": {\n    \"type\": \"file\",\n    \"path\": \"data/*.json\"\n  },\n  \"processor\": {\n    \"type\": \"javascript\",\n    \"script\": \"scripts/process-data.js\"\n  },\n  \"output\": {\n    \"type\": \"file\",\n    \"path\": \"processed/output.json\"\n  }\n}\n```\n\n## Workflow Orchestration Engine\n\n### Core Engine Implementation\n```javascript\nclass WorkflowOrchestrator {\n  constructor(config) {\n    this.config = config;\n    this.tasks = new Map();\n    this.running = new Set();\n    this.completed = new Set();\n    this.failed = new Set();\n    this.logger = new Logger(config.logLevel);\n  }\n\n  async execute(workflowPath) {\n    const workflow = await this.loadWorkflow(workflowPath);\n    \n    try {\n      await this.validateWorkflow(workflow);\n      await this.setupEnvironment(workflow.environment);\n      \n      const result = await this.executeWorkflow(workflow);\n      await this.cleanup();\n      \n      return result;\n    } catch (error) {\n      await this.handleError(error, workflow);\n      throw error;\n    }\n  }\n\n  async executeWorkflow(workflow) {\n    const taskGraph = this.buildDependencyGraph(workflow.tasks);\n    const execution = {\n      id: this.generateExecutionId(),\n      workflow: workflow.name,\n      startTime: Date.now(),\n      tasks: {}\n    };\n\n    while (this.hasRunnableTasks(taskGraph)) {\n      const runnableTasks = this.getRunnableTasks(taskGraph);\n      \n      if (runnableTasks.length === 0) {\n        break; // Circular dependency or all failed\n      }\n\n      await this.executeTaskBatch(runnableTasks, execution);\n    }\n\n    return this.generateExecutionReport(execution);\n  }\n\n  async executeTask(task, execution) {\n    const taskExecution = {\n      id: task.id,\n      name: task.name,\n      startTime: Date.now(),\n      status: 'running'\n    };\n\n    execution.tasks[task.id] = taskExecution;\n    this.running.add(task.id);\n\n    try {\n      // Pre-execution hooks\n      await this.runPreHooks(task);\n      \n      // Task execution\n      const result = await this.runTaskByType(task);\n      \n      // Post-execution hooks\n      await this.runPostHooks(task, result);\n\n      taskExecution.endTime = Date.now();\n      taskExecution.duration = taskExecution.endTime - taskExecution.startTime;\n      taskExecution.status = 'completed';\n      taskExecution.result = result;\n\n      this.completed.add(task.id);\n      this.running.delete(task.id);\n\n      // Handle success callbacks\n      if (task.on_success) {\n        await this.executeCallbacks(task.on_success, taskExecution);\n      }\n\n      return result;\n    } catch (error) {\n      taskExecution.endTime = Date.now();\n      taskExecution.duration = taskExecution.endTime - taskExecution.startTime;\n      taskExecution.status = 'failed';\n      taskExecution.error = error.message;\n\n      this.failed.add(task.id);\n      this.running.delete(task.id);\n\n      // Handle failure callbacks\n      if (task.on_failure) {\n        await this.executeCallbacks(task.on_failure, taskExecution);\n      }\n\n      throw error;\n    }\n  }\n\n  async runTaskByType(task) {\n    switch (task.type) {\n      case 'shell':\n        return await this.executeShellTask(task);\n      case 'http':\n        return await this.executeHttpTask(task);\n      case 'docker':\n        return await this.executeDockerTask(task);\n      case 'javascript':\n        return await this.executeJavaScriptTask(task);\n      case 'python':\n        return await this.executePythonTask(task);\n      default:\n        throw new Error(`Unknown task type: ${task.type}`);\n    }\n  }\n}\n```\n\n### Task Types Implementation\n\n#### Shell Task\n```javascript\nasync executeShellTask(task) {\n  const { spawn } = require('child_process');\n  \n  return new Promise((resolve, reject) => {\n    const process = spawn('sh', ['-c', task.command], {\n      cwd: task.cwd || process.cwd(),\n      env: { ...process.env, ...task.environment },\n      stdio: ['pipe', 'pipe', 'pipe']\n    });\n\n    let stdout = '';\n    let stderr = '';\n\n    process.stdout.on('data', (data) => {\n      stdout += data.toString();\n      if (task.live_output) {\n        console.log(data.toString());\n      }\n    });\n\n    process.stderr.on('data', (data) => {\n      stderr += data.toString();\n    });\n\n    const timeout = setTimeout(() => {\n      process.kill('SIGKILL');\n      reject(new Error(`Task timeout after ${task.timeout}ms`));\n    }, task.timeout || 300000);\n\n    process.on('close', (code) => {\n      clearTimeout(timeout);\n      if (code === 0) {\n        resolve({ stdout, stderr, exitCode: code });\n      } else {\n        reject(new Error(`Shell command failed with exit code ${code}: ${stderr}`));\n      }\n    });\n  });\n}\n```\n\n#### HTTP Task\n```javascript\nasync executeHttpTask(task) {\n  const axios = require('axios');\n  \n  const config = {\n    method: task.method || 'GET',\n    url: task.url,\n    headers: task.headers || {},\n    timeout: task.timeout || 30000\n  };\n\n  if (task.data) {\n    config.data = task.data;\n  }\n\n  if (task.auth) {\n    config.auth = task.auth;\n  }\n\n  try {\n    const response = await axios(config);\n    return {\n      status: response.status,\n      data: response.data,\n      headers: response.headers\n    };\n  } catch (error) {\n    throw new Error(`HTTP request failed: ${error.message}`);\n  }\n}\n```\n\n## Workflow Scheduling\n\n### Cron Integration\n```bash\n#!/bin/bash\n# setup-workflow-cron.sh\n\n# Daily backup workflow\n0 2 * * * cd /path/to/project && node workflow-engine.js run backup-workflow.json\n\n# Hourly health check\n0 * * * * cd /path/to/project && node workflow-engine.js run health-check.json\n\n# Weekly cleanup\n0 0 * * 0 cd /path/to/project && node workflow-engine.js run cleanup-workflow.json\n```\n\n### Systemd Timer (Linux)\n```ini\n# /etc/systemd/system/workflow-orchestrator.timer\n[Unit]\nDescription=Workflow Orchestrator Timer\nRequires=workflow-orchestrator.service\n\n[Timer]\nOnCalendar=*:0/5\nPersistent=true\n\n[Install]\nWantedBy=timers.target\n```\n\n## Monitoring and Alerting\n\n### Workflow Metrics Dashboard\n```javascript\nclass WorkflowMonitor {\n  constructor() {\n    this.metrics = {\n      totalRuns: 0,\n      successfulRuns: 0,\n      failedRuns: 0,\n      averageDuration: 0,\n      taskMetrics: new Map()\n    };\n  }\n\n  recordExecution(execution) {\n    this.metrics.totalRuns++;\n    \n    if (execution.status === 'completed') {\n      this.metrics.successfulRuns++;\n    } else {\n      this.metrics.failedRuns++;\n    }\n\n    // Update average duration\n    const totalDuration = this.metrics.averageDuration * (this.metrics.totalRuns - 1) + execution.duration;\n    this.metrics.averageDuration = totalDuration / this.metrics.totalRuns;\n\n    // Record task metrics\n    for (const [taskId, task] of Object.entries(execution.tasks)) {\n      if (!this.metrics.taskMetrics.has(taskId)) {\n        this.metrics.taskMetrics.set(taskId, {\n          runs: 0,\n          failures: 0,\n          averageDuration: 0\n        });\n      }\n\n      const taskMetrics = this.metrics.taskMetrics.get(taskId);\n      taskMetrics.runs++;\n      \n      if (task.status === 'failed') {\n        taskMetrics.failures++;\n      }\n\n      const taskTotalDuration = taskMetrics.averageDuration * (taskMetrics.runs - 1) + task.duration;\n      taskMetrics.averageDuration = taskTotalDuration / taskMetrics.runs;\n    }\n  }\n\n  getHealthReport() {\n    const successRate = (this.metrics.successfulRuns / this.metrics.totalRuns) * 100;\n    \n    return {\n      overall: {\n        successRate: successRate.toFixed(2) + '%',\n        totalRuns: this.metrics.totalRuns,\n        averageDuration: (this.metrics.averageDuration / 1000).toFixed(2) + 's'\n      },\n      tasks: this.getTaskHealthReport()\n    };\n  }\n}\n```\n\n### Alert Configuration\n```json\n{\n  \"alerts\": [\n    {\n      \"name\": \"workflow-failure\",\n      \"condition\": \"execution.status === 'failed'\",\n      \"channels\": [\"slack\", \"email\"],\n      \"template\": \"Workflow ${workflow.name} failed: ${error.message}\"\n    },\n    {\n      \"name\": \"high-failure-rate\",\n      \"condition\": \"metrics.successRate < 90\",\n      \"channels\": [\"slack\"],\n      \"template\": \"Workflow success rate dropped to ${metrics.successRate}%\"\n    },\n    {\n      \"name\": \"long-duration\",\n      \"condition\": \"execution.duration > workflow.expected_duration * 2\",\n      \"channels\": [\"email\"],\n      \"template\": \"Workflow taking unusually long: ${execution.duration}ms\"\n    }\n  ]\n}\n```\n\n## CLI Interface\n\n### Command-line Usage\n```bash\n# Create new workflow\nworkflow create --name \"deployment\" --template \"web-app\"\n\n# Run workflow\nworkflow run deployment-workflow.json\n\n# Schedule workflow\nworkflow schedule --cron \"0 2 * * *\" backup-workflow.json\n\n# Monitor workflows\nworkflow monitor --live\n\n# View execution history\nworkflow history --limit 10\n\n# Get workflow status\nworkflow status --execution-id abc123\n\n# Validate workflow\nworkflow validate deployment-workflow.json\n\n# Generate workflow from template\nworkflow generate --type \"ci-cd\" --output ci-workflow.json\n```\n\n## Integration Examples\n\n### Slack Integration\n```javascript\nasync function sendSlackNotification(message, channel = '#deployments') {\n  const webhook = process.env.SLACK_WEBHOOK_URL;\n  \n  await axios.post(webhook, {\n    channel: channel,\n    text: message,\n    username: 'Workflow Orchestrator',\n    icon_emoji: ':gear:'\n  });\n}\n```\n\n### Docker Integration\n```json\n{\n  \"id\": \"docker-build\",\n  \"name\": \"Build Docker image\",\n  \"type\": \"docker\",\n  \"config\": {\n    \"dockerfile\": \"Dockerfile\",\n    \"context\": \".\",\n    \"tags\": [\"myapp:latest\", \"myapp:${env.BUILD_NUMBER}\"],\n    \"build_args\": {\n      \"NODE_ENV\": \"production\"\n    }\n  }\n}\n```\n\n### Database Integration\n```json\n{\n  \"id\": \"db-migration\",\n  \"name\": \"Run database migrations\",\n  \"type\": \"database\",\n  \"config\": {\n    \"connection\": \"${env.DATABASE_URL}\",\n    \"migrations_path\": \"migrations/\",\n    \"rollback_on_failure\": true\n  }\n}\n```\n\nThis workflow orchestrator provides enterprise-grade automation capabilities with dependency management, monitoring, and cross-platform execution support.",
      "tags": ["workflow", "automation"]
    }
  ],
  "notes": [
    "Comandos com funcoes sobrepostas foram citados em suas secoes principais e referenciados onde se repetem.",
    "Consulte `.claude/commands/README.md` para fluxos combinados e atalhos adicionais.",
    "Opcionalmente, execute `/all-tools` no inicio da jornada para validar disponibilidades MCP antes de acionar estes comandos."
  ]
}
