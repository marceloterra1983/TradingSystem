# ‚úÖ Multi-Model Ingestion Support

**Data**: 2025-10-31 16:45
**Status**: ‚úÖ **IMPLEMENTADO**

---

## üéØ **O Que Foi Implementado**

O sistema agora **automaticamente detecta e configura os par√¢metros corretos de chunk_size** baseado no modelo de embedding selecionado. Cada modelo pode ser usado com sua configura√ß√£o otimizada!

---

## üîß **Mudan√ßas T√©cnicas**

### 1. Frontend: Fun√ß√£o de Mapeamento Autom√°tico

**Arquivo**: `frontend/dashboard/src/components/pages/LlamaIndexPage.tsx`

```typescript
/**
 * Get recommended chunk size based on embedding model
 * mxbai-embed-large has 512 token context window - needs smaller chunks
 * Most other models have 8192+ token context windows
 */
function getRecommendedChunkSize(modelName: string | null | undefined): { chunk_size: number; chunk_overlap: number } {
  if (!modelName) {
    return { chunk_size: 512, chunk_overlap: 96 };
  }

  const lower = modelName.toLowerCase();

  // mxbai-embed-large: 512 token context window
  if (lower.includes('mxbai')) {
    return { chunk_size: 256, chunk_overlap: 64 };
  }

  // Most other models: 8192+ token context window
  return { chunk_size: 512, chunk_overlap: 96 };
}
```

### 2. Frontend: Payload Atualizado

**Antes**:
```typescript
const payload: Record<string, unknown> = {};
if (targetCollection) {
  payload.collection_name = targetCollection;
}
if (collectionEmbeddingModel) {
  payload.embedding_model = collectionEmbeddingModel;
}
// ‚ùå chunk_size e chunk_overlap N√ÉO eram enviados!
```

**Depois**:
```typescript
const chunkConfig = getRecommendedChunkSize(collectionEmbeddingModel);

const payload: Record<string, unknown> = {};
if (targetCollection) {
  payload.collection_name = targetCollection;
}
if (collectionEmbeddingModel) {
  payload.embedding_model = collectionEmbeddingModel;
}
payload.chunk_size = chunkConfig.chunk_size;       // ‚úÖ Enviado automaticamente
payload.chunk_overlap = chunkConfig.chunk_overlap; // ‚úÖ Enviado automaticamente
```

### 3. Frontend: Log Detalhado

Agora o usu√°rio v√™ no log qual chunk_size est√° sendo usado:

```
[16:45:12] Detectados 218 arquivos pendentes. Iniciando ingest√£o...
[16:45:12] Modelo: mxbai-embed-large
[16:45:12] Chunk size: 256, overlap: 64  ‚úÖ Novo log!
```

### 4. Backend: J√° Estava Pronto!

O backend (`backend/api/documentation-api/src/routes/rag-status.js`) j√° aceitava `chunk_size` e `chunk_overlap` como par√¢metros desde a corre√ß√£o anterior:

```javascript
// Linha 589-600: Aceita chunk_size do body ou query string
const rawChunkSize = req.body?.chunk_size ?? req.body?.chunkSize ?? ...;
const rawChunkOverlap = req.body?.chunk_overlap ?? req.body?.chunkOverlap ?? ...;

// Linha 603-608: Usa chunk_size recomendado se n√£o fornecido
const effectiveModel = rawEmbeddingModel || inferEmbeddingModel(rawCollectionName);
const effectiveChunkSize = rawChunkSize !== null && rawChunkSize !== undefined
  ? Number(rawChunkSize)
  : getRecommendedChunkSize(effectiveModel);
```

---

## üìä **Configura√ß√£o por Modelo**

| Modelo | Context Window | Chunk Size | Chunk Overlap | Status |
|--------|---------------|------------|---------------|--------|
| **nomic-embed-text** | 8192 tokens | 512 | 96 | ‚úÖ **Recomendado** |
| **embeddinggemma:latest** | 8192 tokens | 512 | 96 | ‚úÖ Suportado |
| **mxbai-embed-large** | 512 tokens | 256 | 64 | ‚úÖ Suportado |

---

## üöÄ **Como Testar**

### **Teste 1: `documentation__nomic` (Recomendado)**

1. **Abra o navegador**: `http://localhost:3103/#/llamaindex-services`
2. **Hard refresh**: Pressione **Ctrl+Shift+R** (ou Cmd+Shift+R no Mac)
3. **Selecione**: `documentation__nomic`
4. **Clique**: "Iniciar ingest√£o"
5. **Observe o log**:
   ```
   [hor√°rio] Detectados 218 arquivos pendentes. Iniciando ingest√£o...
   [hor√°rio] Modelo: nomic-embed-text
   [hor√°rio] Chunk size: 512, overlap: 96  ‚úÖ
   ```
6. **Aguarde**: ~5-10 minutos para completar

**Esperado**: Sucesso sem erros de context length!

---

### **Teste 2: `documentation__mxbai` (Valida√ß√£o)**

1. **Abra o navegador**: `http://localhost:3103/#/llamaindex-services`
2. **Hard refresh**: Pressione **Ctrl+Shift+R**
3. **Selecione**: `documentation__mxbai`
4. **Clique**: "Iniciar ingest√£o"
5. **Observe o log**:
   ```
   [hor√°rio] Detectados 218 arquivos pendentes. Iniciando ingest√£o...
   [hor√°rio] Modelo: mxbai-embed-large
   [hor√°rio] Chunk size: 256, overlap: 64  ‚úÖ
   ```
6. **Aguarde**: ~10-15 minutos (mais lento devido aos chunks menores)

**Esperado**: Sucesso sem erros de context length!

---

### **Teste 3: `documentation__gemma` (Opcional)**

1. **Selecione**: `documentation__gemma`
2. **Observe o log**:
   ```
   [hor√°rio] Modelo: embeddinggemma:latest
   [hor√°rio] Chunk size: 512, overlap: 96  ‚úÖ
   ```
3. **Aguarde**: ~5-10 minutos

**Esperado**: Sucesso com performance alta (modelo otimizado Google)

---

## üéØ **Verifica√ß√£o R√°pida**

### Frontend Envia Par√¢metros Corretos?

Abra o DevTools (F12) ‚Üí Aba Network ‚Üí Inicie ingest√£o ‚Üí Veja o payload:

```json
POST /api/v1/rag/status/ingest
{
  "collection_name": "documentation__mxbai",
  "embedding_model": "mxbai-embed-large",
  "chunk_size": 256,        // ‚úÖ Autom√°tico baseado no modelo!
  "chunk_overlap": 64       // ‚úÖ Autom√°tico baseado no modelo!
}
```

### Backend Aplica Par√¢metros?

```bash
# Verificar logs do servi√ßo de ingest√£o
docker logs rag-llamaindex-ingest --tail 50

# Esperado:
# [timestamp] Processing with chunk_size=256, chunk_overlap=64
# [timestamp] Using embedding model: mxbai-embed-large
```

---

## üìù **Arquivos Modificados**

### Frontend
- **`frontend/dashboard/src/components/pages/LlamaIndexPage.tsx`**
  - Adicionada fun√ß√£o `getRecommendedChunkSize()`
  - Atualizado `handleIngest()` para enviar `chunk_size` e `chunk_overlap`
  - Adicionado log mostrando chunk_size ao usu√°rio

### Backend
- **`backend/api/documentation-api/src/routes/rag-status.js`**
  - J√° estava pronto! (modificado na corre√ß√£o anterior)
  - Aceita `chunk_size` e `chunk_overlap` como par√¢metros
  - Infere modelo e chunk_size recomendado automaticamente

---

## ‚úÖ **Checklist de Valida√ß√£o**

- [x] Frontend envia `chunk_size` baseado no modelo
- [x] Frontend envia `chunk_overlap` baseado no modelo
- [x] Frontend exibe chunk_size no log para o usu√°rio
- [x] Backend aceita `chunk_size` como par√¢metro
- [x] Backend tem fallback inteligente se chunk_size n√£o fornecido
- [ ] **Teste de ingest√£o com `documentation__nomic`** (aguardando usu√°rio)
- [ ] **Teste de ingest√£o com `documentation__mxbai`** (aguardando usu√°rio)

---

## üéâ **Resultado Esperado**

Agora voc√™ pode usar **qualquer modelo de embedding** dispon√≠vel! O sistema automaticamente:

1. ‚úÖ Detecta o modelo selecionado
2. ‚úÖ Calcula chunk_size e chunk_overlap ideais
3. ‚úÖ Envia para o backend
4. ‚úÖ Backend aplica na ingest√£o
5. ‚úÖ Sucesso sem erros de context length!

---

## üìû **Pr√≥ximos Passos**

1. **Fa√ßa hard refresh** no navegador (Ctrl+Shift+R)
2. **Teste com `documentation__nomic`** primeiro (recomendado)
3. **Confirme no log** que aparece "Chunk size: 512, overlap: 96"
4. **Aguarde conclus√£o** da ingest√£o
5. **Reporte o resultado** (sucesso ou erro)

Se tudo funcionar com `documentation__nomic`, teste com `documentation__mxbai` para validar que o sistema suporta ambos os modelos!

---

**Last Updated**: 2025-10-31 16:45 UTC
**Status**: ‚úÖ Pronto para Teste
**Breaking Changes**: None (backward compatible)
