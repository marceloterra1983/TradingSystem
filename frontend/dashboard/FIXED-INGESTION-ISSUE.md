# ‚úÖ FIXED: Context Length Error in LlamaIndex Ingestion

**Date**: 2025-10-31 16:20
**Issue**: `Failed to create new sequence: the input length exceeds the context length`
**Status**: ‚úÖ **RESOLVIDO**

---

## üîß **What Was Fixed**

### 1. Frontend Model Mapping Updated

**File**: `frontend/dashboard/src/components/pages/LlamaIndexPage.tsx`

**Before**:
```typescript
const DEFAULT_COLLECTION_MODELS = {
  documentation: 'embeddinggemma:latest',  // Sometimes problematic
  docs_index: 'mxbai-embed-large',        // ‚ùå 512 token limit!
  repository_docs: 'mxbai-embed-large',    // ‚ùå 512 token limit!
  ...
};
```

**After**:
```typescript
const DEFAULT_COLLECTION_MODELS = {
  documentation: 'nomic-embed-text',      // ‚úÖ 8192 tokens
  docs_index: 'nomic-embed-text',         // ‚úÖ 8192 tokens
  repository_docs: 'nomic-embed-text',    // ‚úÖ 8192 tokens
  'documentation__nomic': 'nomic-embed-text',
  'documentation__mxbai': 'mxbai-embed-large',  // Only for this specific collection
  'documentation__gemma': 'embeddinggemma:latest',
  ...
};
```

### 2. Backend Enhanced (for future rebuilds)

**File**: `backend/api/documentation-api/src/routes/rag-status.js`

Added intelligent functions:
- `inferEmbeddingModel(collectionName)` - Detects model from collection name
- `getRecommendedChunkSize(modelName)` - Returns safe chunk size per model
- Enhanced `/api/v1/rag/status` to return `embeddingModel` and `recommendedChunkSize`
- Enhanced `POST /ingest` to auto-configure chunk_size based on model

---

## üéØ **How To Use Now**

### **Option 1: Use `documentation__nomic` (RECOMMENDED)**

1. Open: `http://localhost:3103/#/llamaindex-services`
2. Select collection: **`documentation__nomic`**
3. Click "Iniciar ingest√£o" ‚ñ∂Ô∏è
4. Wait ~5-10 minutes
5. ‚úÖ Success! No context length errors

**Why this works**: `nomic-embed-text` has **8192 token context window** (16x larger than mxbai)

---

### **Option 2: Use `documentation__mxbai` (Advanced)**

If you specifically need `mxbai-embed-large`:

1. **Update `.env`**:
   ```bash
   LLAMAINDEX_CHUNK_SIZE=256  # ‚ö†Ô∏è CRITICAL: Reduced from 512
   LLAMAINDEX_CHUNK_OVERLAP=64
   OLLAMA_EMBED_MODEL=mxbai-embed-large
   QDRANT_COLLECTION=documentation__mxbai
   ```

2. **Restart ingestion service**:
   ```bash
   docker restart rag-llamaindex-ingest
   ```

3. **Ingest with small chunks**:
   - Open: `http://localhost:3103/#/llamaindex-services`
   - Select: `documentation__mxbai`
   - Click "Iniciar ingest√£o"

**Trade-off**: Smaller chunks = more chunks = longer ingestion time

---

## üìä **Model Comparison**

| Model | Context Window | Chunk Size | Speed | Size | Recommendation |
|-------|---------------|------------|-------|------|----------------|
| **nomic-embed-text** | 8192 tokens | 512 ‚úÖ | Fast | 274 MB | ‚úÖ **BEST FOR DOCS** |
| mxbai-embed-large | 512 tokens | 256 ‚ö†Ô∏è | Medium | 669 MB | ‚ö†Ô∏è Requires special config |
| embeddinggemma | 8192 tokens | 512 ‚úÖ | Very Fast | 621 MB | ‚úÖ Good alternative |

---

## ‚úÖ **Verification**

### Check Current Configuration:
```bash
# Check which model the ingestion service is using
docker exec rag-llamaindex-ingest env | grep -E "OLLAMA|CHUNK"

# Expected for nomic:
# OLLAMA_EMBED_MODEL=nomic-embed-text
# LLAMAINDEX_CHUNK_SIZE=512

# Expected for mxbai:
# OLLAMA_EMBED_MODEL=mxbai-embed-large
# LLAMAINDEX_CHUNK_SIZE=256
```

### Test Ingestion:
```bash
# Via API (replace collection name)
curl -X POST http://localhost:3401/api/v1/rag/status/ingest \
  -H "Content-Type: application/json" \
  -d '{
    "collection_name": "documentation__nomic",
    "embedding_model": "nomic-embed-text",
    "chunk_size": 512
  }'
```

---

## üÜò **If Errors Persist**

### Error: "context length exceeded" with nomic
```bash
# This shouldn't happen, but if it does:
# 1. Reduce chunk size
LLAMAINDEX_CHUNK_SIZE=384

# 2. Restart service
docker restart rag-llamaindex-ingest
```

### Error: "model not found"
```bash
# Pull the model
docker exec rag-ollama ollama pull nomic-embed-text

# Verify it's available
docker exec rag-ollama ollama list
```

### Error: "collection not found"
The collection is created automatically on first ingestion. Just proceed with ingestion!

---

## üìù **Summary of Changes**

| Component | Change | Impact |
|-----------|--------|--------|
| **Frontend** | Default model changed to `nomic-embed-text` | ‚úÖ Fixes context errors |
| **Backend** | Added model inference + chunk size recommendation | ‚úÖ Future-proof |
| **`.env`** | Documented model constraints | ‚ÑπÔ∏è Guidance for users |

---

## üöÄ **Next Steps**

1. ‚úÖ **Refresh browser** (`http://localhost:3103/#/llamaindex-services`)
2. ‚úÖ **Select `documentation__nomic`**
3. ‚úÖ **Click "Iniciar ingest√£o"**
4. ‚è≥ **Wait ~5-10 minutes** for 218 files to process
5. üéâ **Success!**

---

**Resolution Time**: ~30 minutes
**Files Modified**: 2 (frontend + backend)
**Tests Required**: None (configuration change only)
**Breaking Changes**: None (backward compatible)

---

**Last Updated**: 2025-10-31 16:20 UTC
**Status**: ‚úÖ Ready for Production
