# Resolu√ß√£o: Erro de Context Length no LlamaIndex

**Data**: 2025-10-31
**Erro**: `Failed to create new sequence: the input length exceeds the context length`
**Status**: ‚úÖ **RESOLVIDO**

---

## üî¥ **Problema**

Ao tentar fazer ingest√£o de documentos, o sistema retornava erro:
```json
{
  "success": false,
  "message": "Failed to create new sequence: the input length exceeds the context length (status code: 500)"
}
```

---

## üîç **Causa Raiz**

O modelo `mxbai-embed-large` tem um **context window muito pequeno** (512 tokens):

| Modelo | Context Window | Tamanho | Recomendado para |
|--------|---------------|---------|------------------|
| `nomic-embed-text` | **8192 tokens** | 274 MB | ‚úÖ **Documenta√ß√£o completa** |
| `mxbai-embed-large` | **512 tokens** | 669 MB | ‚ö†Ô∏è **Apenas textos curtos** |
| `embeddinggemma` | **8192 tokens** | 621 MB | ‚úÖ **Alternativa avan√ßada** |

Com `CHUNK_SIZE=512`, o modelo `mxbai-embed-large` **falha** porque:
- Texto base: ~512 tokens
- Formata√ß√£o Markdown: +50-100 tokens
- Overhead do modelo: +20-50 tokens
- **Total**: ~600-700 tokens ‚Üí **EXCEDE** os 512 tokens do limite

---

## ‚úÖ **Solu√ß√£o Aplicada**

### 1. Configura√ß√£o Corrigida no `.env`

```bash
# Usar nomic-embed-text (context window: 8192 tokens)
OLLAMA_EMBED_MODEL=nomic-embed-text
OLLAMA_EMBEDDING_MODEL=nomic-embed-text
QDRANT_COLLECTION=documentation__nomic

# Chunk size seguro para nomic-embed-text
LLAMAINDEX_CHUNK_SIZE=512
LLAMAINDEX_CHUNK_OVERLAP=96
```

### 2. Servi√ßo Reiniciado

```bash
docker restart rag-llamaindex-ingest
```

Confirma√ß√£o nos logs:
```
Vector store and storage context initialized for collection: documentation__nomic
GPU policy: forced=True, options={'num_gpu': 1}
Application startup complete
```

---

## üìã **Como Usar Corretamente**

### **Passo 1**: Acesse a p√°gina LlamaIndex Services
```
http://localhost:3103/#/llamaindex-services
```

### **Passo 2**: Selecione a cole√ß√£o **documentation__nomic**
- ‚úÖ Use: `documentation__nomic` (nomic-embed-text)
- ‚ùå Evite: `documentation__mxbai` (falha com documentos grandes)

### **Passo 3**: Clique em "Iniciar ingest√£o"
- Sistema ir√° processar 218 arquivos
- Tempo estimado: 5-10 minutos (dependendo da GPU)
- Progress ser√° exibido em tempo real

---

## üìä **Compara√ß√£o de Modelos**

### **nomic-embed-text** ‚úÖ **RECOMENDADO**
- **Context**: 8192 tokens (16x maior que mxbai)
- **Velocidade**: R√°pida
- **Tamanho**: 274 MB (menor)
- **Ideal para**: Documenta√ß√£o t√©cnica, artigos longos, MDX files
- **Chunk size m√°ximo seguro**: 1024 tokens

### **mxbai-embed-large** ‚ö†Ô∏è **USO LIMITADO**
- **Context**: 512 tokens (muito pequeno)
- **Velocidade**: Moderada
- **Tamanho**: 669 MB
- **Ideal para**: Textos curtos, tweets, t√≠tulos
- **Chunk size m√°ximo seguro**: 256 tokens
- **Requer**: `LLAMAINDEX_CHUNK_SIZE=256` no `.env`

### **embeddinggemma** ‚ö° **AVAN√áADO**
- **Context**: 8192 tokens
- **Velocidade**: Muito r√°pida (otimizado Google)
- **Tamanho**: 621 MB
- **Ideal para**: Alta performance, grandes volumes
- **Chunk size m√°ximo seguro**: 1024 tokens

---

## üõ†Ô∏è **Se quiser usar mxbai-embed-large**

**Somente recomendado** se voc√™ realmente precisa das caracter√≠sticas espec√≠ficas desse modelo.

### Ajuste necess√°rio no `.env`:

```bash
# CUIDADO: Context window limitado (512 tokens)
OLLAMA_EMBED_MODEL=mxbai-embed-large
OLLAMA_EMBEDDING_MODEL=mxbai-embed-large
QDRANT_COLLECTION=documentation__mxbai

# CR√çTICO: Reduzir chunk size para 256
LLAMAINDEX_CHUNK_SIZE=256
LLAMAINDEX_CHUNK_OVERLAP=64
```

### Reiniciar servi√ßos:
```bash
docker restart rag-llamaindex-ingest
docker restart rag-llamaindex-query
```

---

## ‚úÖ **Verifica√ß√£o P√≥s-Fix**

### 1. Verificar configura√ß√£o:
```bash
docker exec rag-llamaindex-ingest env | grep -E "OLLAMA|CHUNK|COLLECTION"
```

**Esperado**:
```
OLLAMA_EMBED_MODEL=nomic-embed-text
QDRANT_COLLECTION=documentation__nomic
LLAMAINDEX_CHUNK_SIZE=512
```

### 2. Testar ingest√£o via CLI:
```bash
curl -X POST http://localhost:8201/ingest/directory \
  -H "Content-Type: application/json" \
  -d '{
    "directory_path": "/data/docs",
    "collection_name": "documentation__nomic",
    "embedding_model": "nomic-embed-text"
  }'
```

### 3. Verificar cole√ß√µes dispon√≠veis:
```bash
curl -s http://localhost:6333/collections | jq '.result.collections[].name'
```

**Esperado**:
```
"documentation__nomic"
"documentation__mxbai"
...
```

---

## üìö **Documenta√ß√£o de Refer√™ncia**

### Limites de Context Window

| Modelo | Max Tokens | Max Chunk Size Seguro |
|--------|------------|----------------------|
| nomic-embed-text | 8192 | 1024 |
| mxbai-embed-large | 512 | 256 |
| embeddinggemma | 8192 | 1024 |
| text-embedding-3-small (OpenAI) | 8191 | 1024 |

### C√°lculo de Tokens Seguros

```
Token Count = (Text Length / 4) + Markdown Overhead + Model Overhead

Exemplo com CHUNK_SIZE=512:
- Text base: 512 chars √ó 4 = 128 tokens
- Markdown: +50 tokens (headers, links, code blocks)
- Model overhead: +30 tokens
- Total: ~208 tokens ‚úÖ Seguro para nomic (8192 limit)
- Total: ~208 tokens ‚ùå Arriscado para mxbai (512 limit)
```

---

## üéØ **Recomenda√ß√µes Finais**

1. ‚úÖ **Use `nomic-embed-text`** como padr√£o
2. ‚úÖ Mantenha `CHUNK_SIZE=512` para documenta√ß√£o
3. ‚ö†Ô∏è S√≥ use `mxbai-embed-large` com `CHUNK_SIZE=256`
4. ‚úÖ Teste ingest√£o com arquivos pequenos primeiro
5. ‚úÖ Monitore logs durante ingest√£o: `docker logs -f rag-llamaindex-ingest`

---

## üìû **Suporte**

**Se o erro persistir**:
1. Verifique logs: `docker logs rag-llamaindex-ingest --tail 100`
2. Confirme modelo no Ollama: `docker exec rag-ollama ollama list`
3. Verifique espa√ßo em disco: `df -h`
4. Reinicie stack completo: `docker compose -f tools/compose/docker-compose.rag.yml restart`

---

**Last Updated**: 2025-10-31 15:57 UTC
**Resolved By**: Development Team
**Status**: ‚úÖ Production Ready
