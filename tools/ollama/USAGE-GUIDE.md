---
title: üìò Guia de Uso do Ollama com GPU
sidebar_position: 1
tags: [documentation]
domain: ops
type: guide
summary: "- **Container:** Rodando com suporte a GPU (NVIDIA RTX 5090 - 32GB VRAM)"
status: active
last_review: "2025-10-23"
---

# üìò Guia de Uso do Ollama com GPU

## ‚úÖ Status Atual
- **Container:** Rodando com suporte a GPU (NVIDIA RTX 5090 - 32GB VRAM)
- **Modelos Instalados:** 4 modelos (95GB total)
  - üî• **gpt-oss:120b** (65GB) - MONSTRO! M√°xima qualidade
  - üéØ **gpt-oss:20b** (13GB) - Equilibrado e eficiente  
  - ‚ö° **gemma2:27b** (15GB) - Google, multil√≠ngue
  - üèÉ **llama3.2** (2GB) - R√°pido para respostas simples
- **Endpoint:** http://localhost:11434
- **Uso de VRAM:** ~30GB quando modelo 120B carregado

---

## üéØ Qual Modelo Usar Quando?

| Situa√ß√£o | Modelo | Motivo |
|----------|--------|--------|
| **An√°lise profunda de mercado** | üî• gpt-oss:120b | M√°xima qualidade, racioc√≠nio complexo |
| **Gera√ß√£o de c√≥digo** | üî• gpt-oss:120b | Melhor para l√≥gica complexa |
| **An√°lise t√©cnica detalhada** | üî• gpt-oss:120b | Nuances e precis√£o |
| **Perguntas gerais** | üéØ gpt-oss:20b | Equilibrado, r√°pido e bom |
| **Tradu√ß√£o/Multil√≠ngue** | ‚ö° gemma2:27b | Especializado |
| **Respostas r√°pidas** | üèÉ llama3.2 | Instant√¢neo |
| **Testes/Experimenta√ß√£o** | üèÉ llama3.2 | Econ√¥mico |

**üí° Dica:** Use gpt-oss:120b quando precisar da melhor qualidade, gpt-oss:20b para uso geral!

---

## üöÄ Formas de Usar

### 1Ô∏è‚É£ Via Linha de Comando (Shell)

```bash
# Listar modelos instalados
docker exec ollama ollama list

# Conversar com cada modelo (modo interativo)
docker exec -it ollama ollama run gpt-oss:120b    # MONSTRO (65GB)
docker exec -it ollama ollama run gpt-oss:20b     # Equilibrado (13GB)
docker exec -it ollama ollama run gemma2:27b      # Google (15GB)
docker exec -it ollama ollama run llama3.2        # R√°pido (2GB)

# Fazer perguntas diretamente
docker exec ollama ollama run gpt-oss:120b "An√°lise profunda sobre trading"
docker exec ollama ollama run gpt-oss:20b "Explique machine learning"
docker exec ollama ollama run llama3.2 "Ol√°, como vai?"

# Remover um modelo (liberar espa√ßo)
docker exec ollama ollama rm llama3.2

# Baixar outros modelos (se quiser)
docker exec ollama ollama pull codellama:13b  # Code Llama (7.3GB)
docker exec ollama ollama pull mistral:7b     # Mistral (4.1GB)
docker exec ollama ollama pull llama3.1:70b   # LLaMA 3.1 (40GB)
```

---

### 2Ô∏è‚É£ Via API REST (cURL)

```bash
# üî• MONSTRO - An√°lise profunda (gpt-oss:120b)
curl http://localhost:11434/api/generate -d '{
  "model": "gpt-oss:120b",
  "prompt": "Fa√ßa uma an√°lise detalhada sobre estrat√©gias de trading algor√≠tmico",
  "stream": false
}'

# üéØ EQUILIBRADO - Uso geral (gpt-oss:20b)
curl http://localhost:11434/api/generate -d '{
  "model": "gpt-oss:20b",
  "prompt": "Explique o que √© machine learning",
  "stream": false
}'

# ‚ö° GOOGLE - Multil√≠ngue (gemma2:27b)
curl http://localhost:11434/api/generate -d '{
  "model": "gemma2:27b",
  "prompt": "Translate to English: Como funcionam os mercados financeiros?",
  "stream": false
}'

# üèÉ R√ÅPIDO - Respostas simples (llama3.2)
curl http://localhost:11434/api/generate -d '{
  "model": "llama3.2",
  "prompt": "Ol√°! Como vai?",
  "stream": false
}'

# Chat (conversa√ß√£o com contexto)
curl http://localhost:11434/api/chat -d '{
  "model": "llama3.2",
  "messages": [
    {
      "role": "user",
      "content": "Ol√°! Como voc√™ est√°?"
    }
  ],
  "stream": false
}'

# Chat com streaming (respostas em tempo real)
curl http://localhost:11434/api/chat -d '{
  "model": "llama3.2",
  "messages": [
    {
      "role": "user",
      "content": "Conte uma hist√≥ria curta"
    }
  ],
  "stream": true
}'

# Listar modelos via API
curl http://localhost:11434/api/tags

# Verificar se est√° rodando
curl http://localhost:11434/api/version
```

---

### 3Ô∏è‚É£ Via Python

```python
import requests
import json

# Configura√ß√£o
OLLAMA_API = "http://localhost:11434/api"

# Fun√ß√£o para gerar texto
def generate_text(prompt, model="llama3.2"):
    response = requests.post(
        f"{OLLAMA_API}/generate",
        json={
            "model": model,
            "prompt": prompt,
            "stream": False
        }
    )
    return response.json()["response"]

# Fun√ß√£o para chat
def chat(messages, model="llama3.2"):
    response = requests.post(
        f"{OLLAMA_API}/chat",
        json={
            "model": model,
            "messages": messages,
            "stream": False
        }
    )
    return response.json()["message"]["content"]

# Exemplo de uso
if __name__ == "__main__":
    # Gerar texto simples
    result = generate_text("Explique o que √© trading algor√≠tmico")
    print("Resposta:", result)
    
    # Chat com contexto
    messages = [
        {"role": "user", "content": "Ol√°! Voc√™ pode me ajudar com Python?"},
        {"role": "assistant", "content": "Claro! Como posso ajudar?"},
        {"role": "user", "content": "Como criar uma API REST?"}
    ]
    reply = chat(messages)
    print("Chat:", reply)
```

**Com Streaming (respostas em tempo real):**

```python
import requests
import json

def chat_streaming(prompt, model="llama3.2"):
    """Recebe respostas em tempo real (streaming)"""
    response = requests.post(
        "http://localhost:11434/api/chat",
        json={
            "model": model,
            "messages": [{"role": "user", "content": prompt}],
            "stream": True
        },
        stream=True
    )
    
    print("Resposta: ", end="", flush=True)
    for line in response.iter_lines():
        if line:
            data = json.loads(line)
            if "message" in data:
                content = data["message"]["content"]
                print(content, end="", flush=True)
    print()  # Nova linha no final

# Exemplo
chat_streaming("Conte uma piada sobre programadores")
```

---

### 4Ô∏è‚É£ Via JavaScript/Node.js

```javascript
// Instalar axios: npm install axios

const axios = require('axios');

const OLLAMA_API = 'http://localhost:11434/api';

// Gerar texto
async function generateText(prompt, model = 'llama3.2') {
  const response = await axios.post(`${OLLAMA_API}/generate`, {
    model,
    prompt,
    stream: false
  });
  return response.data.response;
}

// Chat
async function chat(messages, model = 'llama3.2') {
  const response = await axios.post(`${OLLAMA_API}/chat`, {
    model,
    messages,
    stream: false
  });
  return response.data.message.content;
}

// Exemplo de uso
(async () => {
  // Texto simples
  const result = await generateText('O que √© machine learning?');
  console.log('Resposta:', result);
  
  // Chat
  const messages = [
    { role: 'user', content: 'Ol√°! Pode me ajudar?' },
    { role: 'assistant', content: 'Claro! Como posso ajudar?' },
    { role: 'user', content: 'Explique React Hooks' }
  ];
  const reply = await chat(messages);
  console.log('Chat:', reply);
})();
```

---

### 5Ô∏è‚É£ Integra√ß√£o com TradingSystem (React)

Crie um servi√ßo para usar no Dashboard:

```typescript
// frontend/dashboard/src/services/ollamaService.ts

export interface ChatMessage {
  role: 'user' | 'assistant' | 'system';
  content: string;
}

export class OllamaService {
  private baseUrl = 'http://localhost:11434/api';
  private model = 'llama3.2';

  async generate(prompt: string): Promise<string> {
    const response = await fetch(`${this.baseUrl}/generate`, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        model: this.model,
        prompt,
        stream: false
      })
    });
    
    const data = await response.json();
    return data.response;
  }

  async chat(messages: ChatMessage[]): Promise<string> {
    const response = await fetch(`${this.baseUrl}/chat`, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        model: this.model,
        messages,
        stream: false
      })
    });
    
    const data = await response.json();
    return data.message.content;
  }

  async *chatStreaming(messages: ChatMessage[]): AsyncGenerator<string> {
    const response = await fetch(`${this.baseUrl}/chat`, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        model: this.model,
        messages,
        stream: true
      })
    });

    if (!response.body) throw new Error('No response body');
    
    const reader = response.body.getReader();
    const decoder = new TextDecoder();

    while (true) {
      const { done, value } = await reader.read();
      if (done) break;
      
      const chunk = decoder.decode(value);
      const lines = chunk.split('\n').filter(Boolean);
      
      for (const line of lines) {
        try {
          const data = JSON.parse(line);
          if (data.message?.content) {
            yield data.message.content;
          }
        } catch (e) {
          console.error('Error parsing JSON:', e);
        }
      }
    }
  }

  async listModels(): Promise<string[]> {
    const response = await fetch(`${this.baseUrl}/tags`);
    const data = await response.json();
    return data.models.map((m: any) => m.name);
  }
}

export const ollamaService = new OllamaService();
```

**Usar no componente React:**

```tsx
import { useState } from 'react';
import { ollamaService } from '@/services/ollamaService';

export function AIAssistant() {
  const [prompt, setPrompt] = useState('');
  const [response, setResponse] = useState('');
  const [loading, setLoading] = useState(false);

  const handleSubmit = async (e: React.FormEvent) => {
    e.preventDefault();
    setLoading(true);
    
    try {
      const result = await ollamaService.generate(prompt);
      setResponse(result);
    } catch (error) {
      console.error('Error:', error);
    } finally {
      setLoading(false);
    }
  };

  return (
    <div className="p-4">
      <form onSubmit={handleSubmit} className="space-y-4">
        <textarea
          value={prompt}
          onChange={(e) => setPrompt(e.target.value)}
          className="w-full p-2 border rounded"
          placeholder="Fa√ßa uma pergunta..."
          rows={4}
        />
        <button
          type="submit"
          disabled={loading}
          className="px-4 py-2 bg-blue-500 text-white rounded hover:bg-blue-600"
        >
          {loading ? 'Pensando...' : 'Enviar'}
        </button>
      </form>
      
      {response && (
        <div className="mt-4 p-4 bg-gray-100 rounded">
          <h3 className="font-bold mb-2">Resposta:</h3>
          <p className="whitespace-pre-wrap">{response}</p>
        </div>
      )}
    </div>
  );
}
```

---

## üéØ Seus Modelos Instalados

| Modelo | Tamanho | Qualidade | Uso Recomendado |
|--------|---------|-----------|-----------------|
| **gpt-oss:120b** üî• | 65GB | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | An√°lises complexas, c√≥digo avan√ßado, m√°xima qualidade |
| **gpt-oss:20b** üéØ | 13GB | ‚≠ê‚≠ê‚≠ê‚≠ê | Uso geral, equilibrado, √≥timo custo-benef√≠cio |
| **gemma2:27b** ‚ö° | 15GB | ‚≠ê‚≠ê‚≠ê‚≠ê | Multil√≠ngue, Google, tradu√ß√£o |
| **llama3.2** üèÉ | 2GB | ‚≠ê‚≠ê‚≠ê | Respostas r√°pidas, testes, experimenta√ß√£o |

---

## üì¶ Outros Modelos Dispon√≠veis

| Modelo | Tamanho | Uso | Comando |
|--------|---------|-----|---------|
| **llama3.1:70b** | 40GB | Pr√≥ximo GPT-4 | `docker exec ollama ollama pull llama3.1:70b` |
| **llama3.1:8b** | 4.7GB | R√°pido e bom | `docker exec ollama ollama pull llama3.1:8b` |
| **codellama:13b** | 7.3GB | C√≥digo | `docker exec ollama ollama pull codellama:13b` |
| **mistral:7b** | 4.1GB | Equilibrado | `docker exec ollama ollama pull mistral:7b` |
| **qwen2.5:32b** | 19GB | Excelente | `docker exec ollama ollama pull qwen2.5:32b` |

Lista completa: https://ollama.com/library

---

## üîß Gerenciamento do Container

```bash
# Ver logs
docker logs -f ollama

# Parar container
docker stop ollama

# Iniciar container
docker start ollama

# Reiniciar container
docker restart ollama

# Ver uso de GPU
docker exec ollama nvidia-smi

# Ver informa√ß√µes do container
docker inspect ollama

# Remover container (mant√©m volume de dados)
docker rm -f ollama

# Remover tudo (incluindo modelos baixados)
docker rm -f ollama && docker volume rm ollama-data
```

---

## üìä Monitoramento de GPU

```bash
# Ver uso da GPU em tempo real
watch -n 1 nvidia-smi

# Ver uso de recursos do container
docker stats ollama
```

---

## üÜò Troubleshooting

### Container n√£o inicia
```bash
docker logs ollama
docker restart ollama
```

### GPU n√£o est√° sendo usada
```bash
# Verificar se NVIDIA Container Toolkit est√° instalado
docker run --rm --gpus all nvidia/cuda:12.0.0-base-ubuntu22.04 nvidia-smi
```

### Porta 11434 em uso
```bash
# Ver o que est√° usando a porta
sudo lsof -i :11434
ss -tuln | grep 11434
```

### Modelo muito lento
- Modelos grandes (>4GB) s√£o mais lentos
- Use modelos menores: `llama3.2` (2GB) ou `phi3` (2.4GB)
- Verifique se a GPU est√° sendo usada: `docker exec ollama nvidia-smi`

---

## üìö Documenta√ß√£o Oficial

- **Ollama Docs**: https://github.com/ollama/ollama/blob/main/docs/api.md
- **Modelos Dispon√≠veis**: https://ollama.com/library
- **API Reference**: https://github.com/ollama/ollama/blob/main/docs/api.md

---

## üí° Casos de Uso no TradingSystem

1. **An√°lise de Sentimento**: Analisar not√≠cias de mercado
2. **Gera√ß√£o de Relat√≥rios**: Criar relat√≥rios autom√°ticos de performance
3. **Assistente de Trading**: Responder perguntas sobre estrat√©gias
4. **Documenta√ß√£o**: Gerar documenta√ß√£o de c√≥digo
5. **Code Review**: Sugerir melhorias no c√≥digo
6. **Data Analysis**: Interpretar dados de mercado

---

**Pronto! Agora voc√™ tem um LLM local rodando com GPU! üöÄ**

