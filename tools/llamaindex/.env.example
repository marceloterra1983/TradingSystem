# LlamaIndex Service Configuration

# OpenAI API Configuration (optional if using Ollama)
OPENAI_API_KEY=your_openai_api_key
OPENAI_MODEL=gpt-3.5-turbo  # or gpt-4 for higher quality

# Ollama (Local LLM & Embeddings)
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_EMBED_MODEL=nomic-embed-text
OLLAMA_MODEL=llama3  # set if you want LLM-generated answers

# Qdrant Configuration
QDRANT_HOST=localhost
QDRANT_PORT=6333
QDRANT_GRPC_PORT=6334
QDRANT_API_KEY=  # Set in production
QDRANT_HTTPS_ENABLED=false  # Enable in production

# Service Configuration
LOG_LEVEL=INFO  # DEBUG, INFO, WARNING, ERROR, CRITICAL
ENVIRONMENT=development  # development, staging, production

# Rate Limiting
RATE_LIMIT_REQUESTS=100  # Requests per period
RATE_LIMIT_PERIOD=60  # Period in seconds

# Security
JWT_SECRET_KEY=your_jwt_secret_key  # Change in production
JWT_ALGORITHM=HS256
ACCESS_TOKEN_EXPIRE_MINUTES=30

# Monitoring
ENABLE_METRICS=true
METRICS_PORT=9090

# Document Processing
MAX_CHUNK_SIZE=512
CHUNK_OVERLAP=50
# Embedding model hint (legacy; prefer OLLAMA_EMBED_MODEL)
EMBEDDING_MODEL=text-embedding-ada-002

# Cache Configuration
CACHE_TYPE=redis  # memory or redis
REDIS_HOST=localhost  # If using Redis
REDIS_PORT=6379
CACHE_TTL=3600  # Cache TTL in seconds
